<br/> 
<ul>
 <li> <a href="#1">1. Introduction and Motivation</a></li>
  <ul>
   <li><a href="#1.1">1.1. Models, systems, and system states</a></li>
   <li><a href="#1.2">1.2. A language of system states: terms and their algebraic properties</a></li>
   <li><a href="#1.3">1.3. Extending the language of models: vectors and matrices</a></li>
   <li><a href="#1.4">1.4. Review: formulas, sets, and term equality</a></li>
  </ul>
 <li> <a href="#2">2. Vectors</a>
  <ul>
   <li><a href="#2.1">2.1. Defining operations on vectors</a></li>
   <li><a href="#2.2">2.2. Properties of vector operations</a></li>
   <li><a href="#2.3">2.3. <b>Assignment #1: Vector Algebra</b></a></li>
   <li><a href="#2.4">2.4. Common vector properties, relationships, and operators</a></li>
   <li><a href="#2.5">2.5. Solving common problems involving vector algebra</a></li>
   <li><a href="#2.6">2.6. Using vectors and linear combinations to model systems</a></li>
  </ul>
 </li>
 <li> <a href="#3">3. Matrices</a>
  <ul>
   <li><a href="#3.1">3.1. Matrices and multiplication of a vector by a matrix</a></li>
   <li><a href="#3.2">3.2. Interpreting matrices as tables of relationships and transformations of system states</a></li>
   <li><a href="#3.3">3.3. Interpreting multiplication of matrices as composition of system state transformations</a></li>  
   <li><a href="#3.4">3.4. <b>Assignment #2: Using Vectors and Matrices</b></a></li>
   <li><a href="#3.5">3.5. Matrix operations and their interpretations</a></li>
   <li><a href="#3.6">3.6. Matrix properties</a></li>
   <li><a href="#3.7">3.7. Solving the equation %M %v = %w for %M with various properties</a></li>
   <li><a href="#3.8">3.8. Row echelon form and reduced row echelon form</a></li>
   <li><a href="#3.9">3.9. Matrix transpose</a></li>
   <li><a href="#3.10">3.10. Orthogonal matrices</a></li>
   <li><a href="#3.11">3.11. Matrix rank</a></li>
   <li><a href="#3.12">3.12. <b>Assignment #3: Matrix Properties and Operations</b></a></li>
  </ul>
 </li>
 <li> <a href="#R.1">Review 1. Vector and Matrix Algebra and Applications</a> </li>
 <li> <a href="#4">4. Vector Spaces</a> 
  <ul>
   <li><a href="#4.1">4.1. Sets of vectors and their notation</a></li>
   <li><a href="#4.2">4.2. Membership and equality relations involving sets of vectors</a></li>
   <li><a href="#4.3">4.3. Vector spaces as abstract structures</a></li>
   <li><a href="#4.4">4.4. <b>Assignment #4: Vector Spaces and Polynomials</b></a></a></li>
   <li><a href="#4.5">4.5. Basis, dimension, and orthonormal basis of a vector space</a></li>
   <li><a href="#4.6">4.6. Homogenous, non-homogenous, overdetermined, and underdetermined systems</a></li>
   <li><a href="#4.7">4.7. Application: approximating overdetermined systems</a></li>
   <li><a href="#4.8">4.8. Application: approximating a model of system state relationships</a></li>
   <li><a href="#4.9">4.9. Application: distributed computation of least-squares approximations</a></li>
   <li><a href="#4.10">4.10. Orthogonal complements and algebra of vector spaces</a></li>
  </ul>
 </li>
 <li> <a href="#5">5. Linear Transformations</a>
  <ul>
   <li><a href="#5.1">5.1. Set products, relations, and maps</a></li>
   <li><a href="#5.2">5.2. Homomorphisms and isomorphisms</a></li>
   <li><a href="#5.3">5.3. Linear transformations</a></li>
   <li><a href="#5.4">5.4. Orthogonal projections as linear transformations</a></li>
   <li><a href="#5.5">5.5. <b>Assignment #5: Approximations and Linear Transformations</b></a></a></li>
   <li><a href="#5.6">5.6. Matrices as linear transformations</a></li>
   <li><a href="#5.7">5.7. Application: communications</a></li>
   <li><a href="#5.8">5.8. Affine spaces, affine transformations, and optimization</a></li>
   <li><a href="#5.9">5.9. Fixed points, eigenvectors, and eigenvalues</a></li>
  </ul>
 </li>
 <li> <a href="#R.2">Review 2. Vector and Matrix Algebra, Vector Spaces, and Linear Transformations</a> </li>
 <li> <a href="#A">Appendix A. Notes on tools</a>
  <!--<ul>
   <li><a href="#A.1">Aartifact</a></li>
   <li><a href="#A.2">Mathematica/Wolfram Alpha</a></li>
   <li><a href="#A.3">MATLAB</b></a></li>
  </ul>-->
 </li>
</ul>

<a name="lecture1"></a>
<a name="1"></a>
<hr style="margin-bottom:80px;"/>
<h2><span class="secn">1.</span> Introduction and Motivation</h2>

This section introduces at a high level the direction and goals of this course, as well as some terminology.
Do not worry if all the details are not clear; all this material will be covered in greater detail and with many more examples during the course.

<a name="1.1"></a>
<h3><span class="secn">1.1.</span> Models, systems, and system states</h3>

When many real-world problems are addressed or solved mathematically and computationally, the details of those problems are abstracted away until they can be represented directly as idealized mathematical structures (e.g., numbers, sets, matrices, and so on). We can talk
about this distinction by calling the real-world problems <i>systems</i> (i.e., organized collections of interdepentend components) that have
distinguishable <i>system states</i>, and the abstract versions of these systems that only capture some parts or aspects of the different
system states as <i>models</i>. Since models are abstractions, we must also consider the <i>interpretation</i> of a model that explains how
the different parts of the model are related to the different parts of the system (i.e., what each part of the model represents).

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> We represent the set of all real numbers using the symbol \R.
\begin{eqnarray}
  \R & = & { -1, 1.2, 3, 4, 1/3, \pi, %e, ... }
\end{eqnarray}
</div>

The set of real numbers is a very simple kind of model that can be used to represent
quantities or, by considering quantities and fractions of units (e.g., inch, meter, kilogram, and so on), a magnitude.
If we adopt \R as our model of a system, we can then represent individual system states using individual real numbers.

<table class="fig_table">
 <tr>
  <td><b>model<br/>(language for<br>describing<br>system states)</b></td>
  <td><b>example of a<br/>system state<br/>in the model</b></td>
  <td><b>interpretation</b></td>
  <td><b>system</b></td>
 </tr>
 <tr> 
  <td>\R</td>
  <td>7</td>
  <td>number of giraffes</td>
  <td>zoo</td>
 </tr>
 <tr> 
  <td>\R</td>
  <td>1</td>
  <td>distance in AU between the two objects</td>
  <td>the Earth-Sun system</td>
 </tr>
 <tr> 
  <td>\R</td>
  <td>5.6</td>
  <td>temperature in Celsius</td>
  <td>weather in Boston</td>
 </tr>
</table>

Thus, real numbers are a very simple symbolic language for modelling systems. Furthermore, this language allows us to describe characteristics of
systems concisely ("7" is much more concise than a drawing of seven giraffes).

<a name="1.2"></a>
<h3><span class="secn">1.2.</span> A language of system states: terms and their algebraic properties</h3>

Real numbers can be used to characterize the quantities or magnitudes of parts of a system, but this is often not enough.
We may want to capture within our models certain kinds of changes that might occur to a system. In order to do so, we must
add something to our language of models: <i>binary operators</i> such as +, -, \cdot, and so on.

<table class="fig_table">
 <tr>
  <td><b>model<br/>(language for<br>describing<br>system states)</b></td>
  <td><b>a system state<br/>in the model</b></td>
  <td><b>interpretation</b></td>
  <td><b>system</b></td>
 </tr>
 <tr>
  <td>\R with addition (+)</td>
  <td>3</td>
  <td>number of apples in one of the apple baskets</td>
  <td>a collection of apple baskets</td>
 </tr>
 <tr>
  <td>\R with addition (+)</td>
  <td>3 + 2</td>
  <td>number of apples in two of the apple baskets</td>
  <td>a collection of apple baskets</td>
 </tr>
 <tr>
  <td>\R with addition (+)</td>
  <td>2 + 3</td>
  <td>number of apples in two of the apple baskets</td>
  <td>a collection of apple baskets</td>
 </tr>
 <tr>
  <td><b>symbolic language</b></td>
  <td><b>symbol string<br/>(a.k.a., "term")<br/>in the language</b></td>
  <td><b>meaning of symbol string</b></td>
  <td><b>system</b></td>
 </tr>
</table>

The different parts of this language for models have technical names. So far, we have seen real numbers and operators. We can build up
larger and larger symbol strings using operators and real numbers. These are all called <i>terms</i>.

<div class="mathenv proposition_to_know">
<b>Definition:</b> The following table defines the collection of symbol strings corresponding to <i>terms</i>.

<table class="fig_table">
 <tr>
  <td><b>term</b></td>
  <td></td>
 </tr>
 <tr> 
  <td>0</td>
  <td></td>
 </tr>
 <tr> 
  <td>1.2</td>
  <td></td>
 </tr>
 <tr> 
  <td>%x</td>
  <td>%x is a real number</td>
 </tr>
 <tr> 
  <td>%t_1 + %t_2</td>
  <td>if %t_1 and %t_2 are terms</td>
  </td>
 </tr>
 <tr> 
  <td>%t_1 --- %t_2</td>
  <td>if %t_1 and %t_2 are terms</td>
 </tr>
 <tr> 
  <td>--- %t</td>
  <td>if %t is a term</td>
 </tr>
 <tr> 
  <td>%t_1 \cdot %t_2</td>
  <td>if %t_1 and %t_2 are terms</td>
 </tr>
</table>

Note that in these notes, we use the multiplication symbols \cdot and * interchangeably, or omit them entirely.

</div>

Notice that when our language for models only had real numbers, each symbol corresponded to a unique system state. However, by introducing
operators into our language for describing system states, we are now able to write down multiple symbol strings that represent the same
system state (e.g., 2 + 3 and 3 + 2). In other words, "2 + 3" and "3 + 2" <i>have the same meaning</i>.

<div class="mathenv example_to_know">
<b>Exercise:</b> List as many of the algebraic properties of the operators + and \cdot over the real numbers as you can.

For all real numbers %x \in \R, %y \in \R, and %z \in \R, it is true that:
\begin{eqnarray}
 %x + %y & = & %y + %x \\
 %x + (%y + %z) & = & (%x + %y) + %z\\
 %x + 0 & = & %x \\
 %x \cdot %y & = & %y \cdot %x \\
 %x \cdot (%y \cdot %z) & = & (%x \cdot %y) \cdot %z\\
 %x \cdot 1 & = & %x \\
 %x \cdot (%y + %z) & = &  (%x \cdot %y) + (%x \cdot %z)\\
 %x + (-%x) & = & 0
\end{eqnarray}
</div>

<a name="1.3"></a>
<h3><span class="secn">1.3.</span> Extending the language of models: vectors and matrices</h3>

Real numbers and operators are still not sufficient to capture many interesting aspects of real-world systems: they can only capture quantities,
one-dimensional magnitudes (e.g., distance). What if we want to capture multidimensional magnitudes (e.g., temperature and pressure)
or relationships between different quantities? These can correspond to points, lines, planes, spaces; relationships and changes within
such systems can be modelled as translations, rotations, reflections, stretching, skewing, and other transformations. 

<b>Example:</b> Suppose we have $12,000 and two investment opportunities: one has an annual return of 10%, and the other has an annual return
of 20%. How much should we invest in each opportunity to get exactly $1800 over one year?

The above problem can be represented as a system in which two magnitudes %x \in \R and %y \in \R have a relationship. Typically, this is
represented using a system of equations:
\begin{eqnarray}
 %x + %y & = & 12000 \\
 0.1 %x + 0.2 %y & = & 1800
\end{eqnarray}
The possible solutions to this system are the collection of all pairs of real numbers that can be assigned to the pair (%x,%y). These are the
possible system states. Notice that these can be interpreted directly as points on a plane.

How does the above example suggest we might extend our language for system states in a model? We might consider <i>ordered pairs</i> (%x,%y) of
real numbers. More generally, we might consider <i>ordered lists</i> (%x_1,...,%x_%n) of real numbers. In this course, 
we will introduce a few new kinds of terms into our language for models that can help us represent these sorts of relationships within systems:
vectors and matrices. We will also study the algebraic properties of this extended language.

<table class="fig_table">
 <tr>
  <td><b>new term<br/>language construct</b></td>
  <td><b>what it represents</b></td>
 </tr>
 <tr> 
  <td>vectors</td>
  <td>system state in the model</td>
 </tr>
 <tr> 
  <td>matrix</td>
  <td>transitions between system states</td>
 </tr>
 <tr> 
  <td>matrix</td>
  <td>changes to system states</td>
 </tr>
 <tr> 
  <td>matrix</td>
  <td>relationships between system states</td>
 </tr>
 <tr> 
  <td>matrix multiplication</td>
  <td>composition of transformations of system states</td>
 </tr>
</table>

<a name="1.4"></a>
<h3><span class="secn">1.4.</span> Review: sets and formulas</h3>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The symbols <b>and</b>, <b>or</b>, and <b>not</b> are <i>logical operators</i>. The symbols <b>\forall</b> and <b>\exists</b> are
<i>quantifiers</i>, where <b>\forall</b> is read "for all" and is called the <i>universal quantifier</i>, while <b>\exists</b> is read "exists"
and is called the <i>existential quantifier</i>.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Strings of mathematical symbols that can be either true or false are called <i>formulas</i>. The following
table describes how formulas can be built up using logical operators and logical quantifiers, and their meanings.

<table class="fig_table">
 <tr>
  <td><b>formula</b></td>
  <td><b>conditions</b></td>
  <td><b>meaning</b></td>
 </tr>
 <tr> 
  <td><b>true</b></td>
  <td></td>
  <td>always true</td>
 </tr>
 <tr> 
  <td><b>false</b></td>
  <td></td>
  <td>always false</td>
 </tr>
 <tr> 
  <td>%t_1 = %t_2</td>
  <td>%t_1 and %t_2 are terms</td>
  <td>true only if the meaning (e.g., system state)<br/>of %t_1 and %t_2 is the same</td>
 </tr>
 <tr> 
  <td>%t_1 < %t_2</td>
  <td>%t_1 and %t_2 are terms</td>
  <td>true only if %t_1 and %t_2 represent real numbers,<br/>and %t_1 is less than %t_2</td>
 </tr>
 <tr> 
  <td>%f_1 <b>and</b> %f_2</td>
  <td>if %f_1 and %f_2 are formulas</td>
  <td>only true if both %f_1 and %f_2 are true;<br/>otherwise, it is false</td>
  </td>
 </tr>
 <tr> 
  <td>%f_1 <b>or</b> %f_2</td>
  <td>if %f_1 and %f_2 are formulas</td>
  <td>true if %f_1, %f_2, or both are true;<br/>only false if both are false</td>
 </tr>
 <tr> 
  <td><b>not</b> %f</td>
  <td>if %f is a formula</td>
  <td>true if %f is false;<br/>false if %f is true</td>
 </tr>
 <tr> 
  <td>%f_1 <b>implies</b> %f_2</td>
  <td>if %f_1 and %f_2 are formulas</td>
  <td>only true if %f_2 is true whenever %f_1 is true,<br/><i>&nbsp;&nbsp;&nbsp;or equivalently,</i><br/>only true if %f_1 is false or %f_2 is true</td>
  </td>
 </tr>
 <tr> 
  <td>%f_1 <b>iff</b> %f_2</td>
  <td>if %f_1 and %f_2 are formulas</td>
  <td>only true if %f_1 and %f_2 are both true,<br/>or %f_1 and %f_2 are both false</td>
  </td>
 </tr>
 <tr> 
  <td>\forall %x \in %S, \ %f</td>
  <td>if %S is a set and %f is a formula</td>
  <td>true only if taking for <i>every</i> element of %S,<br/> replacing %x inside %f with that element makes %f true</td>
 </tr>
 <tr> 
  <td>\exists %x \in %S, \ %f</td>
  <td>if %S is a set and %f is a formula</td>
  <td>true only if there is <i>at least one</i> element of %S<br/>that can replace %x inside %f so that %f is true</td>
 </tr>
</table>
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The symbols =, &lt;, &gt;, \leq, \geq, \neq, and \in are
<i>relational operators</i> or <i>relations</i>. Given two terms, %t_1 and %t_2, apply any of these operators to the terms produces
a formula that is either true of false (e.g., "5 \leq 6").
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The relational operator = (the <i>equality relation</i> or just <i>equality</i>) has the following properties.

<table class="fig_table">
 <tr>
  <td><b>property</b></td>
  <td><b>definition</b></td>
  <td><b>example</b></td>
 </tr>
 <tr> 
  <td>reflexivity</td>
  <td>for any term %t, &nbsp; %t = %t</td>
  <td>

@

\forall %x \in \R,
  %x = %x
/@

  </td>
 </tr>
 <tr> 
  <td>symmetry</td>
  <td>for any terms %t_1 and %t_2, &nbsp; %t_1 = %t_2 implies %t_2 = %t_1</td>
  <td>

@

\forall %x,%y \in \R,
    %x = %y
  \implies
    %y = %x
/@

  </td>
 </tr>
 <tr> 
  <td>transitivity</td>
  <td>for any terms %t_1, %t_2, and %t_3, &nbsp; %t_1 = %t_2 and %t_2 = %t_3 implies %t_1 = %t_3</td>
  <td>

@

\forall %x,%y,%z \in \R,
    %x = %y
    %y = %z
  \implies
    %x = %z
/@

  </td>
 </tr>
</table>

The above properties always apply to =. The properties below are assumptions we
make in this course (they are extensions of the definition of equality):
<ul>
<li>
<b>equality of vectors</b>: for any terms %t_1, %t_2, and %t_3, &nbsp;&nbsp; %t_1 = %t_3 and %t_2 = %t_4 &nbsp;&nbsp; iff
\begin{eqnarray}
  #[ %t_1 #; %t_2 #] & = & #[ %t_3 #; %t_4 #]
\end{eqnarray}
</li>
<li>
<b>replacement</b>: 
for any terms %t_1, %t_2, and %t_3, &nbsp;&nbsp; %t_1 = %t_2 &nbsp;&nbsp; iff
\begin{eqnarray}
  #[ %t_1 #; %t_3 #] & = & #[ %t_2 #; %t_3 #] \\
  #[ %t_3 #; %t_1 #] & = & #[ %t_3 #; %t_2 #]
\end{eqnarray}
</li>
</ul>
</div>

<a name="2"></a>
<hr style="margin-bottom:120px;"/>
<h2><span class="secn">2.</span> Vectors</h2>

In this section we consider a particular kind of term: a <i>vector</i>. A vector can have many interpretations, but the most common
is that of a point in a geometric space. We first define a notation for names of vectors. Vectors will be written using any
of the the following equivalent notations:

  $$ (2,3) \ [2;3] \  
     <table cellpadding="0" cellspacing="0" style="display:inline;">
      <tr>
        <td class="html_matrix_lft">&nbsp;</td>
        <td>
          <table cellpadding="0" cellspacing="0" style="font-size:12px;">
            <tr><td style="white-space:nowrap;">2</td></tr>
            <tr><td style="white-space:nowrap;">3</td></tr>
          </table>
        </td>
        <td class="html_matrix_rgt">&nbsp;</td>
      </tr>
    </table>
    \ (%x, %y, %z) \ [%x; %y; %z] \ 
    <table cellpadding="0" cellspacing="0" style="display:inline;">
      <tr>
        <td class="html_matrix_lft">&nbsp;</td>
        <td>
          <table cellpadding="0" cellspacing="0" style="font-size:12px;">
            <tr><td style="white-space:nowrap;"><i>x</i></td></tr>
            <tr><td style="white-space:nowrap;"> <i>y</i></td></tr>
            <tr><td style="white-space:nowrap;"> <i>z</i></td></tr>
          </table>
        </td>
        <td class="html_matrix_rgt">&nbsp;</td>
      </tr>
    </table>$$

<a name="2.1"></a>
<h3><span class="secn">2.1.</span> Defining operations on vectors</h3>

We have introduced a new kind of term (vectors) with their own corresponding notation.
As with \R, the set of real numbers, we define symbol to represent the sets of vectors.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> We represent the set of all vectors with two components using the symbol \R^2:
\begin{eqnarray}
  \R^2 & = & { #[ %x #; %y #] \ | \ %x \in \R, %y \in \R }
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> For positive %n \in \N, we represent the set of all vectors with %n components using the symbol \R^{%n}:
\begin{eqnarray}
  \R^{%n} & = & { #[ %x_1 #; \vdots #; %x_%n #] \ | \ %x_1 \in \R, ..., %x_%n \in \R }
\end{eqnarray}
</div>

We will also use symbols to name some geometric manipulations of vectors. One such operation is addition. Suppose we treat vectors as paths
from (0,0), so that (2,3) is the path from (0,0) to (2,3) and (1,2) is the path from (0,0) to (1,2). Then vector addition would represent
the final destination if someone walks first along the length and direction specified by one vector, and then from that destination along
the length and direction specified by the other. The following definition for the operation + and vectors of two components corresponds
to this intuition.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The following formula is assumed to be true; it defines what operation the symbol + represents when applied to two vectors.
We call + <i>vector addition</i> in this context.

@
\forall %x, %y, %x', %y' \in \R,
  %[
  [%x; %y] + [ %x'; %y'] & = & [%x + %x'; %y + %y']
  %]
/@

</div>

Notice that we have defined an entirely new operation; we are merely reusing (or "overloading") the + symbol to represent this operation.
We cannot assume that this operation has any of the properties we normally associate with addition of real numbers. However, we do know
that according to our interpretion of vector addition (walking along one vector, then along the other from that destination), this operation
should be commutative. Does our symbolic definition conform to this interpretation? If it does, then we should be able to show that
[%x;%y] + [%x';%y'] and [%x+%x';%y+%y'] are names for the <i>same</i> vector. Using the commutativity of the real numbers and our definition
above, we can indeed write the proof of this property.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Vector addition is commutative.

@
\forall %x, %y, %x', %y' \in \R,
  %[
  [%x; %y] + [%x'; %y'] & = & [%x + %x'; %y + %y'] \\
                     `` & = & [%x' + %x; %y' + %y] \\
                     `` & = & [%x'; %y'] + [%x; %y]  
  %]
/@

</div>

<a name="lecture2"></a>

Recall that we can view multiplication by a number as repeated addition. Thus, we could use this intuition and our new notion of vector addition
defined above to define multiplication by a real number; in this context, it will be called a <i>scalar</i>, and the operation is known as
<i>scalar multiplication</i>.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The following formula is assumed to be true; it defines what operation the symbol \cdot represents when applied to 
a <i>scalar</i> (a real number) %s \in \R  and a vector. We call \cdot <i>scalar multiplication</i> in this context.

@
\forall %s,%x,%y \in \R,
  %[
  %s * [%x; %y] = [%s * %x; %s * %y]
  %]
/@

Note that, as with multiplication, the symbol is sometimes omitted.
</div>

Scalar multiplication has some of the intuitive algebraic properties that are familiar to us from our experience with \R. Below, we provide
proofs of a few.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The following fact is true for scalar multiplication.

@
\forall %s, %t, %x, %y \in \R,
  %[
  %s (%t [%x; %y]) & = & %s ([%t %x; %t %y]) \\
           ``  & = & [ %s (%t %x); %s (%t %y) ] \\
           ``  & = & [ (%s %t) %x; (%s %t) %y ] \\
           ``  & = & [ (%t %s) %x; (%t %s) %y ] \\
           ``  & = & [ %t (%s %x); %t (%s %y) ] \\
           ``  & = & %t [ %s %x; %s %y ] \\
           ``  & = & %t (%s ( [%x; %y])) 
  %]

  # alternatively, we could also derive...
                                 
  %[
 [ %s (%t %x); %s (%t %y) ] & = & [ (%s %t) %x; (%s %t) %y ] \\
                       ``   & = & (%s %t) [%x;%y]
  %]
/@

</div>

Once we have defined scalar multiplication, we can now assign a meaning to the negation operator (interpreting -%v for any vector %v \in \R^2 as
referring to the scalar multiplication ---1 \cdot %v).

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The following formula is assumed to be true; it defines what operation the symbol --- represents when applied to 
a vector.

@
\forall %x,%y \in \R,
  %[
  - [%x; %y] = [- %x; - %y]
  %]
/@

</div>

The above definition allows us to define vector subtraction: for two vectors %v and %w \in \R^2, %v - %w = %v + (-%w).

Notice that we did not define vector division. In fact, division by a vector is <i>undefined</i> (in the same way that division by 0 is undefined).
However, we can prove a <i>cancellation</i> law for vectors.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Let [%x; %y] = %v \in \R^2 be a vector <b>with at least one nonzero real number component</b>,
and let %a,%b \in \R be scalars. Suppose that %a \cdot %v = %b \cdot %v. If %x is a nonzero component, then we have that:

\begin{eqnarray}
  %a \cdot %v & = & %b \cdot %v \\
  %a \cdot #[ %x #; %y #] & = &   %b \cdot #[ %x #; %y #] \\
  %a \cdot %x & = & %b \cdot %x \\
  %a & = & %b
\end{eqnarray}

Notice that we used division by %x, a real number.
</div>

<a name="2.2"></a>
<h3><span class="secn">2.2.</span> Properties of vector operations </h3>

In this course, we will study collections of vectors that have particular properties. When a collection of vectors satisfies these properties,
we will call it a <i>vector space</i>. We list these eight properties (in the form of equations) below. These must hold for any %u, %v, and %w
in the collection, and \0 must also be a vector in the collection.

\begin{eqnarray}
  %u + (%v + %w) & = & (%u + %v) + %w \\
       %u + %v  & = & %v + %u \\
      \0 + %v  & = & %v \\
       %v + (-%v) & = & \0 \\
        1 * %v & = & %v \\     
 %s * (%u + %v) & = & (%s * %u) + (%s * %v)\\
 (%u + %v) * %s & = & (%u * %s) + (%v * %s)\\
 %s * (%t * %u) & = & (%s * %t) * %u
\end{eqnarray}

Note that these are not unique; we could have specified an alternative equation for the additive identity.

  $$ %v + \0 = %v $$

Each equation can be derived from the other using the commutativity 

  $$ %v + \0 = \0 + %v $$

These eight equations can be called <i>axioms</i> (i.e., assumptions) when we are studying vector spaces without thinking about the internal
representation of vectors. For example, the above derivation of the alternative additive identity equation is based entirely on the axioms
and will work for any vector space. However, to show that some <i>other</i> structure, such as \R^2, is a vector space we must prove that these
properties are satisfied. Thus, to define a new vector space, we usually must:
<ol>
<li>define a way to construct vectors;</li>
<li>define a vector addition operator + (i.e., how to add vectors that we have constructed);</li>
<li>specify the vector that is the additive identity (call it <b>0</b>);</li>
<li>define vector inversion: a process for constructing the inverse of a vector;</li>
<li>prove that the eight properties of a vector space are satisfied by the vectors, +,0, and *.
</ol>

<div class="mathenv example_to_know">
<b>Example:</b> Assuming the following equation is true, solve for %x \in \R and %y \in \R:
\begin{eqnarray}
  #[ %x #; 10 #] & = & 2 \cdot #[ 4 #; %y #]
\end{eqnarray}

@
\forall %x, %y \in \R,
    %[
    [%x; 10] & = & 2 * [4; %y] \\
    %]
  \implies
    %[
        [%x; 10]  & = & [2 * 4 ; 2 * %y] \\
             ``  & = & [8 ; 2 * %y] \\
              %x  & = & 8 \\
             10  & = & 2 * %y \\
      (0.5) * 10 & = & 0.5 * (2 * %y) \\
               5 & = & (0.5 * 2) * %y \\
               5 & = & 1 * %y \\
               5 & = & %y \\
               %y & = & 5
    %]
/@

</div>

<div class="mathenv example_to_know">
<b>Example:</b> Assuming the following equation is true, solve for %x \in \R and %y \in \R:
\begin{eqnarray}
  #[ %x #; 2 \cdot %x #] & = & 5 \cdot #[ 2 \cdot %y #; 20 #]
\end{eqnarray}

@
\forall %x, %y \in \R,
    %[
    [%x; 2 * %x] & = & 5 * [2 * %y; 20] \\
    %]
  \implies
    %[
        [%x; 2 * %x] & = & 5 * [2 * %y; 20] \\
                `` & = & [5 * (2 * %y); 5 * 20] \\
        [%x; 2 * %x] & = & [(5 * 2) * %y; 100] \\
             2 * %x & = & 100 \\
       0.5 (2 * %x) & = & 0.5 * 100 \\
       (0.5 * 2) %x & = & 50 \\
             1 * %x & = & 50 \\
                 %x & = & 50 \\
                 %x & = & 10 * %y \\
          0.1 * 50 & = & 0.1 * (10 * %y) \\
                `` & = & (0.1 * 10) * %y \\
                `` & = & 1 * %y \\
                `` & = & %y \\
                 %y & = & 5
    %]
/@

</div>

<!--assignment1-->
<br/><hr/>
<a name="2.3"></a>
<a name="assignment1"></a>
<h3><span class="secn">2.3.</span>
  <b>Assignment #1: Vector Algebra</b> <!--span class="btn_assignment">(<a href="materials.php?hw=1">show only this assignment</a>)</span-->
</h3>


       <p>In this assignment you will perform step-by-step algebraic manipulations involving vectors and vector operations. In doing
       so, you will assemble machine-verifiable proofs of algebraic facts. 
       <b style="color:firebrick;">Your proofs must be automatically verifiable using the <a href="aartifact/">proof verifier</a>.</b>
       <b>Please submit a single text file <code>a1.txt</code>
       containing your proof scripts for each of the problem parts below.</b>

       <p> Working with machine verification can be frustrating; minute operations that are normally implied must often be explicit.
       However, the process should familiarize you with common practices of rigorous algebraic reasoning in mathematics: finding, applying,
       or expanding definitions to reach a particular formula that represents a desired argument or solution.</p>

<ol>
  <li>
    <ol style="list-style-type:lower-alpha;">
      <li> Finish the proof below by solving for <i>a</i> using a sequence of permitted algebraic manipulations.

@
\forall %a, %b \in \R,
    %[
    [3; %a] = [%b; %b + 8]
    %]
  \implies
    <comment> ... put proof steps here ... </comment>
    %[
    %a = \?  <comment> replace "?" with an integer </comment>
    %]
/@

      </li>
      
      <li> Finish the proof below by solving for <i>b</i> in terms of <i>a</i>.

@
\forall %a,%b \in \R,
  %[
    [b; 38] = [3; 7] a  + [0; 4] b
  %]
  \implies
    <comment> ... put proof steps here ... </comment>
    %[
    %b = \?  <comment> replace "?" with a term containing "a" </comment>
    %]
/@

      </li>
      <li> Finish the proof below to show that [1;2] and [-2;1] are linearly independent.

@
%[
[1; 2] * [-2; 1] = 1 * -2 + 2 * 1 \and
%]
  <comment> ... put proof steps here ... </comment>
  %[
  `([1; 2]) and ([-2; 1]) are orthogonal`
  %]
/@

      </li>
      <li> Solve for <i>x</i> (the solution is an integer).

@
\forall %x \in \R,
    %[
    `([%x; 4]) and ([1; 1]) are orthogonal`
    %]
  \implies
    <comment> ... put proof steps here ... </comment>
    %[
    %x = \?
    %]
/@

      </li>
      <li> Show that no [%x; %y] exists satisfying both of the below properties by deriving a contradiction (e.g., <code>1 = 0</code>).

@
\forall %x,%y \in \R,
    %[
    `([%x; %y]) is a unit vector` \\
    `([%x*%x; %y*%y]) and ([1; 1]) are  orthogonal`
    %]
  \implies
    <comment> ... put proof steps here ... </comment>
    %[
    \? <comment> this should be a contradiction </comment>
    %]
/@

      </li>
    </ol>
  </li>
  <li> We have shown in lecture that \R^2, together with vector addition and scalar multiplication, satisfies some of the vector space axioms.
       In this problem, you will show that the remaining axioms are satisfied.
    <ol style="list-style-type:lower-alpha;">
      <li> Finish the proof below showing that [0; 0] is a <i>left</i> identity for addition of vectors in \R^2.

@
\forall %x,%y \in \R,
  <comment> ... put proof steps here ... </comment>
  %[
  [0; 0] + [%x; %y] = [%x; %y]
  %]
/@

      </li>
      <li> Finish the proof below showing that [0; 0] is a <i>right</i> identity for addition of vectors in \R^2.

@
\forall %x,%y \in \R,
  <comment> ... put proof steps here ... </comment>
  %[
  [%x; %y] + [0; 0] = [%x; %y]
  %]
/@

      </li>
      <li> Finish the proof below showing that 1 is the identity for scalar multiplication of vectors in \R^2.

@
\forall %x,%y \in \R,
  <comment> ... put proof steps here ... </comment>
  %[
  1 * [%x; %y] = [%x; %y]
  %]
/@

      </li>
      <li> Finish the proof below showing that the component-wise definition of inversion is consistent with the inversion axiom for vector spaces.

@
\forall %x,%y \in \R,
  <comment> ... put proof steps here ... </comment>
  %[
  [%x; %y] + (-[%x; %y]) = [0; 0]
  %]
/@

      </li>
      <li> Finish the proof below showing that the addition of vectors in \R^2 is associative.

@
\forall %a,%b,%c,%d,%e,%f \in \R,
  <comment> ... put proof steps here ... </comment>
  %[
  [%a; %b] + ([%c; %d] + [%e; %f]) = ([%a; %b] + [%c; %d]) + [%e; %f]
  %]
/@

      </li>
      <li> Finish the proof below showing that the distributive property applies to vector addition and scalar multiplication for \R^2.

@
\forall %s, %x, %y, %x', %y' \in \R,
  <comment> ... put proof steps here ... </comment>
  %[
  %s ([%x; %y] + [%x'; %y']) = (%s [%x; %y]) + (%s [%x'; %y'])
  %]
/@

      </li>
    </ol>
  </li>
  <li> <p>Any point %p on the line between vectors %u and %v can be expressed as %a(%u-%v)+%u for some scalar %a. In fact, it can also be
       expressed as %b(%u-%v)+%v for some other scalar %b. In this problem, you will prove this fact for \R^3.</p>
       
       <p>Given some %p = %a(%u-%v)+%u, find a formula for %b in terms of %a so that %p = %b(%u-%v)+%v. Add this formula to the beginning
       of the proof (after <code>a = </code>) and then complete the proof.</p>

@
\forall %a,%b \in \R, \forall %u,%v,%p \in \R^3,
    %[
    %p & = & %a (%u - %v) + %u \\
    %a & = & \? <comment> define a in terms of b </comment>
    %]
  \implies
    <comment> ... put proof steps here ... </comment>
    %[
    %p = %b (%u - %v) + %v
    %]
/@

       <p>The verifier's library contains
       a variety of derived algebraic properties for \R and \R^3 in addition to the vector space axioms for \R^3. Look over them
       to see which might be useful.</p>
  </li>
</ol>
<hr/><br/>
<!--/assignment1-->

<a name="2.4"></a>
<h3><span class="secn">2.4.</span> Common vector properties, relationships, and operators </h3>

We introduce two new operations on vectors (e.g. %u and %v): the norm (|| ... ||) and the dot product (%u \cdot %v). When interpreting some other structure,
such as \R^2, as a vector space, we must provide definitions for these operators in order to use them.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The operator \cdot when applied to two vectors with the same number of components is called the <i>dot product</i>, and
is defined for %x,%y,%x',%y' \in \R as:
\begin{eqnarray}
 [%x; %y] \cdot [%x'; %y'] & = & %x*%x' + %y*%y'
\end{eqnarray}
The operator || ... || when applied to a vector is called the <i>norm</i> and is defined as:
\begin{eqnarray}
 || [%x;%y] || & = & &radic;(%x*%x + %y*%y)
\end{eqnarray}
The norm || %v || of a vector %v represents its length in Euclidean space.
</div>

Notice that the dot product is a new, distinct form of multiplication (distinct from multiplication of real numbers, and distinct from scalar multiplication). Also
notice that the two operations are related:

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The following equation is true for all %x,%y,%x',%y' \in \R:
\begin{eqnarray}
  || [%x; %y] || & = & &radic;(%x*%x + %y*%y) = &radic;([%x; %y] \cdot [%x; %y])
\end{eqnarray}
</div>

These operations can be shown to have various algebraic properties.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The dot product is commutative. Notice that below, the dot product is a real number.

@
\forall %a,%b,%c,%d \in \R,
  %[
  [%a; %b] * [%c; %d] & = & %a*%c + %b*%d  \\
               `` & = & %c*%a + %d*%b  \\
               `` & = & [%c; %d] * [%a; %b]
  %]
/@

</div>

We also introduce several vector properties. Some deal with a single vector; some deal with two vectors; and some deal with three vectors.

<a name="lecture3"></a>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The table below summarizes the definitions of several vector properties and relationships and how they are related in some 
cases to vector operators and associated algebraic properties.

<table class="fig_table">
 <tr>
  <td><b>property</b></td>
  <td><b>definition(s)</b></td>
  <td><b>algebraic properties for \R^2 <br/>%u = [%x;%y], %v = [%x',%y'], %w = [%x'',%y'']</b></td>
 </tr>
 <tr> 
  <td>%v has length %s</td>
  <td>||v|| = %s or<br/> &radic;(%v \cdot %v) = %s</td>
  <td>||v|| = &radic;(%x*%x + %x*%y) = &radic;([%x,%y] \cdot [%x,%y])</td>
 </tr>
 <tr>
  <td>%v is a unit vector</td>
  <td>||%v|| = 1 or <br/> %v \cdot %v = 1</td>
  <td>1 = ||v|| = &radic;(%x*%x + %x*%y) = &radic;([%x,%y] \cdot [%x,%y]) <br/> 1 = ||v|| = %x*%x + %x*%y = [%x,%y] \cdot [%x,%y]</td>
 </tr>
 <tr>
  <td>%u and %v are linearly dependent <br/> %u and %v are collinear</td>
  <td>&exist; %a \in \R, %a \cdot %u = %v</td>
  <td>%y/%x = %y'/%x'<br/>(the vectors have the same slope)</td>
 </tr>
 <tr>
  <td>%u and %v are linearly independent</td>
  <td>\forall %a \in \R, %a \cdot %u \neq %v<br/><i>&nbsp;&nbsp;&nbsp;or equivalently,</i><br/><b>not</b> (\exists %a \in \R, %a \cdot %u = %v)</td>
  <td>%y/%x \neq %y'/%x'<br/>(the vectors have different slopes)</td>
 </tr>
 <tr>
  <td>%u and %v are orthogonal</td>
  <td>%u \cdot %v = 0</td>
  <td>%y/%x = -%x'/%y'</td>
 </tr>
 <tr>
  <td>%w is a projection of %v onto %u</td>
  <td>%w = (%v \cdot (%u/||%u||)) \cdot %u/||%u||</td>
  <td></td>
 </tr>
 <tr>
  <td>%d is the (Euclidean) distance<br/>between %v and %w</td>
  <td>%d = ||%u - %v|| = ||%v - %u||</td>
  <td>%d = \sqrt((%x - %x')^2 + (%y - %y')^2)</td>
 </tr>
 <tr>
  <td>%L is the unique line<br/>parallel to %v \in \R^2</td>
  <td>%L = { %a \cdot %v | %a \in \R }<br/>%L = { %p | \exists %a \in \R, %p = %a \cdot %v }</td>
  <td>{ [%x',%y'] &nbsp; | &nbsp; %y' = %m %x' } where %m = %y/%x</td>
 </tr>
 <tr>
  <td>%L is the unique line<br/>orthogonal to %v \in \R^2</td>
  <td>%L = { %w | %v \cdot %w = 0 }</td>
  <td></td>
 </tr>
 <tr>
  <td>%L is the unique line<br/>defined by the<br/>two points %u \in \R^2 and %v \in \R^2</td>
  <td>%L = { %a (%u - %v) + %u | %a \in \R }<br/>%L = { %p | \exists %a \in \R, %p = %a (%u - %v) + %u }</td>
  <td></td>
 </tr>
 <tr>
  <td>%P is the unique plane<br/>orthogonal to %v \in \R^3</td>
  <td>%P = { %w | %v \cdot %w = 0 }</td>
  <td></td>
 </tr>
 <tr>
  <td>%P is the unique plane<br/>of linear combinations of<br/>%v, %w \in \R^3 where %v and %w are<br/>linearly independent</td>
  <td>%P = { %a %v + %b %w | %a \in \R, %b \in \R }</td>
  <td></td>
 </tr>
 <tr>
  <td>%w is a linear combination of %u and %v</td>
  <td>&exist; %a,%b \in \R, %w = %a%u + %b%v</td>
  <td></td>
 </tr>
 <tr>
  <td>{%u, %v, %w} are linearly independent</td>
  <td>
    <b>not</b> (%u is a linear combination of %v and %w) <b>and</b><br/>
    <b>not</b> (%v is a linear combination of %u and %w) <b>and</b><br/>
    <b>not</b> (%w is a linear combination of %u and %v)
  </td>
  <td></td>
 </tr>
</table>
</div>

In \R^2, we can derive the linear independence of [%x;%y] and [%x';%y'] from the orthogonality of [%x;%y] and [%x';%y'] using a proof
by contradiction. 

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If [%x;%y] and [%x';%y'] are orthogonal (and have nonzero length), then they are linearly independent.

Suppose that [%x;%y] and [%x';%y'] are both orthogonal and linearly dependent. Since they are linearly dependent, we have that
there exists an %a \in \R such that:
\begin{eqnarray}
  #[ %x' #; %y' #] & = & %a \cdot #[ %x #; %y #] \\
  %x' & = & %a \cdot %x \\
  %y' & = & %a \cdot %y
\end{eqnarray}
Since they are orthogonal, we have that:
\begin{eqnarray}
 #[ %x #; %y #] \cdot #[ %x' #; %y' #] & = & 0 \\
 #[ %x #; %y #] \cdot #[ %a \cdot %x #; %a \cdot %y #] & = & 0 \\
 %x \cdot (%a \cdot %x) + %y \cdot (%a \cdot %y) & = & 0 \\
 %a \cdot (%x^2 + %y^2) & = & 0 \\
 %x^2 + %y^2 & = & 0 \\
 %x^2 & = & - %y^2 \\
 (%x^2)/(%y^2) & = & - 1 \\
 (%x/%y)^2 & = & - 1 \\
 %x/%y & = & &radic;(-1)
\end{eqnarray}
No real numbers %y and %x satisfy the above equation, so we must have introduced a contradiction by supposing that [%x;%y] and [%x';%y'] 
are <i>not</i> linearly independent. Thus, they must be linearly independent.

Also, notice that [0;0] is the only solution to %x^2 + %y^2 = 0. Thus, <i>all</i> vectors in
\R^2 are linearly dependent, linearly independent, <i>and</i> orthogonal with [0;0].
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If %v \in \R^2 is a vector that has nonzero length,
then we know that the vector %v/||%v||: (1) is linearly dependent with %v, and (2) is a unit vector.

<ol>
  <li>Because %v is a vector, then ||%v|| is a real number that is nonzero, and 1/||%v|| is a real number. By our definition of
      linear dependence, (1/||%v||) \cdot %v = %v/||%v|| is linearly dependent with %v because 1/||%v|| exists.
  </li>
  <li>Let %v = [%x;%y]. We have that:
\begin{eqnarray}
     1/||%v|| & = &  1 / ||[%x;%y]|| \\
              & = & 1 / \sqrt(%x^2 + %y^2)
\end{eqnarray}
Then, we have that:
\begin{eqnarray}
 (1 / \sqrt(%x^2 + %y^2)) \cdot #[ %x #; %y #]  & = & #[ %x / \sqrt(%x^2 + %y^2) #; %y / \sqrt(%x^2 + %y^2) #]
\end{eqnarray}
If we take the norm of the above vector, we have:
\begin{eqnarray}
 || #[ %x / \sqrt(%x^2 + %y^2) #; %y / \sqrt(%x^2 + %y^2) #] || & = & \sqrt( &nbsp;&nbsp; (%x / \sqrt(%x^2 + %y^2))^2 + (%y / \sqrt(%x^2 + %y^2))^2 &nbsp;&nbsp; )  \\ 
                                                             & = & \sqrt( &nbsp;&nbsp;  (%x^2 / (%x^2 + %y^2)) + (%y^2 / (%x^2 + %y^2)) &nbsp;&nbsp; ) \\
                                                             & = & \sqrt( &nbsp;&nbsp; (%x^2 + %y^2) / (%x^2 + %y^2) &nbsp;&nbsp; ) \\
                                                             & = & \sqrt(1) \\
                                                             & = & 1
\end{eqnarray}
Thus, || &nbsp; %v/||%v|| &nbsp; || = 1, so %v/||%v|| is a unit vector.
  </li>
</ol>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> List all the unit vectors that are linearly dependent with:

  $$ #[ 5 #; 12 #] $$

It is sufficient to solve for %s \in \R, %x \in \R, and %y \in \R that satisfy the following equations:
\begin{eqnarray}
  %s #[ %x #; %y #] & = & #[ 5 #; 12 #] 
\end{eqnarray}
\begin{eqnarray}
  ||(%x,%y)|| & = & 1
\end{eqnarray}
One approach is to write %x and %y in terms of %s and then solve for %s using the second equation.
</div>


<div class="mathenv example_to_know">
<b>Example:</b> Solve the following three problems.
<ol style="list-style-type:lower-alpha;">
  <li>
    Solve the following equation for %x \in \R:
\begin{eqnarray}
  || #[ %x #; 2 \cdot \sqrt(2) #] || & = & 3
\end{eqnarray}
  </li>
  <li>
    Solve the following equation for %x \in \R and %y \in \R:
\begin{eqnarray}
  #[ %x #; %y #] has length 10 \\
  #[ %x #; %y #] and #[ 2 #; 0 #] are linearly dependent
\end{eqnarray}
  </li>
  <li>
    Determine if the following two vectors are linearly dependent or linearly independent:
\begin{eqnarray}
  #[ 2 #; 3 #; 0 #] and #[ 2 #; 0 #; 1 #]
\end{eqnarray}
  </li>
</ol>
</div>

In the case of vectors in \R^2, the properties of linear dependence and orthogonality can be defined in terms of
the slopes of the vectors involved. However, note that these definitions in terms of slope are a <i>special case</i> for \R^2.
The more general definitions (in terms of scalar multiplication and dot product, respectively) apply to any vectors in a vector space.
Thus, we can derive the slope definitions from the more general definitions.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If two vectors [%x;%y] \in \R^2 and [%x';%y'] \in \R^2 are linearly dependent, then %x/%y = %x'/%y'.
If the two vectors a linearly dependent, then there exists a scalar %s \in \R such that:

\begin{eqnarray}
  %s \cdot #[ %x' #; %y' #] & = & #[ %x #; %y #] \\
  #[ %s \cdot %x' #; %s \cdot %y' #] & = & #[ %x #; %y #] \\
  %s \cdot %x' & = & %x \\
  %s \cdot %y' & = & %y \\
  (%s \cdot %x') / (%s \cdot %y') & = & %x/%y \\
  %x'/%y' & = & %x/%y
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If two vectors [%x;%y] \in \R^2 and [%x';%y'] \in \R^2 are orthogonal, then %x/%y = -%y'/%x':

\begin{eqnarray}
  #[ %x #; %y #] \cdot #[ %x' #; %y' #] & = & 0 \\
  %x \cdot %x' + %y \cdot %y' & = & 0 \\
  %x \cdot %x' & = & - %y \cdot %y' \\
  %x  & = & - %y \cdot %y'/%x' \\
  %x/%y  & = & -  %y'/%x'
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> List all unit vectors orthogonal to:

  $$ #[ 4 #; ---3 #] $$

It is sufficient to solve for %x \in \R and %y \in \R that satisfy the following equations:
\begin{eqnarray}
  #[ %x #; %y #] \cdot #[ 4 #; ---3 #] & = & 0 \\
  ||(%x,%y)|| & = & 1 
\end{eqnarray}
We can solve the above for the two vectors by first solving for both possible values of
%x, then finding the corresponding values for %y:
\begin{eqnarray}
  4 %x --- 3 %y & = & 0 \\
  %y & = & (4/3) %x \\
  \sqrt(%x^2 + %y^2) & = & 1 \\
  \sqrt(%x^2 + ((4/3) %x)^2) & = & 1 \\
  \sqrt((9/9)%x^2 + (16/9)%x^2 & = & 1 \\
  \sqrt((25/9) %x^2) & = & 1 \\
  \sqrt((25/9) %x^2) & = & 1 \\
  \pm (5/3) %x & = & 1 \\
  %x & = & \pm 3/5
\end{eqnarray}
Thus, the two vectors are:
\begin{eqnarray}
  #[ %x #; %y #] & \in & { #[ 3/5 #; 4/5 #] , #[ ---3/5 #; ---4/5 #] }
\end{eqnarray}
</div>

Orthogonal projections of vectors have a variety of interpretations and applications; one simple interpretation of the projection of
a vector %v onto a vector %w is <i>the shadow</i> of vector %v on %w with respect to a light source that is orthogonal to %w.

<div class="mathenv example_to_know">
<b>Example:</b> Consider the vector following vectors, where %u, %v \in \R^2 and %u is a unit vector:
\begin{eqnarray}
  %v & = & #[ %x #; %y #] \\
  %u & = & #[ 1 #; 0 #]
\end{eqnarray}
What is the orthogonal projection of %v onto %u? It is simply the %x component of %v, which is %x. Thus:
\begin{eqnarray}
  #[ %x #; 0 #] is the projection of %v onto %u
\end{eqnarray}
Notice that we can obtain the length of the orthogonal projection using the dot product:
\begin{eqnarray}
  %v \cdot %u & = & #[ %x #; %y #] \cdot #[ 1 #; 0 #] \\
              & = & %x
\end{eqnarray}
Since %u is a unit vector, we can then simply multiply %u by the scalar %x to obtain the actual projection:
\begin{eqnarray}
  (%v \cdot %u) \cdot %u & = & (#[ %x #; %y #] \cdot #[ 1 #; 0 #]) \cdot #[ 1 #; 0 #] \\
                         & = & %x \cdot #[ 1 #; 0 #] \\
                         & = & #[ %x #; 0 #] \\
\end{eqnarray}
</div>

The above example can be generalized to an arbitrary vector %v and unit vector %u; then, it can be generalized to any vector %u by using
the fact that %u/||%u|| is always a unit vector.

<div class="mathenv example_to_know">
<b>Example:</b> Compute the orthogonal projection of %v onto %u where:
\begin{eqnarray}
  %v & = & #[ 9 #; 2 #] \\
  %u & = & #[ 3 #; 4 #]
\end{eqnarray}

We can apply the formula:
\begin{eqnarray}
  ||%u|| & = & \sqrt(3^2 + 4^2) \\
         & = & \sqrt(9 + 16) \\
         & = & 5 \\
  %u/||%u|| & = & #[ 3/5 #; 4/5 #] \\
  (%v \cdot (%u/||%u||)) \cdot (%u/||%u||) & = & (#[ 9 #; 2 #] \cdot #[ 3/5 #; 4/5 #]) \cdot #[ 3/5 #; 4/5 #] \\
                                           & = & (27/5 + 8/5) \cdot #[ 3/5 #; 4/5 #] \\
                                           & = & (35/5) \cdot #[ 3/5 #; 4/5 #] \\
                                           & = & 7 \cdot #[ 3/5 #; 4/5 #] \\ \\
                                           & = & #[ 21/5 #; 28/5 #]
\end{eqnarray}

<a href="http://www.wolframalpha.com/input/?i=projection+of+%289%2C2%29+onto+%283%2C4%29">Other tools available online</a>
can be used to perform and check computations such as the above.
</div>

<a name="lecture4"></a>

<div class="mathenv example_to_know">
<b>Example:</b> Solve the problems below.

<ol style="list-style-type:lower-alpha;">
  <li>
Compute the orthogonal projection of %v onto %u where %u is a unit vector and:
\begin{eqnarray}
  %v & = & #[ 4 #; 2 #] \\
  %u & = & #[ 1/\sqrt(2) #; 1/\sqrt(2) #]
\end{eqnarray}

We can apply the formula:
\begin{eqnarray}
  (%v \cdot (%u/||%u||)) \cdot (%u/||%u||) & = & (#[ 4 #; 2 #] \cdot #[ 1/\sqrt(2) #; 1/\sqrt(2) #]) \cdot #[ 1/\sqrt(2) #; 1/\sqrt(2) #] \\
                                           & = & (6/\sqrt(2)) \cdot #[ 1/\sqrt(2) #; 1/\sqrt(2) #] \\
                                           & = & #[ 3 #; 3 #]
\end{eqnarray}

  </li>
  <li>
Compute the projection of %v onto %w where:
\begin{eqnarray}
  %v & = & #[ 13 \cdot 3 #; 13 \cdot 2 #] \\
  %w & = & #[ 5 #; 12 #]
\end{eqnarray}

We can apply the formula:
\begin{eqnarray}
  ||%w|| & = & \sqrt((5)^2 + (12)^2) \\
         & = & \sqrt(25 + 144) \\
         & = & 13 \\
  %w/||%w|| & = & #[ 5/13 #; 12/13 #] \\
  (%v \cdot (%w/||%w||)) \cdot (%w/||%w||) & = & (#[ 13 \cdot 3 #; 13 \cdot 2 #] \cdot #[ 5/13 #; 12/13 #]) \cdot #[ 5/13 #; 12/13 #] \\
                                           & = & (15 + 24) \cdot #[ 5/13 #; 12/13 #] \\
                                           & = & 39 \cdot #[ 5/13 #; 12/13 #] \\
                                           & = & #[ 15 #; 36 #]
\end{eqnarray}
  </li>
</ol>
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> Given the points %u = [%x_1,%y_1] and %v = [%x_2,%y_2], we can find the equation of the line between these two points in the
form %y = %m%x + %b.

We recall the definition for a line %L defined by two points:
\begin{eqnarray}
  %L & = & { %p | \exists %a \in \R, %p = %a (%u - %v) + %u }.
\end{eqnarray}
Thus, if [%x; %y] is on the line, we have:
\begin{eqnarray}
  #[%x#; %y#] & = & %a (#[%x_1#;%y_1#] - #[%x_2#;%y_2#]) + #[%x_1#;%y_1#].
\end{eqnarray}
This implies the following system of equations (one from the %x components in the above, and one from the %y components):
\begin{eqnarray}
 %x & = & %a (%x_1 - %x_2) + %x_1 \\
 %y & = & %a (%y_1 - %y_2) + %y_1
\end{eqnarray}
If we solve for %a in terms of %x, we can recover a single equation for the line:
\begin{eqnarray}
 %a & = & (%x - %x_1)/(%x_1 - %x_2) \\
 %y & = & ((%x - %x_1)/(%x_1 - %x_2)) (%y_1 - %y_2) + %y_1 \\
 %y & = & ((%y_1 - %y_2)/(%x_1 - %x_2)) (%x - %x_1)  + %y_1
\end{eqnarray}
Notice that we can set %m = (%y_1 - %y_2)/(%x_1 - %x_2) because that is exactly the slope of the line between [%x_1;%y_1] and [%x_2;%y_2].
\begin{eqnarray}
 %y & = & %m (%x - %x_1)  + %y_1 \\
 %y & = & %m%x - %m %x_1 + %y_1
\end{eqnarray}

We see that we can set %b = - %m %x_1 + %y_1.
\begin{eqnarray}
  %m & = & (%y_1 - %y_2)/(%x_1 - %x_2) \\
  %b & = & - %m %x_1 + %y_1 \\
  %y & = & %m%x + %b \\
  %L & = & { #[%x#;%y#] &nbsp; | &nbsp; %y = %m %x + %b }
\end{eqnarray}
</div>

<a name="lecture5"></a>

<a name="2.5"></a>
<h3><span class="secn">2.5.</span> Solving common problems involving vector algebra </h3>

<!--
lines (as an equation?)
planes (as an equation?)

as constriants:
  find vector v on plane P or line L, but orthogonal to P or L
  
find a unit vector on a line

find a unit vector that's orthogonal to something
find a vector of a specific length that is orthogonal

point(s) that fall on one or more lines/planes

with constants; with variables

rewrite vector as a linear combination of one other; two others
-->


We review the operations and properties of vectors introduced in this section by considering several example problems.


<div class="mathenv example_to_know">
<b>Example:</b> Are the vectors [2; 1] and [3; 2] linearly independent?

There are at least two ways we can proceed in checking pairwise linear independence. Both involve checking if the vectors
are linearly dependent. If they are linearly dependent, then they cannot be linearly independent. If they are not linearly
dependent, they must be linearly independent.

We can compare the slopes; we see they are different, so they must be linearly independent.

  $$ 1/2 \neq 2/3 $$

We can also use the definition of linear dependence. If they are linearly dependent, then we know that
\begin{eqnarray}
  \exists %a \in \R, %a #[2#; 1#] = #[3#; 2#]
\end{eqnarray}
Does such an %a exist? We try to solve for it:
\begin{eqnarray}
 %a #[2#; 1#] & = & #[3#; 2#] \\
 2%a & = & 3 \\
 %a & = & 2 \\
  4 & = & 3
\end{eqnarray}

Since we derive a contradiction, there is no such %a, so the two vectors are not linearly dependent, which means they
are linearly independent.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Given %v = [15; 20], list the vectors that are orthogonal to %v, but of the same length as %v.

The two constraints on the vectors [%x;%y] we seek are:

\begin{eqnarray}
 #[15#; 20#] \cdot #[%x#; %y#] & = & 0\\
 ||#[15#; 20#]||  & = & ||#[%x#; %y#]||
\end{eqnarray}

We take the first constraint and solve for %x in terms of %y.

\begin{eqnarray}
 #[15#; 20#] \cdot #[%x#; %y#] & = & 0\\
 15%x + 20%y & = & 0\\
 %x & = & (-20/15) %y
\end{eqnarray}

We now plug this into the second equation.

\begin{eqnarray}
 ||#[15#;20#]||  & = & ||#[(-20/15) %y#;%y#]|| \\
 \sqrt(15^2 + 20^2) & = & \sqrt((400/225) %y^2 + y^2) \\
 \sqrt(625) & = & \sqrt((625/225) %y^2) \\
 25 & = & \pm (25/15) %y \\
 15 & = & \pm %y \\
 %y & = & \pm 15
\end{eqnarray}

Thus, the vectors are [-20; 15] and [20; -15].
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Given constants %a,%b,%c \in \R, find a vector orthogonal to the plane %P defined 
by:
\begin{eqnarray}
  %P & = & { #[%x#; %y#; %z#] &nbsp; | &nbsp; %a (%x + %y + %z) + %b (%y + %z) + %c %z = 0 }.
\end{eqnarray}
We only need to rewrite the equation defining the plane in a more familiar form. For any [%x; %y; %z] \in %P, we know that:
\begin{eqnarray}
 %a (%x + %y + %z) + %b (%y + %z) + %c %z & = & 0\\
 %a%x + (%a + %b) %y + (%a + %b + %c) %z & = & 0\\
 #[%a #; %a + %b#; %a + %b + %c#] \cdot #[%x#; %y#; %z#] & = & 0
\end{eqnarray}
In order to be orthogonal to a plane, a vector must be orthogonal to all vectors [%x;%y;%z] on that plane.
Since all points on the plane are orthogonal to [%a; %a + %b; %a + %b + %c] by the above argument, [%a; %a + %b; %a + %b + %c] is such a point.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Define the line %L that is orthogonal to the vector [%a; %b] but also crosses [%a; %b] (i.e, [%a; %b] falls on the line).

We know that the line must be parallel to the vector that is orthogonal to [%a; %b]. The line %L_0 crossing [0; 0] that is orthogonal
to [%a; %b] is defined as:
\begin{eqnarray}
  %L_0 & = & { #[%x' #; %y' #] &nbsp; | &nbsp; #[ %a #; %b #] \cdot #[ %x' #; %y' #] = 0 }.
\end{eqnarray}
However, we need the line to also cross the point [%a; %b]. This is easily accomplished by adding the vector [%a; %b] to all the points
on the orthogonal line going through [0; 0] (as defined above). Thus, we have:
\begin{eqnarray}
  %L & = & { #[ %x' #; %y' #] + #[ %a #; %b #] &nbsp; | &nbsp; #[ %a #; %b #] \cdot #[ %x' #; %y' #] = 0 }.
\end{eqnarray}
If we want to find points [ %x ; %y ] on the line directly without the intermediate term [ %x' ; %y' ], we can solve for 
[ %x' ; %y' ] in terms of [ %x ; %y ]:
\begin{eqnarray}
  #[ %x #; %y #] & = & #[ %x' #; %y' #] + #[ %a #; %b #] \\
  #[ %x' #; %y' #] & = & #[ %x #; %y #] --- #[ %a #; %b #]
\end{eqnarray}
We can then substitute to obtain a more direct definition of %L (in terms of a constraint on the
vectors [ %x ; %y ] in %L):
\begin{eqnarray}
  %L & = & { #[ %x' #; %y' #] + #[ %a #; %b #] &nbsp;&nbsp; | &nbsp;&nbsp; #[ %a #; %b #] \cdot #[ %x' #; %y' #] = 0 } \\
     & = & { (#[ %x #; %y #] --- #[ %a #; %b #]) + #[ %a #; %b #] &nbsp;&nbsp; | &nbsp;&nbsp; #[ %a #; %b #] \cdot (#[ %x #; %y #] --- #[ %a #; %b #]) = 0 } \\
     & = & { #[ %x #; %y #] &nbsp;&nbsp; | &nbsp;&nbsp; #[ %a #; %b #] \cdot (#[ %x #; %y #] --- #[ %a #; %b #]) = 0 }
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Is [7; ---1] on the line defined by the points %u = [19; 7] and %v = [1; ---5]?

To solve this problem, we recall the definition for a line defined by two points:

  $${ %p | \exists %a \in \R, %p = %a (%u - %v) + %u }.$$

Thus, we want to know if [7; -1] is in the set defined as above. This can only occur if there exists
an %a such that [7; -1] = %a (%u - %v) + %u.

We solve for %a; if no solution exists, then the point [7; -1] is not on the line %L. If an %a exists, then it is. In this case,
%a = 1/3 is a solution to both equations, so [7; -1] is on the line.

\begin{eqnarray}
 #[7#; -1#] & = & %a(#[19#; 7#] --- #[1#; -5#]) + #[1#; ---5#]\\
         & = & (#[19%a --- 1%a#; 7%a + 5%a#]) + #[1#; ---5#]\\
         & = & (#[18%a + 1#; 12%a --- 5#])\\
       7 & = & 18%a+1\\
    ---1 & = & 12%a --- 5\\
      %a & = & 1/3
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Define the line that is orthogonal to the vector [3; 5] but also crosses [3; 5] (i.e, [3; 5] falls on the line).

We know that the line must be parallel to the vector that is orthogonal to [3; 5]. The line crossing [0; 0] that is orthogonal
to [3; 5] is defined as:
\begin{eqnarray}
  { #[%x#; %y#] &nbsp; | &nbsp; #[3#; 5#] \cdot #[%x#; %y#] = 0 }
\end{eqnarray}
We can rewrite the above in a more familiar form:
\begin{eqnarray}
  5 %y & = & --- 3 %x \\
  %y & = & --- (3/5) %x
\end{eqnarray}
However, we need the line to also cross the point [3; 5]. This is easily accomplished by adding the vector [3; 5] to all the points
on the orthogonal line going through [0; 0] (as defined above). Thus, we have:
\begin{eqnarray}
  { #[3#; 5#] + #[%x#; %y#] &nbsp; | &nbsp; #[3#; 5#] \cdot #[%x#; %y#] = 0 } 
\end{eqnarray}
We can rewrite the above by defining:
\begin{eqnarray}
   #[%x'#; %y'#] & = & #[3#; 5#] + #[%x#; %y#] \\
   #[%x'#; %y'#] --- #[3#; 5#] & = & #[%x#; %y#] \\
\end{eqnarray}
Now we substitute [%x;%y] with [%x',%y'] in our definition of the line:
\begin{eqnarray}
  { #[%x'#; %y'#] &nbsp; | &nbsp; #[3#; 5#] \cdot (#[%x'#; %y'#] - #[3#; 5#]) = 0 }
\end{eqnarray}
We can now write the equation for the line:
\begin{eqnarray}
  3 (%x' --- 3) + 5 (%y' --- 5) & = & 0 \\
  5 (%y' --- 5) & = & -3 (%x' --- 3) \\
  5 %y' --- 25 & = & -3 (%x' --- 3) \\
  5 %y' & = & -3 (%x' --- 3) + 25\\
  %y' & = & ---3/5 (%x' --- 3) + 5\\
  %y' & = & ---3/5 %x' + 9/5 + 5
\end{eqnarray}
Notice that, alternatively, we could have instead simply found the %y-intercept %b \in \R of the following equation using the point [3;5]:
\begin{eqnarray}
  %y & = & --- (3/5) %x + %b\\
  5 & = & --- (3/5) (3) + %b\\
  5 + 9/5 & = & %b\\
  %b & = & 5 + 9/5
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Is [8; -6] a linear combination of the vectors [19; 7] and [1; -5]?

We recall the definition of a linear combination and instantiate it for this example:

\begin{eqnarray}
  \exists %a,%b \in \R, #[8#; -6#] = %a #[19#; 7#] + %b #[1#; ---5#].
\end{eqnarray}

Thus, if we can solve for %a and %b, then [8; ---6] is indeed a linear combination.

\begin{eqnarray}
 #[8#; ---6#] & = & %a #[19#; 7#] + %b #[1#; ---5#]\\
         & = & #[19%a #; 7%a #] +  #[1%b#; ---5%b#]\\
       8 & = & 19%a + %b \\
       %b & = & -19%a + 8 \\
       ---6 & = & 7%a --- 5%b \\
       ---6 & = & 7%a --- 5(---19%a + 8) \\
       ---6 & = & 7%a + 95%a --- 40 \\
       34 & = & 102 %a \\
       %a & = & 1/3 \\
       %b & = & ---19/3 + 8 = 5/3
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Are the vectors %V = {[2; 0; 4; 0], [6; 0; 4; 3], [1; 7; 4; 3]} linearly independent?

The definition for linear independence requires that none of the vectors being considered can be expressed
as a sum of the others. Thus, we must check all pairs of vectors against the remaining third vector not in the pair.
There are 3!/(2!*1!) such pairs:
\begin{eqnarray}
 can #[2#; 0#; 4#; 0#] be expressed as a combination of #[6#; 0#; 4#; 3#] and #[1#; 7#; 4#; 3#]? \\
 can #[6#; 0#; 4#; 3#] be expressed as a combination of #[2#; 0#; 4#; 0#] and #[1#; 7#; 4#; 3#]? \\
 can #[1#; 7#; 4#; 3#] be expressed as a combination of #[6#; 0#; 4#; 3#] and #[2#; 0#; 4#; 0#]?
\end{eqnarray}

For each combination, we can check whether the third vector is linearly dependent.
If it is linearly dependent, we can stop and say that the three vectors are not
linearly independent. If it is not, we must continue checking all the pairs. If all the pairs are incapable of
being scaled and added in some way to obtain the third vector, then the three vectors are linearly independent.

  $$ not (%u, %v, %w are linearly independent) &nbsp;&nbsp;iff&nbsp;&nbsp; (\exists %a,%b \in \R, %u,%v,%w \in %V, %w is a linear combination of %u and %v)$$

Notice that this an example of a general logical rule:

  $$ not (\forall %x \in %S, %p) &nbsp;&nbsp;iff&nbsp;&nbsp; (\exists %x \in %S, not %p)$$

We check each possible combination and find that we derive a contradiction if we assume they are not independent.

\begin{eqnarray}
 #[2#; 0#; 4#; 0#] & = & #[6#; 0#; 4#; 3#] %a + #[1#; 7#; 4#; 3#] %b \\
 0 & = & 7 %b \\
 %b & = & 0 \\
 2 & = & 6 %a + 1 %b \\
 %a & = & 2/6 \\
 0 & = & 3 (2/6) + 3 (0) \\
 0 & = & 1
\end{eqnarray}

\begin{eqnarray}
 #[6#; 0#; 4#; 3#] & = & #[2#; 0#; 4#; 0#] %a + #[1#; 7#; 4#; 3#] %b \\
 3 & = & 3 %b \\
 %b & = & 1 \\
 6 & = & 2 %a + %b \\
 6 & = & 2 %a + 1 \\
 5 & = & 2 %a \\
 %a & = & 5/2 \\
 4 & = & 4 (5/2) + 4 \\
 0 & = & 10
\end{eqnarray}

\begin{eqnarray}
 #[1#; 7#; 4#; 3#] & = & #[2#; 0#; 4#; 0#] %a + #[6#; 0#; 4#; 3#] %b \\
 7 & = & 0 %a + 0 %b \\
 7  & = & 0
\end{eqnarray}

Thus, %V is a set of linearly independent vectors.
</div>

<a name="lecture6"></a>

<a name="2.6"></a>
<h3><span class="secn">2.6.</span> Using vectors and linear combinations to model systems</h3>

In the introduction we noted that in this course, we would define a symbolic language for working with a certain
collection of idealized mathematical objects that can be used to model system states of real-world systems in an abstract
way. Because we are considering a particular collection of objects (vectors, planes, spaces, and their relationships),
it is natural to ask what kinds of problems are well-suited for such a representation (and also what problems are
not well-suited).

What situations and associated problems can be modelled using vectors and related operators and properties?
Problems involving concrete objects that have a position, velocity, direction, geometric shape, and relationships between
these (particularly in two or three dimensions) are natural candidates. For example, we have seen that it is possible
to compute the projection of one vector onto another. However, these are just a particular example of a more general
family of problems that can be studied using vectors and their associated operations and properties.

A vector of real numbers can be used to represent an object or collection of objects with some fixed number of characteristics (each
corresponding to a dimension or component of the vector) where each characteristic has a range of possible values.
This range could be a set of magnitudes (e.g., position, cost, mass), a discrete collection of states (e.g.,
absence or presence of an edge in a graph), or even a set of relationships (e.g., for every cow, there are four
cow legs; for every $1 invested, there is a return of $0.02). Thus, vectors are well-suited for representing 
problems involving many instances of objects where all the objects have the same set of possible characteristics
along the same set of linear dimensions. In these instances, many vector operations also have natural interpretations.
For example, addition and scalar multiplication (i.e., linear combinations) typically correspond to the aggregation
of a property across multiple instances or copies of objects with various properties (e.g., the total mass of a
collection of objects).

In order to illustrate how vectors and linear combinations of vectors might be used in applications, we recall the
notion of a <i>system</i>. A <i>system</i> is any physical or abstract phenomenon, or observations of a phenomenon, that
we characterize as a collection of real values along one or more <i>dimensions</i>. A
<i>system state</i> or <i>state</i> a system is a particular collection
of real values. For example, if a system is represented by \R^4, states of that system are represented by individual vectors
in \R^4 (note that not all vectors need to correspond to valid or possible states; see the examples below).

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following system: a barn with cows and chickens inside it. There are several dimensions along
which an observer might be able to measure this system (we assume that the observer has such poor eyesight that chickens and
cows are indistinguishable from above):
<ul>
 <li>number of chickens inside</li>
 <li>number of cows inside</li>
 <li>number of legs that can be seen by peeking under the door</li>
 <li>number of heads that can be seen by looking inside from a high window</li>
</ul>
Notice that we could represent a particular <i>state</i> of this system using a vector in \R^4. However, notice also that
many vectors in \R^4 will <i>not</i> correspond to any system that one would expect to observe. Usually, the number of
legs and heads in the entire system will be a <i>linear combination</i> of two vectors: the number of legs per cow,
and the number of legs per chicken:

\begin{eqnarray}
 #[ 1 <i style="color:gray;">head</i> #; 2 <i style="color:gray;">legs</i> #] \cdot 
   %x <i style="color:gray;">chickens</i> + 
      #[1 <i style="color:gray;">head</i> #; 4 <i style="color:gray;">legs</i> #] \cdot %y <i style="color:gray;">cows</i> 
      & = &  #[ %x+%y <i style="color:gray;">heads</i>#; 2%x+4%y <i style="color:gray;">legs</i> #]
\end{eqnarray}
 
<br/>Given this relationship, it may be possible to derive some characteristics of the system given only partial information.
Consider the following problem: how many chickens and cows are in a barn if 8 heads and 26 legs were observed?<br/><br/>

\begin{eqnarray}
 #[ 1 <i style="color:gray;">head</i> #; 2 <i style="color:gray;">legs</i> #] \cdot 
   %x <i style="color:gray;">chickens</i> +
      #[1 <i style="color:gray;">head</i> #; 4 <i style="color:gray;">legs</i> #] \cdot %y <i style="color:gray;">cows</i> 
      & = & #[ 8 <i style="color:gray;">heads</i>#; 26 <i style="color:gray;">legs</i> #]
\end{eqnarray}

Notice that a linear combination of vectors can be viewed as a translation from a vector describing one set of
dimensions to a vector describing another set of dimensions. Many problems might exist in which the values are
known along one set of dimensions and unknown along another set.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> We can restate the example from the introduction using linear combinations. Suppose we have $12,000 and two
investment opportunities: A has an annual return of 10%, and B has an annual return of 20%. How much should we invest
in each opportunity to get $1800 over one year?

The two investment opportunities are two-dimensional vectors representing the rate of return on a dollar:

\begin{eqnarray}
     #[ 1 <i style="color:gray;">dollar</i> #; 0.1 <i style="color:gray;">interest</i> #] and #[ 1 <i style="color:gray;">dollar</i> #; 0.2 <i style="color:gray;">interest</i> #]
\end{eqnarray}

The problem is to find what combination of the two opportunities would yield the desired observation of the entire system:

\begin{eqnarray}
    #[ 1 <i style="color:gray;">dollar</i> #; 0.1 <i style="color:gray;">dollars of interest</i> #] 
    \cdot x <i style="color:gray;">dollars in opportunity A</i> 
    + #[ 1 <i style="color:gray;">dollar</i> #; 0.2 <i style="color:gray;">dollars of interest</i> #] 
     \cdot y <i style="color:gray;">dollars in opportunity B</i> 
     & = & #[ 12,000 <i style="color:gray;">dollars</i> #; 1800 <i style="color:gray;">dollars of interest</i> #]
\end{eqnarray}

Next, we consider a problem with discrete dimensions.
</div>

<b>Example:</b> Suppose there is a network of streets and intersections and the city wants to set up cameras
at some of the intersections. Cameras can only see as far as the next intersection. Suppose there are five
streets (#1, #2, #3, #4, #5) and four intersections (A, B, C, and D) at which cameras can be placed, and the city
wants to make sure a camera can see every street while not using any cameras redundantly (i.e., two cameras
should not film the same street).

Vectors in \R^5 can represent which streets are covered by a camera. A fixed collection of vectors, one for each
intersection, can represent what streets a camera can see from each intersection. Thus, the system's dimensions
are:
<ul>
 <li>is street #1 covered by a camera?</li>
 <li>is street #2 covered by a camera?</li>
 <li>is street #3 covered by a camera?</li>
 <li>is street #4 covered by a camera?</li>
 <li>is street #5 covered by a camera?</li>
 <li>is there a camera at intersection A? (represented by the variable %a below)</li>
 <li>is there a camera at intersection B? (represented by the variable %b below)</li>
 <li>is there a camera at intersection C? (represented by the variable %c below)</li>
 <li>is there a camera at intersection D? (represented by the variable %d below)</li>
</ul>
Four fixed vectors will be used to represent which streets are adjacent to which intersections:

\begin{eqnarray}
    #[0 #; 1 #; 0 #; 0 #; 1 #] , 
    #[1 #; 0 #; 1 #; 1 #; 0 #] ,
    #[1 #; 1 #; 0 #; 0 #; 0 #] ,
    #[1 #; 0 #; 0 #; 1 #; 1 #]
\end{eqnarray}

Placing the cameras in the way required is possible if there is integer solution to the following equation
involving a linear combination of the above vectors:

\begin{eqnarray}
    #[0 #; 1 #; 0 #; 0 #; 1 #] %a +  #[1 #; 0 #; 1 #; 1 #; 0 #] %b + #[1 #; 1 #; 0 #; 0 #; 0 #] %c + #[1 #; 0 #; 0 #; 1 #; 1 #] %d 
      & = & #[1 #; 1 #; 1 #; 1 #; 1 #]
\end{eqnarray}

<div class="mathenv example_to_know">
<b>Example:</b> Suppose a chemist wants to model a chemical reaction. The dimensions of the system might be:
<ul>
 <li>how many molecules of C_3H_8 are present?</li>
 <li>how many molecules of O_2 are present?</li>
 <li>how many molecules of CO_2 are present?</li>
 <li>how many molecules of H_2O are present?</li>
 <li>how many atoms of carbon are present?</li>
 <li>how many atoms of hydrogen are present?</li>
 <li>how many atoms of oxygen are present?</li>
</ul>

Individual vectors in \R^3 can be used to represent how many atoms of each element are in each type of
molecule being considered:

\begin{eqnarray}
  C_3H_8: #[ 3 #; 8 #; 0 #] ,\~ O_2:  #[ 0 #; 0 #; 2 #] ,\~ CO_2: #[ 1 #; 0 #; 2 #] ,\~ H_2O: #[ 0 #; 2 #; 1 #]
\end{eqnarray}

Suppose we know that the number of atoms in a system may never change during a reaction, and that some quantity
of C_3H_8 and O_2 can react to yield only CO_2 and H_2O. How many molecules of each compound will be involved in the reaction? That is the solution
to the following linear combination.

\begin{eqnarray}
  #[ 3 #; 8 #; 0 #] %x_1 + #[ 0 #; 0 #; 2 #] %x_2  & = & #[ 1 #; 0 #; 2 #] %x_3 + #[ 0 #; 2 #; 1 #] %x_4
\end{eqnarray}

For example, suppose we start with 1000 molecules of C_3H_8 and 5000 molecules of O_2. If both of these compounds react to produce only
CO_2 and H_2O, how many molecules of each will be produced?

\begin{eqnarray}
  #[ 3 #; 8 #; 0 #] 1000 + #[ 0 #; 0 #; 2 #] 5000  & = & #[ 1 #; 0 #; 2 #] %a + #[ 0 #; 2 #; 1 #] %b \\
  #[ 3000 #; 8000 #; 10000 #] & = & #[ 1 #; 0 #; 2 #] %a + #[ 0 #; 2 #; 1 #] %b \\
  3000 & = & 1 \cdot %a + 0 \cdot %b \\
  %a & = & 3000 \\
  8000 & = & 0 \cdot %a + 2 \cdot %b \\
  %b & = & 4000 \\
  10000 & = & 2 \cdot 3000 + 1 \cdot 4000
\end{eqnarray}

Thus, %a = 3000 molecules of CO_2 and %b = 4000 molecules of H_2O will be produced.
</div>

The notion of a linear combination of vectors is common and can be used to mathematically model a wide variety of problems.
Thus, a more concise notation for linear combinations of vectors would be valuable. This is one of the issues addressed by introducing
a new type of term: the <i>matrix</i>.

<a name="3"></a>
<hr style="margin-bottom:120px;"/>
<h2><span class="secn">3.</span> Matrices</h2>

In this section we introduce a new kind of term: a <i>matrix</i>. We define some operations on matrices and some
properties of matrices, and we describe some of the possible ways to interpret and use matrices.

<a name="3.1"></a>
<h3><span class="secn">3.1.</span> Matrices and multiplication of a vector by a matrix</h3>

Matrices are a concise way to represent and reason about linear combinations and linear independence of vectors (e.g., setwise linear
independence might be difficult to check using an exhaustive approach), reinterpretations of systems using different dimensions,
and so on. One way to interpret a matrix is as a collection of vectors. Multiplying a matrix by a vector corresponds to computing a linear
combination of that collection of vectors.

As an example, we consider the case for linear combinations of two vectors. The two scalars in the linear combination
can be interpreted as a 2-component vector. We can then put the two vectors together into a single object in our
notation, which we call a <i>matrix</i>.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b>

\begin{eqnarray}
  #[%a#,%b#;%c#,%d#] \cdot #[%x#;%y#] & = & #[%a#;%c#] \cdot %x + #[%b#;%d#] \cdot %y
\end{eqnarray}
</div>

Notice that the columns of the matrix are the vectors used in our linear combination. Notice also that we can now
reinterpret the result of multiplying a vector by a matrix as taking the dot product of each of the matrix rows with the
vector.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b>

\begin{eqnarray}
  #[%a#,%b#;%c#,%d#] \cdot #[%x#;%y#] & = & #[%a#;%c#] \cdot %x + #[%b#;%d#] \cdot %y \\
                                      & = & #[%a %x #; %c %x#] + #[%b %y#; %d %y#] \\
                                      & = & #[%a%x + %b%y#; %c%x + %d%y#] \\
                                      & = & #[(%a,%b) \cdot (%x,%y)#; (%c,%d) \cdot (%x,%y)#]
\end{eqnarray}
</div>

Because a matrix is just two column vectors, we can naturally extend this definition of multiplication 
to cases in which we have multiplication of a <i>matrix</i> by a matrix: we simply multiply each column of the
second matrix by the first matrix and write down each of the resulting columns in the result matrix.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b>

\begin{eqnarray}
  #[%a#,%b#;%c#,%d#] \cdot#[%x #, %s #;%y #, %t#] & = & #[(%a,%b) \cdot (%x,%y) #, (%a,%b) \cdot(%s,%t) #; (%c,%d) \cdot (%x,%y) #, (%c,%d) \cdot (%s,%t)#]
\end{eqnarray}
</div>

These definitions can be extended naturally to vectors and matrices with more than two components. If we denote using %M_{ij} the entry in a matrix %M found
in the %ith row and %jth column, then we can define the result of matrix multiplication of two matrices %A and %B as a matrix %M such that

\begin{eqnarray}
  %M_{ij} &nbsp;&nbsp;=&nbsp;&nbsp; %ith row of %A \cdot %jth column of %B.
\end{eqnarray}

<a name="3.2"></a>
<h3><span class="secn">3.2.</span> Interpreting matrices as tables of relationships and transformations of system states</h3>

We saw how vectors can be used to represent system states. We can extend this interpretation to matrices and use
matrices to represent relationships between the dimensions of system states. This allows us to interpret matrices as transformations
between system states (or partial observations of system states).

If we again consider the example system involving a barn of cows and chickens, we can reinterpret the matrix as a table of relationships between
dimensions. Each entry in the table has a unit indicating the relationship it represents.

<table>
 <tr>
  <td></td>
  <td align="center">chickens</td>
  <td align="center">cows</td>
  <td></td>
 </tr>
 <tr>
  <td>heads</td>
  <td>1 <i style="color:gray;">head/chicken</i></td>
  <td>1 <i style="color:gray;">head/cow</i></td>
  <td></td>
 </tr>
 <tr>
  <td>legs</td>
  <td>2 <i style="color:gray;">legs/chicken</i></td>
  <td>4 <i style="color:gray;">legs/cow</i></td>
  <td></td>
 </tr>
</table>

Notice that the <i>column labels</i> in this table represent the dimensions of an "input" vector that could be multiplied by this matrix, and
the <i>row labels</i> specify the dimensions of the "output" vector that is obtained as a result. That is,
if we multiply using the above matrix a vector that specifies the number of chickens and the number of cows in a system state, we will get a
vector that specifies the number of heads and legs we can observe in that system.

\begin{eqnarray}
 #[ 1 <i style="color:gray;">head/chicken</i> #,  1 <i style="color:gray;">head/cow</i> #; 
   2 <i style="color:gray;">legs/chicken</i> #, 4 <i style="color:gray;">legs/cow</i> #] 
   \cdot 
   #[x <i style="color:gray;">chickens</i> #; y <i style="color:gray;">cows</i> #]
      & = &  #[ %x+%y <i style="color:gray;">heads</i>#; 2%x+4%y <i style="color:gray;">legs</i> #]
\end{eqnarray}

Thus, we can interpret multiplication by this matrix as a function that takes system states that only specify the number of chickens and
cows, and converts them to system states that only specify the number of heads and legs:

  $$ (<i style="color:gray;"># chickens</i> \times <i style="color:gray;"># cows</i>) \to (<i style="color:gray;"># heads</i> \times <i style="color:gray;"># legs</i>) $$

<a name="lecture7a"></a>

<a name="3.3"></a>
<h3><span class="secn">3.3.</span> Interpreting multiplication of matrices as composition of system state transformations</h3>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose that we have a system with the following dimensions.
<ul>
 <li>number of wind farms</li>
 <li>number of coal power plants</li>
 <li>units of power</li>
 <li>units of cost (e.g., pollution)</li>
 <li>number of single family homes (s.f.h.'s)</li>
 <li>number of businesses</li>
</ul>

Two different matrices might specify the relationships between some combinations of dimensions in this system.

\begin{eqnarray}
 %M_1 = #[ 100 <i style="color:gray;">power/wind farm</i> #, 250 <i style="color:gray;">power/coal plant</i> #; 
    50 <i style="color:gray;">cost/wind farm</i> #,  400 <i style="color:gray;">cost/coal plant</i> #]
   , \~
 %M_2 = #[ 4 <i style="color:gray;">s.f.h./unit power</i> #,  -2 <i style="color:gray;">s.f.h./unit cost</i> #; 
    1 <i style="color:gray;">businesses/unit power</i> #,  0 <i style="color:gray;">businesses/unit cost</i> #]
\end{eqnarray}

Notice that these two matrices both represent transformations between partial system state descriptions.

  $$ %T_1: (<i style="color:gray;"># wind farms</i> \times <i style="color:gray;"># coal plants</i>) \to (<i style="color:gray;">units of power</i> \times <i style="color:gray;">units of cost</i>) $$
  $$ %T_2: (<i style="color:gray;">units of power</i> \times <i style="color:gray;">units of cost</i>) \to (<i style="color:gray;"># s.f.h.</i> \times <i style="color:gray;"># businesses</i>) $$

Notice that because the interpretion of a result obtained using the first transformation matches the interpretation of an input to the second, we
can compose these transformations to obtain a third transformation.

  $$ %T_2 \circ %T_1: (<i style="color:gray;"># wind farms</i> \times <i style="color:gray;"># coal plants</i>) \to (<i style="color:gray;"># s.f.h.</i> \times <i style="color:gray;"># businesses</i>) $$

This corresponds to multiplying the two matrices to obtain a third matrix. Notice that the units of the resulting matrix can be computed using a
process that should be familiar to you from earlier coursework.

\begin{eqnarray}
 #[ 4 <i style="color:gray;">s.f.h./unit power</i> #,  -2 <i style="color:gray;">s.f.h./unit cost</i> #; 
    1 <i style="color:gray;">businesses/unit power</i> #,  0 <i style="color:gray;">businesses/unit cost</i> #]
   \cdot 
 #[ 100 <i style="color:gray;">power/wind farm</i> #, 250 <i style="color:gray;">power/coal plant</i> #; 
    50 <i style="color:gray;">cost/wind farm</i> #,  400 <i style="color:gray;">cost/coal plant</i> #]
   & = & 
 #[ 300 <i style="color:gray;">s.f.h./wind farm</i> #,  200 <i style="color:gray;">s.f.h./coal plant</i> #; 
    100 <i style="color:gray;">business/wind farm</i> #,  250 <i style="color:gray;">businesses/coal plant</i> #]
\end{eqnarray}

Thus, given some vector describing the number of wind farms and coal plants in the system, we can multiply that vector by (%M_2 \cdot %M_1) 
to compute the number of single family homes and business we expect to find in that system.
</div>

<a name="lecture7b"></a>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose that a gram of gold costs $50, while a gram of silver costs $10. After purchasing some of each, you have spent $350 on
15 grams of material. How many grams of each commodity have you purchased?
<ol style="list-style-type:lower-alpha;">
 <li>Write down four dimensions describing this system.
 </li>
 <li>
  Define a matrix %A that can be used to convert a description of a system state that specifies only the amount of gold and silver purchased into
  a description of the system state that specifies only the cost and total weight.
 </li>
 <li>Write down a matrix equation describing this problem and solve it to find the solution.
 </li>
 <li>
  Define a matrix %B such that for any description of a system state %v that specifies only the total weight and amount spent, %B \cdot %v
  is a description of that system state that specifies the amount of gold and silver in that system state.
 </li>
</ol>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we characterize our system in terms of two dimensions:
<ul>
 <li>number of single family homes (s.f.h.'s)</li>
 <li>number of power plants (p.p.'s)</li>
</ul>
In this example, instead of studying the relationships between dimensions, we want to study how the dimensions
change (possibly in an interdependent way) over time. For example, the following matrix might capture how
the system state evolves from year to year:
\begin{eqnarray}
 %M = #[ 2 <i style="color:gray;">s.f.h. <span style="color:firebrick">in year 1</span>/s.f.h. <span style="color:firebrick">in year 0</span></i> #, -1 <i style="color:gray;">s.f.h. <span style="color:firebrick">in year 1</span>/p.p. <span style="color:firebrick">in year 0</span></i> #; 
    0 <i style="color:gray;">p.p. <span style="color:firebrick">in year 1</span>/s.f.h <span style="color:firebrick">in year 0</span></i> #,  1 <i style="color:gray;">p.p. <span style="color:firebrick">in year 1</span>/p.p. <span style="color:firebrick">in year 0</span></i> #]
\end{eqnarray}
We can parameterize the above in terms of a year %t \in \R. Notice that the matrix above is just a special case of the matrix below (when %t = 0):
\begin{eqnarray}
 %M = #[ 2 <i style="color:gray;">s.f.h. <span style="color:firebrick">in year %t+1</span>/s.f.h. <span style="color:firebrick">in year %t</span></i> #, -1 <i style="color:gray;">s.f.h. <span style="color:firebrick">in year %t+1</span>/p.p. <span style="color:firebrick">in year %t</span></i> #; 
    0 <i style="color:gray;">p.p. <span style="color:firebrick">in year %t+1</span>/s.f.h <span style="color:firebrick">in year %t</span></i> #,  1 <i style="color:gray;">p.p. <span style="color:firebrick">in year %t+1</span>/p.p. <span style="color:firebrick">in year %t</span></i> #]
\end{eqnarray}
What does %M \cdot %M represent? If we consider the units, we have:
\begin{eqnarray}
 %M \cdot %M = #[ 4 <i style="color:gray;">s.f.h. <span style="color:firebrick">in year %t+2</span>/s.f.h. <span style="color:firebrick">in year %t</span></i> #, -3 <i style="color:gray;">s.f.h. <span style="color:firebrick">in year %t+2</span>/p.p. <span style="color:firebrick">in year %t</span></i> #; 
    0 <i style="color:gray;">p.p. <span style="color:firebrick">in year %t+2</span>/s.f.h <span style="color:firebrick">in year %t</span></i> #,  1 <i style="color:gray;">p.p. <span style="color:firebrick">in year %t+2</span>/p.p. <span style="color:firebrick">in year %t</span></i> #]
\end{eqnarray}
Suppose %v = [%x; %y] \in \R^2 represents the number of single family homes and factories in a given year. We can then define the number of single
family homes and factories after %t years as:
\begin{eqnarray}
 %M^{%t} \cdot %v & = & #[ 2 #, -1 #; 0 #, 1 #]^{%t} \cdot #[%x #; %y #] \\
\end{eqnarray}
If we wanted to write the number of single family homes and factories as a function of %t \in \R and an initial state %x_0, %y_0 \in \R, we
could nest the dot products as follows and use algebra to simplify:
\begin{eqnarray}
 <i style="color:gray;"># s.f.h. <span style="color:firebrick">in year %t</span> & = & 2 ( 2 ( 2 ( ... 2 (2 %x_0 - %y_0) - %y_0 ... ) - %y_0) - %y_0) - %y_0 \\
                  & = & 2^{%t} %x_0 - (2^{%t-1} + ... + 4 + 2 + 1) %y_0 \\
                  & = & 2^{%t} %x_0 - (2^{%t} - 1) %y_0 \\
                  & = & 2^{%t} %x_0 - 2^{%t} %y_0 + %y_0 \\
 <i style="color:gray;"># p.p. <span style="color:firebrick">in year %t</span> & = & 0 \cdot %x_{%t} + 1 \cdot ( ... (0 \cdot %x_2 + 1 \cdot ( 0 \cdot %x_1 + (0 \cdot %x_0 + 1 \cdot %y_0))) ... ) \\
  & = & %y_0
\end{eqnarray}
</div>



<!--assignment2-->
<br/><hr/>
<a name="3.4"></a>
<a name="assignment2"></a>
<h3><span class="secn">3.4.</span>
  <b>Assignment #2: Using Vectors and Matrices</b> <!--span class="btn_assignment">(<a href="materials.php?hw=2">show only this assignment</a>)</span-->
</h3>


       <p>In this assignment you will solve problems involving vectors and matrices.
       <b>Please submit a single file <code>a2.*</code> containing your solutions. The file extension may be anything you choose;
       please ask before submitting a file in an exotic or obscure file format (plain text is preferred).</b>

<ol>
  <li> Consider the line L in \R^2 that passes through the following two vectors:
\begin{eqnarray}
  %u & = & #[9 #; 7 #] \\
  %v & = & #[1 #; -5 #]
\end{eqnarray}
    <ol style="list-style-type:lower-alpha;">
      <li>Using set notation, define %L in terms of %u and %v.
        <solution>
        Below are some possible definitions:
\begin{eqnarray}
  %L & = & { %a (%u - %v) + %u &nbsp;|&nbsp; %a \in \R } \\
  %L & = & { %a (%u - %v) + %v &nbsp;|&nbsp; %a \in \R } \\
  %L & = & { %a (#[9 #; 7 #] - #[1 #; -5 #]) + #[1 #; -5 #] &nbsp;|&nbsp; %a \in \R } \\
  %L & = & { #[%x #; %y #] &nbsp;|&nbsp; %y = (3/2) %x - 13/2 }
\end{eqnarray}
        Solutions are acceptable as long as the definition is correct set notation for the set of
        vectors representing the line.
        </solution>
      </li>
      <li>Determine whether the following vectors are on the line %L:
\begin{eqnarray}
  #[25 #; 31 #] & , & #[7 #; ---1 #] & , & #[---3 #; ---11 #]
\end{eqnarray}
        <solution>
        The quickest way is to derive the %y = (3/2) %x --- 13/2 equation for points on the line
        and use it to check each point:
\begin{eqnarray}
 31 & = & (3/2) \cdot 25 --- 13/2 & , & so #[25 #; 31 #] \in %L \\
 ---1 & \neq & (3/2) \cdot 7 --- 13/2 & , & so #[7 #; ---1 #] \not\in %L \\ 
 ---11 & = & (3/2) \cdot (---3) --- 13/2 & , & so #[---3 #; ---11 #] \in %L
\end{eqnarray}
        </solution>
      </li>
      <li>Find all unit vectors orthogonal to %L.
        <solution>
        It is sufficient to find the two unit vectors on the line through the origin
        that is perpendicular to %L. A vector that is parallel to %L can be obtained using %u - %v:
\begin{eqnarray}
  #[9 #; 7 #] --- #[1 #; -5 #] & = & #[8 #; 12 #]
\end{eqnarray}
        The unit vectors must be orthogonal to the above vector. Thus, we have the following
        two equations for the unit vectors [ %x ; %y ] that we want to find:
\begin{eqnarray}
  #[8 #; 12 #] \cdot #[%x #; %y #] & = & 0 \\
                 || #[%x #; %y #] || & = & 1 \\
\end{eqnarray}
        The two vectors that satisfy the above are:
\begin{eqnarray}
  #[%x #; %y #] & \in & { #[3/\sqrt(13) #; ---2/\sqrt(13) #] , #[---3/\sqrt(13) #; 2/\sqrt(13) #] }
\end{eqnarray}
        </solution>
      </li>
      <li>Define the line %L^\bot that passes through the origin and is orthogonal to %L using an equation of the form:
\begin{eqnarray}
  %y & = & %m \cdot %x + %b
\end{eqnarray}
          In other words, find %m, %b \in \R such that:
\begin{eqnarray}
  %L^\bot & = & { #[%x #; %y #] &nbsp; | &nbsp; %y = %m \cdot %x + %b }
\end{eqnarray}
        <solution>
          We can use one of the unit vectors we found in part <b>(c)</b> above to determine
          the slope of %L^\bot. Since the line passes through the origin, %b = 0.
\begin{eqnarray}
  %m & = & (2/\sqrt(13)) &nbsp;/&nbsp; (---3/\sqrt(13)) & = & ---2/3 \\
  %b & = & 0
\end{eqnarray}          
        </solution>
      </li>
    </ol>
  </li>

  <li> For each of the following collections of vectors, determine whether the collection is
linearly independent. If it is, explain why; if not, show that one of the vectors is a
linear combination of the other vectors in the collection.

    <ol style="list-style-type:lower-alpha;">
      <li> The following two vectors in \R^2:
\begin{eqnarray}
  #[2 #; 1 #] & , & #[3 #; 2 #]
\end{eqnarray}
        <solution>
          The following equation has no solution %a \in \R since assuming there is a solution
          leads to a contradiction:
\begin{eqnarray}
  %a \cdot #[2 #; 1 #] & = & #[3 #; 2 #] \\
   %a \cdot 2 & = & 3 \\
    %a \cdot 1 & = & 2 \\
    %a & = & 2 \\
    2 \cdot 2 & \neq & 3
\end{eqnarray}
          Thus, the two vectors do not satisfy the definition of linear independence.
          They must be linearly independent, so the collection <b>is linearly independent</b>.
        </solution>
      </li>
      <li> The following three vectors in \R^2:
\begin{eqnarray}
  #[-2 #; 1 #] & , & #[1 #; 3 #] & , & #[2 #; 4 #]
\end{eqnarray}
        <solution>
          We must check if <i>any</i> of the three vectors might be a linear combination
          of the other two. In fact, there is at least one such vector, so the collection of
          vectors <b>is not linearly independent</b>:
\begin{eqnarray}
  #[-2 #; 1 #] & = &  5 \cdot #[1 #; 3 #] + (---7/2) \cdot #[2 #; 4 #]
\end{eqnarray}
          Notice that we could have concluded this immediately without finding the above
          counterexample
          because these vectors are in \R^2 and there are at least two linearly independent
          vectors in the collection. Thus, those two vectors can be used in a linear
          combination to obtain the third.          
        </solution>
      </li>
      <li> The following three vectors in \R^4:
\begin{eqnarray}
  #[2 #; 0 #; 4 #; 0 #] & , & #[6 #; 0 #; 4 #; 3 #] & , & #[1 #; 7 #; 4 #; 3 #]
\end{eqnarray}
        <solution>
          We must check if <i>any</i> of the three vectors might be a linear combination
          of the other two. We check if the first can be a linear combination of the second
          and third; since we arrive at a contradiction below, it cannot.
\begin{eqnarray}
  #[2 #; 0 #; 4 #; 0 #] & = &  %a \cdot #[6 #; 0 #; 4 #; 3 #] + %b \cdot #[1 #; 7 #; 4 #; 3 #] \\
  2 & = & 6 %a + %b \\
  0 & = & 7 %b \\
  %b & = & 0 \\
  %a & = & 1/3 \\
  4 & \neq & 4/3 + 0
\end{eqnarray}
          Next, we check if the second vector can be a linear combination of the first and third.
\begin{eqnarray}
  #[6 #; 0 #; 4 #; 3 #] & = &  %a \cdot #[2 #; 0 #; 4 #; 0 #] + %b \cdot #[1 #; 7 #; 4 #; 3 #] \\
  6 & = & 2 %a + %b \\
  0 & = & 7 %b \\
  %b & = & 0 \\
  %a & = & 3 \\
  4 & \neq & 12 + 0
\end{eqnarray}
          Finally, we check if the third can be a linear combination of the first and second:
\begin{eqnarray}
  #[1 #; 7 #; 4 #; 3 #] & = &  %a \cdot #[6 #; 0 #; 4 #; 3 #] + %b \cdot #[2 #; 0 #; 4 #; 0 #] \\
  7 & = & 0 \cdot %a + 0 \cdot %b \\
  7 & \neq & 0
\end{eqnarray}          
          Since no vector is a linear combination of the other two, the collection <b>is linearly
          independent</b>.
        </solution>
      </li>
    </ol>
  </li>
  
  <li> Consider the following vectors in \R^3:
\begin{eqnarray}
  %u & = & #[3 #; 7 #; 9 #] \\
  %v & = & #[2 #; ---5 #; 3 #] \\
  %w & = & #[3 #; 4 #; ---3 #]
\end{eqnarray}

    <ol style="list-style-type:lower-alpha;">
      <li> Compute the orthogonal projection of %w onto the vector:
\begin{eqnarray}
  #[1/\sqrt(12) #; 1/\sqrt(12) #; 1/\sqrt(12) #]
\end{eqnarray}
        <solution>
        We use the formula for an orthogonal projection. First, we compute the norm of
        the vector onto which we are projecting.
\begin{eqnarray}
  || #[1/\sqrt(12) #; 1/\sqrt(12) #; 1/\sqrt(12) #] || & = & \sqrt(3/12)\\
\end{eqnarray}
        Notice that 2 \cdot \sqrt(3/12) = \sqrt((4 \cdot 3)/12) = 1. Thus,
        to obtain a unit vector, we simply multiply the vector onto which we are projecting
        by the scalar 2.
\begin{eqnarray}
  2 \cdot #[1/\sqrt(12) #; 1/\sqrt(12) #; 1/\sqrt(12) #] & = & #[2/\sqrt(12) #; 2/\sqrt(12) #; 2/\sqrt(12) #]
\end{eqnarray}
        To project %w onto the above unit vector, we can use the orthogonal projection formula:
\begin{eqnarray}
  (#[3 #; 4 #; -3 #] \cdot #[2/\sqrt(12) #; 2/\sqrt(12) #; 2/\sqrt(12) #]) \cdot #[2/\sqrt(12) #; 2/\sqrt(12) #; 2/\sqrt(12) #] 
        & = & (8/\sqrt(12)) \cdot #[2/\sqrt(12) #; 2/\sqrt(12) #; 2/\sqrt(12) #] \\
        & = & #[16/12 #; 16/12 #; 16/12 #] \\
        & = & #[4/3 #; 4/3 #; 4/3 #]
\end{eqnarray}
        </solution>
      </li>
      <li> Determine whether each of the following points lies on the plane perpendicular to %u:
\begin{eqnarray}
  #[3 #; 0 #; -1 #] & , &  #[7 #; -9 #; -5 #]   & , &  #[1 #; 1 #; -1 #]
\end{eqnarray}
        <solution>
        If a vector lies on the plane perpendicular to %u, then it must be orthogonal to %u.
        Thus, we compute the dot product of each of these vectors with %u:
\begin{eqnarray}
  #[3 #; 0 #; -1 #] \cdot #[3 #; 7 #; 9 #] & = & 9 + 0 - 9 & = & 0, so this vector is on the plane; \\
  #[7 #; -9 #; -5 #] \cdot #[3 #; 7 #; 9 #] & = & 21 - 63 - 45 & \neq & 0, so this vector is not on the plane; \\
  #[1 #; 1 #; -1 #] \cdot #[3 #; 7 #; 9 #] & = & 3 + 7 - 9 & \neq & 0, so this vector is not on the plane.
\end{eqnarray}
        </solution>
      </li>
      <li> <b>Extra credit:</b> Given the vector %v and %w, let %P be the plane orthogonal to %v, and let %Q be the plane orthogonal to %w.
      Find a vector %p \in \R^3 that lies on the line in \R^3 along which %P and %Q intersect, and provide a definition of the line.
        <solution>
          We have the following:
\begin{eqnarray}
  %P & = &  { %p &nbsp;|&nbsp; %p \cdot %v = 0 } \\
  %Q & = &  { %p &nbsp;|&nbsp; %p \cdot %w = 0 }
\end{eqnarray}        
          The line along which %P and %Q intersect is the set of vectors that are orthogonal to both
          %P and %Q.
\begin{eqnarray}
  %P \cap %Q & = &  { %p &nbsp;|&nbsp; %p \cdot %v = 0 &nbsp;and&nbsp; %p \cdot %w = 0}
\end{eqnarray}
          We can expand the two equations %p \cdot %v = 0 and %p \cdot %w = 0 in order to obtain
          a system of equations that restricts the possible components of %p = [ %x ; %y ; %z ]:
\begin{eqnarray}
  #[ %x #; %y #; %z #] \cdot #[2 #; ---5 #; 3 #] & = & 0 \\
  #[ %x #; %y #; %z #] \cdot #[3 #; 4 #; ---3 #] & = & 0 \\
  2 %x --- 5 %y + 3 %z & = & 0 \\
  3 %x + 4 %y --- 3 %z & = & 0 \\
  5 %x --- %y & = & 0 \\
  %y & = & 5 %x \\
  %z & = & (23/3) %x
\end{eqnarray}
          Given the above, we can introduce a scalar %a and write: 
\begin{eqnarray} 
  %x & = & %a \\
  %y & = & 5 %a \\
  %z & = & (23/3) %a \\
  %P \cap %Q & = & { %a \cdot #[ 1 #; 5 #; (23/3) #] &nbsp;|&nbsp; %a \in \R }
\end{eqnarray}
        </solution>
      </li>
    </ol>
  </li>

  <li> <p>You decide to drive the 2800 miles from New York to Los Angeles in a hybrid vehicle. 
           A hybrid vehicle has two modes: using only the electric motor and battery, it can travel 1 mile on 3 units of battery power; 
           using only the internal combustion engine, it can travel 1 mile on 0.1 liters of gas (about 37 mpg) while also charging the 
           battery with 1 unit of battery power. At the end of your trip, you have 1400 fewer units of battery power than you did when 
           you began the trip. How much gasoline did you use (in liters)?</p>

           <p>You should define a system with the following dimensions:
            <ul>
             <li>net change in the total units of battery power;</li>
             <li>total liters of gasoline used;</li>
             <li>total number of miles travelled;</li>
             <li>number of miles travelled using the electric motor and battery;</li>
             <li>number of miles travelled using the engine.</li>
            </ul>
           </p>
           
           <p>You should define a matrix %M \in \R^{3 \times 2} to characterize this system. Then, write down an equation containing
           that matrix (and three variables in \R), and solve it to obtain the quantity of gasoline.</p>
\begin{eqnarray}
  %M \cdot #[%x#; %y#] & = & #[?#; ?#; ?#]
\end{eqnarray}
    <solution>
    One possible solution is to solve the matrix equation below for %n, the number of liters of gasoline used:
\begin{eqnarray}
 #[ -3 <i style="color:gray;">battery power/mile on battery</i> #, 1 <i style="color:gray;">battery power/mile on engine</i> 
 #; 1 <i style="color:gray;">miles/mile on battery</i> #, 1 <i style="color:gray;">miles/mile on engine</i>
 #; 0 <i style="color:gray;">liters of gas/mile on battery</i> #, -0.1 <i style="color:gray;">liters of gas/mile on engine</i> #] 
 \cdot #[ %x <i style="color:gray;">miles on battery</i> #; %y <i style="color:gray;">miles on engine</i> #] & = & #[ -1400 <i style="color:gray;">battery power</i> #; 2800 <i style="color:gray;">miles</i> #; %n <i style="color:gray;">liters of gas</i> #]
\end{eqnarray}
    The above equation can be rewrittenn as:
\begin{eqnarray}
  ---3 \cdot %x + 1 \cdot %y & = & -1400 \\
   %x + %y & = & 2800 \\
   0 \cdot %x - 0.1 \cdot %y & = & %n
\end{eqnarray}
    We then have that 245 liters were used:
\begin{eqnarray}
   %x & = & 2800 --- %y \\
   ---3 ( 2800 --- %y)  + %y & = & ---1400  \\
   ---8400 + 4 %y & = & ---1400  \\
             4 %y & = & 7000  \\
               %y & = & 1750  \\
      %n & = & --- 0.1 \cdot 1750 \\
      %n & = & ---175
\end{eqnarray}
    Notice that it is possible to solve an equation for a 2 \times 2 matrix first, and then
    use %x and %y to determine %n. For example:
\begin{eqnarray}
 #[ -3 #, 1 #; 1 #, 1 #] \cdot #[%x#; %y#] & = & #[ -1400 #; 2800 #]
\end{eqnarray}
    It is also possible to immediately notice that 10 \cdot %n is the number of miles travelled
    on %n liters of gasoline. Then, the following matrix equation can be set up and solved:
\begin{eqnarray}
 #[ -3 <i style="color:gray;">battery power/mile on battery</i> #, 10 <i style="color:gray;">battery power/liter of gas</i> 
 #; 1 <i style="color:gray;">miles/mile on battery</i> #, 10 <i style="color:gray;">miles/liter of gas</i> #] 
 \cdot #[ %x <i style="color:gray;">miles on battery</i> #; %n <i style="color:gray;">liters of gas</i> #] & = & #[ -1400 <i style="color:gray;">battery power</i> #; 2800 <i style="color:gray;">miles</i> #]
\end{eqnarray}
    </solution>
  </li>

  <li> <p>Suppose we create a very simple system for modelling how predators and prey interact in a closed environment. 
       Our system has only two dimensions: the number of prey animals, and the number of predators. We want to model 
       how the state of the system changes from one generation to the next.</p>
       <p>If there are %x predators and %y prey animals in a given generation, in the next generation the following will be the case:
        <ul>
         <li>all predators already in the system will stay in the system;</li>
         <li>all prey animals already in the system will stay in the system;</li>
         <li>for every prey animal, two new prey animals are introduced into the system;</li>
         <li>for every predator, two prey animals are removed from the system;</li>
         <li>we ignore any other factors that might affect the state (e.g., natural death or starvation).</li>
        </ul>

       <ol style="list-style-type:lower-alpha;">
        <li> <p>Specify explicitly a matrix %T in \R^{2 \times 2} that takes a description of the system state
             in one generation and produces the state of the system during the next generation. <b>Note:</b> you may simply
             enter the matrix on its own line for this part of the problem, but you must also use it in the
             remaining three parts below.</p>
             <solution>
             The following matrix reflects the constraints imposed by the description of a
             state transformation:
\begin{eqnarray}
  %T & = & #[1 <i style="color:gray;"># predators at time %t+1 / predator at time %t</i> 
  #, 0 <i style="color:gray;"># predators at time %t+1 / prey at time %t</i>
  #; -2 <i style="color:gray;"># prey at time %t+1 / predator at time %t</i>
  #, 3 <i style="color:gray;"># prey at time %t+1 / prey at time %t</i>
  #]
\end{eqnarray}
             </solution>
             
        </li>
        <li> <p>Show that the number of predators does not change from one generation to the next.</p>
\begin{eqnarray}
  %T \cdot #[%x#; %y#] & = & #[%x#;%y'#]
\end{eqnarray}
             <solution>
             It is sufficient to derive that the number of predators does not change
             after a transformation is applied. Let %x be the number of predators before
             the transformation is applied, and let %x' be the number of predators after it is applied.
             Then we have that:
\begin{eqnarray}
  #[1 #, 0 #; -2 #, 3 #] \cdot #[%x #; %y#] & = & #[%x' #; %y' #] \\
  1 \cdot %x + 0 \cdot %y & = & %x' \\
  %x & = & %x'
\end{eqnarray}
             </solution>
        </li>
        <li> <p>Determine what initial state [%x_0; %y_0] is such that there is no change in the state from one generation to another.</p>    

\begin{eqnarray}
  %T \cdot #[%x_0#; %y_0#] & = & #[%x_0#; %y_0#]
\end{eqnarray}

             <solution>
             It is sufficient to solve the equation for %x_0 and %y_0:
\begin{eqnarray}
  %T \cdot #[%x_0#; %y_0#] & = & #[%x_0#; %y_0#] \\
  #[1 #, 0 #; -2 #, 3 #] \cdot #[%x_0#; %y_0#] & = & #[%x_0#; %y_0#] \\
  1 \cdot %x_0 + 0 \cdot %y_0 & = & %x_0 \\
  -2 \cdot %x_0 + 3 \cdot %y_0 & = & %y_0
\end{eqnarray}
             The first equation of real numbers above imposes no constraints on %x_0 or %y_0.
             Thus, any vector that satisfies the last equation would be a solution. All vectors
             on the line defined by this equation are solutions:
\begin{eqnarray}
           -2 \cdot %x_0 + 3 \cdot %y_0 & = & %y_0 \\
           -2 \cdot %x_0 & = & -2 %y_0 \\
           %y_0 & = & %x_0
\end{eqnarray}    
             </solution>
        </li>
        <li> <p>Suppose that in the fourth generation of an instance of this system (that is, after the transformation has 
                 been applied three times), we have 2 predators and 56 prey animals. How many predators and prey animals were 
                 in the system in the first generation (before any transformations were applied)? Let [%x; %y] represent the
                 state of the system in the first generation. Set up a matrix equation that involves [%x; %y] and
                 matrix multiplication, and solve it to obtain the answer.</p>
             <solution>
             It is sufficient to solve the following equation for %x and %y:
\begin{eqnarray}
  %T^3 \cdot #[%x#; %y#] & = & #[2 #; 56 #]
\end{eqnarray}
             We solve it below:
\begin{eqnarray}
  #[1 #, 0 #; -2 #, 3 #] \cdot #[1 #, 0 #; -2 #, 3 #] \cdot #[1 #, 0 #; -2 #, 3 #] \cdot #[%x#; %y#] & = & #[2 #; 56 #] \\
  #[1 #, 0 #; -8 #, 9 #] \cdot #[1 #, 0 #; -2 #, 3 #] \cdot #[%x#; %y#] & = & #[2 #; 56 #] \\
  #[1 #, 0 #; -26 #, 27 #] \cdot #[ %x #; %y #] & = & #[ 2 #; 56 #] \\
   %x --- 0 \cdot %y & = & 2 \\
   %x              & = & 2 \\
  ---26 %x + 27 %y & = & 56 \\
             27 %y & = & 108 \\
                %y & = & 4
\end{eqnarray}
             </solution>
        </li>
      </ol>
  </li>
  
</ol>
<hr/><br/>
<!--/assignment2-->



<a name="3.5"></a>
<h3><span class="secn">3.5.</span> Matrix operations and their interpretations</h3>


The following table summarizes the matrix operations that we are considering in this course.

<table class="fig_table">
 <tr>
  <td><b>term</b></td>
  <td><b>definition</b></td>
  <td><b>restrictions</b></td>
  <td><b>general properties</b></td>
 </tr>
 <tr>
  <td>%M_1 + %M_2</td>
  <td>component-wise</td>
  <td>matrices must have<br/>the same number of rows<br/>and columns</td>
  <td>
    commutative,<br/> 
    associative,<br/> 
    has identity (matrix with all 0 components),<br/> 
    has inverse (multiply matrix by -1),<br/>
    scalar multiplication is distributive
  </td>
 </tr>
 <tr>
  <td>%M_1 \cdot %M_2</td>
  <td>row-column-wise <br/> dot products</td>
  <td>columns in %M_1 = rows in %M_2<br/>rows in %M_1 \cdot %M_2 = rows in %M_1<br/>columns in %M_1 \cdot %M_2 = columns in %M_2</td>
  <td>
    associative,<br/>
    has identity \I (1s in diagonal and 0s elsewhere),<br/>
    distributive over matrix addition,<br/>
    <b>not commutative</b> in general,<br/>
    <b>no inverse</b> in general<br/>
  </td>
 </tr>
 <tr>
  <td>%M^{-1}</td>
  <td></td>
  <td>
    columns in %M = rows in %M<br/>
    matrix is invertible
  </td>
  <td>
    %M^{-1} \cdot %M = %M \cdot %M^{-1} = \I
  </td>
 </tr>
</table>

The following tables list some high-level intuitions about how matrix operations can be understood.

<table class="fig_table">
 <tr>
  <td><b>level of<br/>abstraction</b></td>
  <td colspan="4"><b>interpretations of multiplication<br/>of a vector by a matrix</b></td>
 </tr>
 <tr>
  <td>applications</td>
  <td>transformation of<br/>system states</td>
  <td>extraction of information<br/>about system states</td>
  <td>computing properties of<br/>combinations or aggregations<br/>of objects (or system states)</td>
  <td>conversion of system <br/> state observations<br/> from one set of dimensions<br/>to another</td>
 </tr>
 <tr>
  <td>geometry</td>
  <td>"moving" vectors in <br/> a space (stretching,<br/>skewing, rotating,<br/> reflecting)</td>
  <td>projecting vectors</td>
  <td>taking a linear combination<br/> of two vectors</td>
  <td>reinterpreting vector notation<br/>as referring to a collection<br/>of non-canonical vectors</td>
 </tr>
</table>

<table class="fig_table">
 <tr>
  <td><b>level of<br/>abstraction</b></td>
  <td colspan="4"><b>interpretations of multiplication of two matrices</b></td>
 </tr>
 <tr>
  <td>applications</td>
  <td>composition of system state<br/>transformations or conversions</td>
 </tr>
 <tr>
  <td>geometry</td>
  <td>sequencing of motions of vectors within <br/> a space (stretching, skewing, rotating,<br/> reflecting)</td>
 </tr>
</table>

<table class="fig_table">
 <tr>
  <td><b>level of<br/>abstraction</b></td>
  <td colspan="2"><b>invertible matrix</b></td>
  <td colspan="2"><b>singular matrix</b></td>
 </tr>
 <tr>
  <td>applications</td>
  <td>reversible transformation<br/> of system states</td>
  <td>extraction of <i>complete</i> <br/>information uniquely determining<br/>a system state</td>
  <td>irreversible transformation<br/> of system states</td>
  <td>extraction of <i>incomplete</i> <br/>information about<br/>a system state</td>
 </tr>
 <tr>
  <td>geometry</td>
  <td colspan="2">reversible transformation or motion<br/>of vectors in a space</td>
  <td colspan="2">projection onto a <i>strict subset</i> of<br/> a set of vectors (space)</td>
 </tr>
 <tr>
  <td>symbolic</td>
  <td colspan="2">
    reversible transformation of <br/>information numerically encoded in matrix<br/>
    (example of such information: system of<br/>linear equations encoded as matrix)
  </td>
  <td colspan="2">irreversible/"lossy" transformation of<br/>information encoded in matrix</td>
 </tr>
</table>

Suppose we interpret multiplication of a vector by a matrix %M as a function from vectors to vectors:

  $$ %f(%v) = %M %v.$$

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Notice that for any %M, if %f(%v) = %M %v then %f is always a function because %M %v has only one possible result (i.e.,
matrix multiplication is <i>deterministic</i>) for a given %M and %v.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If %f(%v) = %M %v, then %f is invertible if %M is an invertible matrix. The inverse of %f is then defined to be:

  $$ %f^{-1}(%v) = %M^{-1} %v.$$

Notice that f^{-1} is a function because %M^{-1} %v only has one possible result. Notice that %f^{-1} is the inverse of %f because

  $$ %f^{-1}(%f(%v)) = %M^{-1} %M %v = \I %v = %v.$$

</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If the columns of a matrix %M are linearly dependent and %f(%v) = %M %v, then %M cannot have an inverse. We consider the case
in which %M \in \R^{2 \times 2}. Suppose we have that
\begin{eqnarray}
 %M & = & #[%a #, %b #; %c #, %d #].
\end{eqnarray}
If the columns of %M are linearly dependent, then we know that there is some %s \in \R such that
\begin{eqnarray}
 %s #[%a #; %c#] & = & #[%b #; %d#].
\end{eqnarray}
This means that we can rewrite %M:
\begin{eqnarray}
 %M & = & #[%a #, %s%a #; %c #, %s %c #].
\end{eqnarray}
Since matrix multiplication can be interpreted as taking a linear combination of the column vectors, this means that for %x,%y \in \R,
\begin{eqnarray}
 #[%a #, %s%a #; %c #, %s%c #] #[%x #; %y#] & = & #[%a #; c#] %x + #[%s%a #; %s%c#] %y & = & (%x + %s %y) #[%a #; c#]
\end{eqnarray}
But this means that for any two vectors [%x;%y] and [%x';%y'], if %x + %s%y = %x' + %s%y' then multiplying by %M will lead to the same result.
Thus, %f is a function that takes two different vector arguments and maps them to the same result. If we interpret %f as a relation and take
its inverse %f^{-1}, %f^{-1} cannot be a function.

Thus, %M cannot have an inverse (if it did, then %f^{-1} would be a function).
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If the columns of a matrix %M are linearly independent and %f(%v) = %M %v, then %M has an inverse. We consider the case
in which %M \in \R^{2 \times 2}. Suppose we have that
\begin{eqnarray}
 %M & = & #[%a #, %b #; %c #, %d #].
\end{eqnarray}
If the columns of %M are linearly independent, then we know that %a %d - %b % c \neq 0:
\begin{eqnarray}
 %a/%c & \neq & %b/%d \\
 %a %d & \neq & %b %c \\
 %a %d - %b %c& \neq & 0
\end{eqnarray}
Suppose we pick the following %M^{-1}:
\begin{eqnarray}
 %M^{-1} & = & (1/(%a %d - %b %c)) \cdot #[%d #, -%b #; -%c #, %a #].
\end{eqnarray}
The we have that:
\begin{eqnarray}
 %M^{-1} \cdot %M & = & (1/(%a %d - %b %c)) \cdot #[%d #, -%b #; -%c #, %a #] \cdot #[%a #, %b #; %c #, %d #] \\
                  & = & (1/(%a %d - %b %c)) \cdot #[%a %d - %b %c #, %b%d - %b%d #; %a%c - %a%c #, %a %d - %b %c #] \\
                  & = & #[1 #, 0 #; 0 #, 1 #] \\
                  & = & \I
\end{eqnarray}
</div>

<a name="lecture7c"></a>

<div class="mathenv example_to_know">
<b>Example:</b> Solve the following problems.
<ol style="list-style-type:lower-alpha;">
 <li>Determine which of the following matrices are not invertible:
\begin{eqnarray}
  #[2 #, -7 #; -4 #, 14 #] & , &  #[0 #, 0 #; 0 #, 1 #] & , &  #[2 #, 3 #; 0 #, 1 #]
\end{eqnarray}

The columns of the first matrix are linearly dependent, so it is not invertible. The first column of
the second matrix can be obtained by multiplying the second column by 0, so the two columns of that
matrix are linearly dependent; thus, it is not invertible. For the third matrix, the following equation
has no solution:
\begin{eqnarray}
  #[ 2 #; 0 #] & = & %s \cdot #[ 3 #; 1 #]
\end{eqnarray}
Thus, the third matrix is invertible. It is also possible to determine this by computing the
determinant for each matrix.
 </li>
 <li>
  The matrix below is <b>not</b> invertible, and the following equation is true. What is the matrix? List all four of its components (real numbers).
\begin{eqnarray}
  #[%a #, %b #; %b #, %c #] \cdot  #[1 #; 2 #] & = &  #[5 #; 10 #]
\end{eqnarray}
  Since the matrix is not invertible, it must be that its determinant is 0. Thus, we have the
  following system of equations:
\begin{eqnarray}
  %a + 2 %b & = & 5 \\
  %b + 2 %c & = & 10 \\
  %a %c --- %b^2 & = & 0
\end{eqnarray}
  If we solve the above, we get:
\begin{eqnarray}
  %a & = & 1 \\
  %b & = & 2 \\
  %c & = & 4
\end{eqnarray}
 </li>
</ol>
</div>

<a name="lecture8a"></a>

<a name="3.6"></a>
<h3><span class="secn">3.6.</span> Matrix properties</h3>

The following are subsets of \R^{n \times n} that are of interest in this course because they
correspond to transformations and systems that have desirable or useful properties. For some of these
sets of matrices, matrix multiplication and inversion have properties that they do not in general.

<table class="fig_table">
 <tr>
  <td><b>subset of \R^{n \times n}</b></td>
  <td><b>definition</b></td>
  <td><b>closed under<br/>matrix<br/>multiplication</b></td>
  <td><b>properties of<br/>matrix multiplication</b></td>
  <td><b>inversion</td>
 </tr>
 <tr>
  <td>identity matrix</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = 1 if %i=%j, 0 otherwise </td>
  <td>closed</td>
  <td><b>commutative</b>,<br/> associative,<br/> distributive with addition, <br/>has identity</td>
  <td>has inverse (itself);<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>elementary matrix</td>
  <td>
    can be obtained via an<br/>elementary row operation<br/>from \I:
    <ul style="margin-right:-40px; padding-right:-40px;">
     <li>add <b>nonzero</b> multiple <br/>of one row of the matrix<br/>to another row</li>
     <li>multiply a row by a<br/><b>nonzero</b> scalar</li>
     <li>swap two rows of the<br/>matrix</li>
    </ul>
    Note: the third is a combination<br/>of the first two operations.
  </td>
  <td></td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>scalar matrices</td>
  <td>\exists %s \in \R, \forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = %s if %i=%j, 0 otherwise </td>
  <td>closed</td>
  <td><b>commutative</b>,<br/> associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>diagonal matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} \in \R if %i=%j, 0 otherwise </td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>matrices with<br/>constant diagonal</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ii} = %M_{jj}</td>
  <td></td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
 <tr>
  <td>symmetric matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = %M_{ji}</td>
  <td></td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
 <tr>
  <td>symmetric matrices<br/>with constant diagonal</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ii} = %M_{jj} and %M_{ij} = %M_{ji}</td>
  <td>closed</td>
  <td><b>commutative</b>, <br/>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
 <tr>
  <td>upper triangular matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = 0 if i > j</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td><b>not</b> invertible in general;<br/>closed under inversion<br/>when invertible</td>
 </tr>
 <tr>
  <td>lower triangular matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = 0 if i < j</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td><b>not</b> invertible in general;<br/>closed under inversion<br/>when invertible</td>
 </tr>
 <tr>
  <td>invertible matrices</td>
  <td>\exists %M^{-1} s.t. %M^{-1} %M = %M %M^{-1} = \I</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>square matrices</td>
  <td>all of \R^{n \times n}</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
</table>

Two facts presented in the above table are worth noting.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose %A is invertible. Then the inverse of %A^{-1} is %A, because %A %A^{-1} = \I. Thus, (%A^{-1})^{-1} = %A.
</div>


<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If %A and %B are invertible, so is %A%B. That is, invertible matrices are closed under matrix multiplication. We can show
this by using the associativity of matrix multiplication. Since %A and %B are invertible, there exist %B^{-1} and %A^{-1} such that:

\begin{eqnarray}
 (%B^{-1} %A^{-1}) %A %B & = & (%B^{-1} %A^{-1}) %A %B\\
                        & = & %B^{-1} (%A^{-1} %A) %B\\
                        & = & %B^{-1} \I %B\\
                        & = & %B^{-1} %B\\
                        & = & \I.
\end{eqnarray}

Thus, since there exists a matrix (%B^{-1} %A^{-1}) such that (%B^{-1} %A^{-1}) %A %B = \I, (%B^{-1} %A^{-1}) is the inverse of %A%B.
</div>








<div class="mathenv example_to_know">
<b>Example:</b> 
Given an invertible upper triangular matrix %M \in \R^{2 \times 2}, 
show that %M^{-1} is also upper triangular. <b>Hint:</b> write out
the matrix %M explicitly.

Suppose we have the following upper triangular matrix:

\begin{eqnarray}
  %M & = & #[ %x #, %y #; 0 #, %z #]
\end{eqnarray}

If it is invertible, then there exists an inverse such that:

\begin{eqnarray}
  #[ %x #, %y #; 0 #, %z #] \cdot #[ %a #, %b #; %c #, %d #] & = & #[ 1 #, 0 #; 0 #, 1 #] 
\end{eqnarray}

This implies that

\begin{eqnarray}
  %x%a + %y%c & = & 1 \\
  %x%b + %y%d & = & 0 \\
         %z%c & = & 0 \\
         %z%d & = & 1
\end{eqnarray}

Because %M is invertible, we know that %z \neq 0. Since %z%c = 0, This means that %c = 0. Thus, we have that the inverse is upper triangular.

Alternatively, we can observe that %c = 0, and that using the formula for the inverse would yield a lower-left
matrix entry equal to ---%c/(\det %M) = 0.
</div>


<div class="mathenv example_to_know">
<b>Example:</b> In terms of %a, %b \in \R where %a \neq 0 and %b \neq 0, compute the inverse of:

  $$ #[ %a #, 0 #; 0 #, %b #] #[ %a #, 0 #; 0 #, %b #] #[ %a #, 0 #; 0 #, %b #] #[ %a #, 0 #; 0 #, %b #] $$

While we could perform the instances of matrix multiplication step-by-step and then invert the result (either by solving an equation or
using the formula for the inverse of a matrix in \R^{2 \times 2}), it's easier to recall that diagonal matrices behave in a manner that is very
similar to the real numbers. Thus, the above product is equal to

  $$ #[ %a^4 #, 0 #; 0 #, %b^4 #], $$

and its inverse simply has the multiplicative inverses of the two diagonal entries as its diagonal entries:

  $$ #[ 1/%a^4 #, 0 #; 0 #, 1/%b^4 #].$$

</div>













Another fact not in the table is also worth noting.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Any product of a finite number of elementary row matrices is invertible. This fact follows from the fact that all
elementary matrices are invertible, and that the set of invertible matrices is closed under multiplication.
</div>

Given the last fact, we might ask whether the opposite is true: are all invertible matrices the product of a finite number of elementary
matrices? The answer is <b>yes</b>, as we will see further below.



<a name="3.7"></a>
<h3><span class="secn">3.7.</span> Solving the equation %M %v = %w for %M with various properties</h3>

Recall that for %v,%w \in \R^n and %M \in \R^{n \times n}, an equation of the following form
can represent a system of equations:

  $$%M %v = %w$$

Notice that if %M is invertible, we can solve for %v by multiplying both sides by %M^{-1}. More generally, if %M is a member of
some of the sets in the above table, we can find straightforward algorithms for solving such an equation for %v.

<table class="fig_table">
 <tr>
  <td><b>%M is ... </b></td>
  <td><b>algorithm to solve %M %v = %w for %v</b></td>
 </tr>
 <tr>
  <td>the identity matrix</td>
  <td>%w is the solution</td>
 </tr>
 <tr>
  <td>an elementary matrix</td>
  <td>perform a row operation on %M to obtain \I;<br/>perform the same operation on %w</td>
 </tr>
 <tr>
  <td>a scalar matrix</td>
  <td>divide the components of %w by the scalar</td>
 </tr>
 <tr>
  <td>a diagonal matrix</td>
  <td>divide each component of %w by the<br/>corresponding matrix component</td>
 </tr>
 <tr>
  <td>an upper triangular matrix</td>
  <td>
    start with the last entry in %v, which is easily obtained;<br/>
    move backwards through %v, filling in the values by<br/>
    substituting the already known variables
  </td>
 </tr>
 <tr>
  <td>a lower triangular matrix</td>
  <td>
    start with the first entry in %v, which is easily obtained;<br/>
    move forward through %v, filling in the values by<br/>
    substituting the already known variables
  </td>
 </tr>
 <tr>
  <td>
    product of a lower triangular matrix<br/>
    and an upper triangular matrix</td>
  <td>
    combine the algorithms for upper and lower triangular<br/>
    matrices in sequence (see example below)
  </td>
 </tr>
 <tr>
  <td>an invertible matrix</td>
  <td>compute the inverse and multiply %w by it</td>
 </tr>
</table>

<div class="mathenv example_to_know">
<b>Example:</b> We consider an equation %M %v = %w where %M is a diagonal matrix (the identity matrix and all scalar matrices are also diagonal
matrices).

\begin{eqnarray}
 #[ 4 #, 0 #, 0 #;  0 #, 3 #, 0 #;0 #, 0 #, 5 #] #[%x #; %y #; %z#] & = & #[2 #; 9 #; 10 #] \\
       4%x & = & 2 \\
       3%y & = & 9 \\
       5%z & = & 10 \\
       %x & = & 1/2 \\
       %y & = & 3 \\
       %z & = & 2
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> We consider an equation %M %v = %w where %M is a lower triangular matrix.

\begin{eqnarray}
 #[ 4 #, 0 #, 0 #;  2 #, 4 #, 0 #; 4 #, 3 #, 5 #] #[%x #; %y #; %z#] & = & #[2 #; 9 #; 18 #] \\
       4%x & = & 2 \\
       2%x + 4%y & = & 9 \\
       4%x + 3%y + 5%z & = & 10 \\
       %x & = & 1/2 \\
       %y & = & 2 \\
       %z & = & 2
\end{eqnarray}
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> Suppose that %M = %L %U where %L is lower triangular and %U is upper triangular. How can we solve %M %v = %w?

First, note that because matrix multiplication is associative, we have

\begin{eqnarray}
 %M %v & = & (%L %U) %v \\
       & = & %L (%U %v)
\end{eqnarray}

We introduce a new vector for the intermediate result %U %v, which we call %u. Now, we have a system of matrix equations.

\begin{eqnarray}
 %U %v & = & %u \\
 %L %u & = & %w
\end{eqnarray}

We first solve for %u using the algorithm for lower triangular matrices, then we solve for %v using the algorithm for upper triangular
matrices.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Solve the following equation for %x,%y,%z \in \R:
\begin{eqnarray}
 #[ ---2 #, 0 #, 0 #; 1 #, 2 #, 0 #; 1 #, ---1 #, ---1 #] \cdot #[ 3 #, 0 #, 1 #; 0 #, 1 #, 2 #; 0 #, 0 #, 1 #] \cdot #[ %x #; %y #; %z #] & = & #[ ---8 #; 12 #; ---1 #]
\end{eqnarray}

We first divide the problem into two steps using the intermediate vector [ %a ; %b ; %c ]:
\begin{eqnarray}
 #[ -2 #, 0 #, 0 #; 1 #, 2 #, 0 #; 1 #, ---1 #, ---1 #] \cdot #[ %a #; %b #; %c #] & = & #[ ---8 #; 12 #; ---1 #] \\
 #[ 3 #, 0 #, 1 #; 0 #, 1 #, 2 #; 0 #, 0 #, 1 #] \cdot #[ %x #; %y #; %z #] & = & #[ %a #; %b #; %c #] \\
 #[ %a #; %b #; %c #] & = & #[ 4 #; 4 #; 1 #] \\
 #[ %x #; %y #; %z #] & = & #[ 1 #; 2 #; 1 #]
\end{eqnarray}

</div>

<div class="mathenv example_to_know">
<b>Example:</b> The inverse of a matrix in \R^{2 \times 2} can be computed as follows.

\begin{eqnarray}
 #[ %a #, %b #;  %c #, %d #]^{#-1} & = & #[ %d/(%a%d --- %b%c) #, ---%b/(%a%d-%b%c) #;  ---%c/(%a%d --- %b%c) #, %a/(%a%d --- %b%c) #]
\end{eqnarray}

Thus, if we find that the determinant of a matrix %M \in \R^{2 \times 2} is nonzero, the algorithm for solving %M %v = %w is
straightforward. Consider the following example.

\begin{eqnarray}
 #[ 1 #, 2 #;  3 #, 4 #] #[%x #; %y #] & = & #[ 5 #; 13 #] \\
 #[ 1 #, 2 #;  3 #, 4 #]^{#-1} #[ 1 #, 2 #;  3 #, 4 #] #[%x #; %y #] & = &  #[ 1 #, 2 #;  3 #, 4 #]^{#-1} #[ 5 #; 13 #] \\
 #[ 1 #, 0 #;  0 #, 1 #] #[%x #; %y #] & = &  
       #[ 4/((1\cdot4)-(2\cdot3)) #, ---2/((1\cdot4)-(2\cdot3)) #;  ---3/((1\cdot4)-(2\cdot3)) #, 1/((1\cdot4)-(2\cdot3)) #] #[ 5 #; 13 #] \\
 #[ 1 #, 0 #;  0 #, 1 #] #[%x #; %y #] & = &  #[ ---2 #, 1 #;  3/2 #, 1/---2 #] #[ 5 #; 13 #] \\
 #[%x #; %y #] & = &  #[ 3 #;  1 #]
\end{eqnarray}
</div>

<a name="lecture9"></a>

<a name="3.8"></a>
<h3><span class="secn">3.8.</span> Row echelon form and reduced row echelon form</h3>

We define two more properties that a matrix may possess.

<table class="fig_table">
 <tr>
  <td>%M is in row echelon form</td>
  <td>
    <ul style="margin-right:0px; padding-right:-40px; margin-bottom:0px;">
      <li>all nonzero rows are above any rows consisting of all zeroes</li>
      <li>the first nonzero entry (from the left) of a nonzero row is strictly<br/>to the right of the first nonzero entry of the row above it</li>
      <li>all entries in a column below the first nonzero entry in a row are zero<br/>(the first two conditions imply this)</li>
    </ul>
  </td>
 </tr>
 <tr>
  <td>%M is in reduced row echelon form</td>
  <td>
    <ul style="margin-right:0px; padding-right:-40px; margin-bottom:0px;">
      <li>%M is in row echelon form</li>
      <li>the first nonzero entry in every row is 1;<br/>this 1 entry is the only nonzero entry in its column</li>
    </ul>
  </td>
 </tr>
</table>

We can obtain the reduced row echelon form of a matrix using a sequence of appropriately chosen elementary row operations.

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we want to find the reduced row echelon form of the matrix below. We list the steps of the procedure.

\begin{eqnarray}
 #[ 1 #, 2 #, 1 #;  -2 #, -3 #, 1 #;  3 #, 5 #, 0 #] \to  #[ 1 #, 2 #, 1 #;  0 #, 1 #, 3 #;  3 #, 5 #, 0 #]
                                                     \to  #[ 1 #, 2 #, 1 #;  0 #, 1 #, 3 #;  0 #, -1 #, -3 #]
                                                     \to  #[ 1 #, 2 #, 1 #;  0 #, 1 #, 3 #;  0 #, 0 #, 0 #]
                                                     \to  #[ 1 #, 0 #, -5 #;  0 #, 1 #, 3 #;  0 #, 0 #, 0 #]
\end{eqnarray}
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> For any matrix %M, there may be more than one way to reach the reduced row echelon form using elementary row operations. However,
it is always possible, and there is exactly one unique reduced row echelon form for that matrix %M. We do not prove this result in this course, 
but fairly short proofs by induction can be found elsewhere (such as <a href="http://mathdl.maa.org/images/cms_upload/Yuster19807.pdf">here</a>).
Because the reduced row echelon form of a matrix %M is unique, we use the following notation to denote it:

  $$ \rref %M.$$
</div>

<a name="lecture10a"></a>

<div class="mathenv example_to_know">
<b>Example:</b> Determine which of the following matrices are elementary:

\begin{eqnarray}
 #[ 1 #, 0 #, 0 #;  0 #, 1 #, 0 #] & , &
 #[ 0 #, 0 #, 2 #;  0 #, 1 #, 0 #;  1 #, 0 #, 0 #] & , & 
 #[ 1 #, 0 #, 0 #;  0 #, 1 #, 0 #;  0 #, 0 #, 0 #] & , & 
 #[ 1 #, 0 #, 0 #;  0 #, 1 #, 0 #;  -2 #, 0 #, 1 #]
\end{eqnarray}

The matrices are:
(a) not elementary (not square, so not invertible),
(b) not elementary (composition of two elementary row operations applied to the identity),
(c) not elementary (multiplication of a row by the 0 scalar is not invertible),
and (d) elementary (multiple of first row added to last row).
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Determine which of the following matrices are in reduced row echelon form:
\begin{eqnarray}
 #[ 1 #, 0 #, -2 #;  0 #, 1 #, 4 #] & , &
 #[ 1 #, 0 #, 0 #;  0 #, 1 #, 0 #;  0 #, 1 #, 3 #;  0 #, 0 #, 1 #] & , & 
 #[ 1 #, 0 #;  0 #, 1 #;  0 #, 0 #] & , & 
 #[ 1 #, 0 #;  0 #, 1 #;  0 #, 1 #; 0 #, 0 #] & , &
 #[ 0 #, 0 #, 0 #, 1 #, 0 #;  0 #, 0 #, 0 #, 0 #, 1 #]
\end{eqnarray}

The matrices are:
(a) in reduced row echelon form,
(b) <b>not</b> in reduced row echelon form,
(c) in reduced row echelon form,
(d) <b>not</b> in reduced row echelon form,
and (e) in reduced row echelon form.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose the matrix below is in reduced row echelon form. Solve for %a, %b \in \R.
\begin{eqnarray}
 #[ 1 #, %a #, &nbsp;&nbsp;&nbsp;0 #;  0 #, %b #, &nbsp;&nbsp; 1 #;  0 #, 0 #, 1 --- %a #]
\end{eqnarray}

Without loss of generality, we can focus on four possibility: %a is either zero or nonzero, and
%b is either zero or nonzero. We can further simplify this by considering 1 as the only nonzero
value of interest. Then we have that:
<ul>
  <li>if %a = 0 and %b = 0, then the matrix is not in reduced row echelon form;</li>
  <li>if %a = 0 and %b = 1, then the matrix is not in reduced row echelon form;</li>
  <li><b>if %a = 1 and %b = 0, then the matrix is in reduced row echelon form;</b></li>
  <li>if %a = 1 and %b = 1, then the matrix is not in reduced row echelon form.</li>
</ul>
Thus, %a = 1 and %b = 0.
</div>

<b>Question:</b> For a given matrix in reduced row echelon form, how many different matrices can be reduced to it (one or more than one)?

<div class="mathenv proposition_to_know">
<b>Fact:</b> It is always true that \rref %M = \rref (\rref %M). Thus, the \rref operation is <i>idempotent</i>.
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> If \rref %M is \I (the identity matrix), then %M is invertible. This is because \I is an elementary matrix, and the row operations 
used to obtain \rref %M from %M can be represented as a product of elementary (and thus, invertible) matrices %E_1 \cdot ... \cdot %E_%n. Thus, we have:

\begin{eqnarray}
%E_1 \cdot ... \cdot %E_%n \cdot %M & = & \I \\
(%E^{-1}_%n \cdot ... \cdot %E^{-1}_1) \cdot %E_1 \cdot ... \cdot %E_%n \cdot %M & = & (%E^{-1}_%n \cdot ... \cdot %E^{-1}_1) \cdot \I \\
                                                                              %M & = & (%E^{-1}_%n \cdot ... \cdot %E^{-1}_1) \cdot \I
\end{eqnarray}

Since elementary matrices and \I are invertible, and invertible matrices are closed under matrix multiplication, %M must be invertible, too.
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> A matrix %M \in \R^{n \times n} is not invertible iff the bottom row of \rref %M has all zeroes. This is because when all
rows of \rref %M \in \R^{n \times n} have at least one nonzero value,  \rref %M must be the identity matrix (try putting a nonzero value on the bottom row, and see what the
definition of reduced row echelon form implies about the rest of the matrix). Since \rref %M = \I, this implies that %M must then be invertible by the fact immediately above this one.
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> For any invertible matrix %M \in \R^{n \times n}, the reduced row echelon form of %M is \I \in \R^{n \times n}.

We can show this is true using a proof by contradiction. Suppose that %M is invertible, but \rref %M \neq \I. We know that \rref %M can be
obtained via a finite number of elementary row operations %E_1 \cdot ... \cdot %E_%n:

  $$(%E_1 \cdot ... \cdot %E_%n) %M = \rref %M.$$

If \rref %M is not \I, then the last row of \rref %M must consist of only zeroes. But because %M is invertible, we have:

\begin{eqnarray}
 ((%E_1 \cdot ... \cdot %E_%n) %M) %M^{-1} & = & (\rref %M) \cdot %M^{-1}\\
     %E_1 \cdot ... \cdot %E_%n & = & (\rref %M) \cdot %M^{-1}
\end{eqnarray}

Since the product of elementary matrices is invertible, (\rref %M) \cdot %M^{-1} is also invertible. But if the last row of \rref %M consists of only
zeroes, then the last row of (\rref %M) %M^{-1} also contains only zeroes, so it cannot be that (\rref %M) %M^{-1} is invertible. Thus, we have a contradiction, so our assumption that \rref %M \neq \I is false.

The following table provides an alternative illustration of how the contradiction is derived:

<table class="fig_table">
 <tr>
  <td align="center">%M is invertible</td>
  <td align="center">\rref %M \neq \I</td>
 </tr>
 <tr>
  <td align="center">the matrix %M^{-1} exists</td>
  <td align="center">the last row of \rref %M is all zeroes</td>
 </tr>
 <tr>
  <td align="center" colspan="2">(%E_1 \cdot ... \cdot %E_%n) %M = \rref %M</td>
 </tr>
 <tr>
  <td align="center" colspan="2">((%E_1 \cdot ... \cdot %E_%n) %M) %M^{-1} = (\rref %M) \cdot %M^{-1}</td>
 </tr>
 <tr>
  <td align="center">%E_1 \cdot ... \cdot %E_%n = (\rref %M) \cdot %M^{-1}</td>
  <td align="center">the last row of  (\rref %M) \cdot %M^{-1}<br/>is all zeroes</td>
 </tr>
 <tr>
  <td align="center">(\rref %M) \cdot %M^{-1} is invertible<br/>because it is a product of the<br/>invertible matrices %E_1, ..., %E_%n</td>
  <td align="center">(\rref %M) \cdot %M^{-1} is not invertible<br/>because multiplication by it is<br/>a many-to-one function</td>
 </tr>
</table>

</div>

The above result implies the following fact.

<div class="mathenv proposition_to_know">
<b>Fact:</b> If a matrix %M is invertible, it is the product of a finite number of elementary matrices. This is because \rref %M is
the identity, which is an elementary matrix, and %M can be reduced via a finite number of invertible row operations to \I. Thus, the
elementary matrices can be used to generate every possible invertible matrix.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose that for some matrix %M \in \R^{2 \times 2}, the following row operations can be applied to %M (in the order
specified) to obtain the identity matrix \I:
<ul>
  <li>add the bottom row to the top row;</li>
  <li>swap the two rows;</li>
  <li>multiply the bottom row by 1/3.</li>
</ul>
Find the matrix %M^{-1}.

We know that performing the three row operations on %M will result in \I. Thus,
we can write out the row operations as three matrices %E_1, %E_2, and %E_3:
\begin{eqnarray}
  %E_3 \cdot %E_2 \cdot %E_1 \cdot %M & = & \I \\
  #[ 1 #, 0 #; 0 #, 1/3 #]  \cdot #[ 0 #, 1 #; 1 #, 0 #] \cdot #[ 1 #, 1 #; 0 #, 1 #] 
  \cdot %M & = & \I \\
\end{eqnarray}
Thus, we have that:
\begin{eqnarray}
  %M^{-1} & = & #[ 1 #, 0 #; 0 #, 1/3 #]  \cdot #[ 0 #, 1 #; 1 #, 0 #] \cdot #[ 1 #, 1 #; 0 #, 1 #] \\
          & = & #[ 0 #, 1 #; 1/3 #, 1/3 #]

\end{eqnarray}
We can also find %M:
\begin{eqnarray}
  \det %M & = & ---1/3 \\ 
  %M & = & 1/(---1/3) \cdot #[ 1/3 #, ---1 #; ---1/3 #, 0 #] \\
     & = & #[ ---1 #, 3 #; 1 #, 0 #]
\end{eqnarray}
</div>

The following table summarizes the results.

<table class="fig_table">
 <tr>
  <td colspan="4"><b>fact</b></td>
  <td><b>justification</b></td>
 </tr>
 <tr>
  <td><b>(1)</b></td>
  <td>{%M | %M is a finite product of elementary matrices}</td>
  <td>=</td>
  <td>{%M | \rref %M = \I}</td>
  <td>\I is an elementary matrix;<br/>sequences of row operations<br/>are equivalent to multiplication by<br/>elementary matrices</td>
 </tr>
 <tr>
  <td><b>(2)</b></td>
  <td>{%M | %M is a finite product of elementary matrices}</td>
  <td>\subset</td>
  <td>{%M | %M is invertible}</td>
  <td>elementary matrices are invertible;<br/>products of invertible matrices are invertible</td>
 </tr>
 <tr>
  <td><b>(3)</b></td>
  <td>{%M | \rref %M = \I}</td>
  <td>\subset</td>
  <td>{%M | %M is invertible}</td>
  <td>fact <b>(1)</b> in this table;<br/>fact <b>(2)</b> in this table;<br/>transitivity of equality</td>
 </tr>
 <tr>
  <td><b>(4)</b></td>
  <td>{%M | %M is invertible}</td>
  <td>\subset</td>
  <td>{%M | \rref %M = \I}</td>
  <td>proof by contradiction;<br/>non-invertible %M implies<br/> \rref %M has all zeroes in bottom row</td>
 </tr>
 <tr>
  <td><b>(5)</b></td>
  <td>{%M | %M is invertible}</td>
  <td>=</td>
  <td>{%M | \rref %M = \I}</td>
  <td>for any sets %A,%B,<br/>%A \subset %B and %B \subset %A<br/>implies %A = %B</td>
 </tr>
 <tr>
  <td><b>(6)</b></td>
  <td>{%M | %M is a finite product of elementary matrices}</td>
  <td>=</td>
  <td>{%M | %M is invertible}</td>
  <td>fact <b>(1)</b> in this table;<br/>fact <b>(5)</b> in this table;<br/>transitivity of equality</td>
 </tr>
</table>

Given all these results, we can say that the properties for a matrix %M in the following table are all equivalent:
if any of these is true, then all of them are true. If any of these is false, then all of them are false.

<table class="fig_table">
 <tr><td>%M is invertible</td></tr>
 <tr><td>\det %M \neq 0</td></tr>
 <tr><td>the columns of %M are (setwise) linearly independent</td></tr>
 <tr><td>%M%v = %w has exactly one solution</td></tr>
 <tr><td>%M is a finite product of elementary matrices</td></tr>
</table>

<div class="mathenv proposition_to_know">
<b>Fact:</b> For matrices %M \in \R^{n \times n}, \rref %M is guaranteed to be upper triangular. Notice that this means that for some finite
product of elementary matrices %E_1 \cdot ... \cdot %E_%n, it is the case that

  $$%M = (%E_1 \cdot ... \cdot %E_%n) \cdot %U.$$
</div>

If %E_1 ,..., %E_%n are all lower triangular, then then %M has an %L%U decomposition. However, this will not always be the case. But recall that
%E_1 ,..., %E_%n are all elementary matrices.

<div class="mathenv proposition_to_know">
<b>Fact:</b> Given any product of elementary matrices %E_1 \cdot ... \cdot %E_%n, it is possible to find a lower triangular matrix
%L by applying a finite number of elementary <i>swap</i> operations
%S_1 ,..., %S_%n such that %L = %S_1 \cdot ...  \cdot %S_%n \cdot %E_1 \cdot ... \cdot %E_%n is lower triangular; then we have:
\begin{eqnarray}
 %L & = & %S_1 \cdot ...  \cdot %S_%n \cdot %E_1 \cdot ... \cdot %E_%n\\
 (%S_%n^{-1} \cdot ...  \cdot %S_1^{-1}) \cdot %L & = & %E_1 \cdot ... \cdot %E_%n
\end{eqnarray}
Thus, %E_1 \cdot ... \cdot %E_%n can be decomposed into a lower triangular matrix and a product of elementary swap matrices.
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> Any product of a finite number of <i>swap</i> elementary matrices %S_1 ,..., %S_%n is a permutation matrix.
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> Any matrix %M can be written as the product of three matrices %P \cdot %L \cdot %U where %P is a permutation matrix, %L is a lower-triangular matrix, and %U is an upper-triangular matrix,
where:
\begin{eqnarray}
 %P & = & %S_1 \cdot ... \cdot %S_%n\\
 %L & = & %E_1 \cdot ... \cdot %E_%k\\
 %U & = & \rref %M\\
\end{eqnarray}
</div>




<div class="mathenv example_to_know">
<b>Example:</b> Suppose we have a system in which a single object is traveling along one spatial dimension (i.e., in a straight line).
The object has a distance travelled, a velocity, and an acceleration; these are the three dimensions of the system:
<ul>
 <li>distance (<i style="color:gray;">m</i>)</li>
 <li>velocity (<i style="color:gray;">m/s</i>)</li>
 <li>acceleration (<i style="color:gray;">m/s^2</i>)</li>
</ul>

Consider the following equations that can be used to compute the distance the object travels and its final velocity given its 
acceleration %a \in \R^{+}, initial velocity %v_0 \in \R^{+}, and the amount of time %t \in  \in \R^{+} it travels.
\begin{eqnarray}
 %d & = & 0.5 %a %t^2 + %v_0 %t \\
 %v & = & %a %t
\end{eqnarray}
Suppose we want to convert a view of the system state that describes the acceleration and velocity at time 0 into a view of the system
state that represents the distance travelled and velocity at time %t. This conversion operation can be represented using a matrix %M.
\begin{eqnarray}
 %M & = & #[ 0.5 %t^2 <i style="color:gray;"><b>dist. <span style="color:firebrick;">at %t+1</span></b> / <b>accel. <span style="color:firebrick;">at %t</span></b></i> #, %t <i style="color:gray;"><b>dist. <span style="color:firebrick;">at %t+1</span></b> / <b>vel. <span style="color:firebrick;">at %t</span></b></i> #; 
    %t <i style="color:gray;"><b>vel. <span style="color:firebrick;">at %t+1</span></b> / <b>accel. <span style="color:firebrick;">at %t</span></b> </i> #,  1 <i style="color:gray;"><b>vel. <span style="color:firebrick;">at %t+1</span></b> / <b>vel. <span style="color:firebrick;">at %t</span></b> </i> #]\\
   & = & #[ 0.5 %t^2 <i style="color:gray;">m / (m/s^2)</i> #, %t <i style="color:gray;">m / (m/s)</i> #; 
    %t <i style="color:gray;">(m/s) /(m/s^2) </i> #,  1 <i style="color:gray;">(m/s)/(m/s) </i> #] \\
    & = & #[ 0.5 %t^2 <i style="color:gray;">s^2</i> #, %t <i style="color:gray;">s</i> #; 
    %t <i style="color:gray;">s </i> #,  1 <i style="color:gray;">scalar identity</i> #]
\end{eqnarray}
This matrix is invertible. This immediately tells us that it is possible to derive the initial
acceleration and velocity given only the amount of time that has elapsed,
the distance travelled, and the final velocity.

By computing the inverse of this matrix, we can obtain formulas that allow us to derive the
initial velocity and acceleration of an object given how much time has passed, how far it has travelled, and its current velocity.
\begin{eqnarray}
 %M^{-1} 
   & = & 
     #[ -2/%t^2 <i style="color:gray;"><b><b>accel. <span style="color:firebrick;">at %t</span></b> / dist. <span style="color:firebrick;">at %t+1</span></b></i> 
     #, %t <i style="color:gray;"><b>accel. <span style="color:firebrick;">at %t</span></b> / <b>vel. <span style="color:firebrick;">at %t+1</span></b></i> 
     #; %t <i style="color:gray;"><b>vel. <span style="color:firebrick;">at %t</span></b> / <b>dist. <span style="color:firebrick;">at %t+1</span></b> </i> 
     #,  1 <i style="color:gray;"><b>vel. <span style="color:firebrick;">at %t</span></b> / <b>vel. <span style="color:firebrick;">at %t+1</span></b> </i>
     #] \\
   & = & 
     #[ -2/%t^2 <i style="color:gray;">(m/s^2)/m</i>
     #, 2/%t <i style="color:gray;">(m/s^2) / (m/s)</i>
     #; 2/%t <i style="color:gray;">(m/s) / m </i>
     #,  -1 <i style="color:gray;">(m/s)/(m/s) </i>
     #] \\
   & = & 
     #[ -2/%t^2 <i style="color:gray;">1/s^2</i>
     #, 2/%t <i style="color:gray;">1 / s</i>
     #; 2/%t <i style="color:gray;">1 / s </i>
     #,  -1 <i style="color:gray;">scalar identity</i>
     #]
\end{eqnarray}
The formulas can be obtained by multiplying %M^{-1} by a system state describing the distance travelled and current velocity.
This yields:
\begin{eqnarray}
 %a & = & -2%d/%t^2 + 2%v/%t \\
 %v_0 & = & 2%d/%t - %v
\end{eqnarray}
We could have also obtained the above formulas using manipulation of equations of real numbers,
or via the corresponding row operations. Let us consider the latter approach:
\begin{eqnarray}
 #[ 0.5 %t^2  #, %t #; %t #, 1 #] \cdot #[ %a #; %v_0 #] & = & #[ %d #; %v #] \\
 #[ 0.5 %t^2  #, %t #; %t --- (2/%t \cdot 0.5 %t^2) #, 1 --- (2/%t \cdot %t) #] \cdot #[ %a #; %v_0 #] & = & #[ %d #; %v --- %d \cdot (2/%t)#] \\
 #[ 0.5 %t^2  #, %t #; 0 #, 1 --- 2 #] \cdot #[ %a #; %v_0 #] & = & #[ %d #; %v --- (2%d/%t) #] \\
 #[ 0.5 %t^2  #, %t #; 0 #, 1 #] \cdot #[ %a #; %v_0 #] & = & #[ %d #; (2%d/%t) --- %v #] \\
 #[ 0.5 %t^2 --- (%t \cdot 0)  #, %t --- (%t \cdot 1) #; 0 #, 1 #] \cdot #[ %a #; %v_0 #] & = & #[ %d --- (%t \cdot ((2%d/%t) --- %v)) #; (2%d/%t) --- %v #] \\
 #[ 0.5 %t^2 #, 0 #; 0 #, 1 #] \cdot #[ %a #; %v_0 #] & = & #[ --- %d + %v %t #; (2%d/%t) --- %v #] \\
 #[ 1  #, 0 #; 0 #, 1 #] \cdot #[ %a #; %v_0 #] & = & #[ (2/%t^2) \cdot (--- %d + %v %t) #; (2%d / %t) --- %v #] \\
                               #[ %a #; %v_0 #] & = & #[ ---2%d/%t^2 + %v/%t #; 2%d / %t --- %v #]
\end{eqnarray}
</div>






<a name="lecture10"></a>
<a name="lecture11"></a>

<a name="3.9"></a>
<h3><span class="secn">3.9.</span> Matrix transpose</h3>

<div class="mathenv proposition_to_know">
<b>Definition:</b> The transpose of a matrix %M \in \R^{n \times n}, denoted %M^\top, is defined to be %A such that for all %i and %j, %A_{ij} = %M_{ji}.
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> If a matrix %M is scalar, diagonal, or symmetric, %M^\top = %M. If a matrix %M is upper triangular, %M^\top is lower triangular (and vice versa).
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Below are some examples of matrices and their transposes:
\begin{eqnarray}
  #[ 1 #, 0 #; 0 #, 1 #]^\top & = & #[ 1 #, 0 #; 0 #, 1 #] \\
  #[ %a #, %b #; %c #, %d #]^\top & = &  #[ %a #, %c #; %b #, %d #] \\
  #[ 1 #, 2 #; 3 #, 4 #; 5 #, 6 #]^\top & = & #[ 1 #, 3 #, 5 #; 2 #, 4 #, 6 #] \\
  #[ 1 #, 0 #, 0 #; 2 #, 3 #, 0  #; 4 #, 5 #, 6 #]^\top & = & #[ 1 #, 2 #, 4 #; 0 #, 3 #, 5  #; 0 #, 0 #, 6 #]


\end{eqnarray}


</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> For %A,%B \in \R^{n \times n}, it is always the case that:
\begin{eqnarray}
  (%A^\top)^\top & = &  %A \\
  (%A + %B)^\top & = &  %A^\top + %B^\top \\
  %s(%B^\top) & = &  (%s%B)^\top
\end{eqnarray}
</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> It is always the case that (%A%B)^\top = %B^\top %A^\top. If %M = %A%B, then:
\begin{eqnarray}
  (%A%B)_{ij} & = & %ith row of %A \cdot %jth column of %B \\
          & = & %ith column of %A^\top \cdot %jth row of %B^\top \\
          & = & %jth row of %B^\top \cdot %ith column of %A^\top  \\
          & = & (%B^\top %A^\top)_{ji}.
\end{eqnarray}
</div>


<div class="mathenv example_to_know">
<b>Example:</b> We consider the following example with matrices in \R^{3 \times 3}. Let %a, %b, %c, %x, %y, and %z be vectors in \R^3, with
%v_%i representing the %ith entry in a vector %v. Suppose we have the following product of two matrices. Notice that %a, %b, and %c are the rows of the left-hand matrix,
and %x, %y, and %z are the columns of the right-hand matrix.
\begin{eqnarray}
  #[ %a_1 #, %a_2 #, %a_3 #; %b_1 #, %b_2 #, %b_3 #; %c_1 #, %c_2 #, %c_3 #]
  \cdot
  #[ %x_1 #, %y_1 #, %z_1 #; %x_2 #, %y_2 #, %z_2 #; %x_3 #, %y_3 #, %z_3 #]
   & = &
  #[ (%a_1, %a_2, %a_3) \cdot (%x_1, %x_2, %x_3) #, (%a_1, %a_2, %a_3) \cdot (%y_1, %y_2, %y_3) #, (%a_1, %a_2, %a_3) \cdot (%z_1, %z_2, %z_3) 
  #; (%b_1, %b_2, %b_3) \cdot (%x_1, %x_2, %x_3) #, (%b_1, %b_2, %b_3) \cdot (%y_1, %y_2, %y_3) #, (%b_1, %b_2, %b_3) \cdot (%z_1, %z_2, %z_3)
  #; (%c_1, %c_2, %c_3) \cdot (%x_1, %x_2, %x_3)#,  (%c_1, %c_2, %c_3) \cdot (%y_1, %y_2, %y_3) #, (%c_1, %c_2, %c_3) \cdot (%z_1, %z_2, %z_3)
  #] \\
  & = & #[ %a \cdot %x #, %a \cdot %y #, %a \cdot %z
  #; %b \cdot %x #, %b \cdot %y #, %b \cdot %z
  #; %c \cdot %x #, %c \cdot %y #, %c \cdot %z
  #]
\end{eqnarray}
Suppose we take the transpose of both sides of the equation above. Then we would have:
\begin{eqnarray}
  
  ( #[ %a_1 #, %a_2 #, %a_3 #; %b_1 #, %b_2 #, %b_3 #; %c_1 #, %c_2 #, %c_3 #]
  \cdot
  #[ %x_1 #, %y_1 #, %z_1 #; %x_2 #, %y_2 #, %z_2 #; %x_3 #, %y_3 #, %z_3 #] )^\top
  & = &
  #[ %a \cdot %x #, %a \cdot %y #, %a \cdot %z
  #; %b \cdot %x #, %b \cdot %y #, %b \cdot %z
  #; %c \cdot %x #, %c \cdot %y #, %c \cdot %z
  #]^\top \\
  & = & #[ %a \cdot %x #, %b \cdot %x #, %c \cdot %x
  #; %a \cdot %y #, %b \cdot %y #, %c \cdot %y
  #; %a \cdot %z #, %b \cdot %z #, %c \cdot %z
  #] \\
 & = &
   #[ %x \cdot %a #, %x \cdot %b #, %x \cdot %c
  #; %y \cdot %a #, %y \cdot %b #, %y \cdot %c
  #; %z \cdot %a #, %z \cdot %b #, %z \cdot %c
  #] \\
  & = &
  #[ %x_1 #, %x_2 #, %x_3 
  #; %y_1 #, %y_2 #, %y_3 
  #; %z_1 #, %z_2 #, %z_3 #]
  \cdot
  #[ %a_1 #, %b_1  #, %c_1
  #; %a_2 #, %b_2 #, %c_2
  #; %a_3  #, %b_3  #, %c_3 #] \\
  & = &
  #[ %x_1 #, %y_1 #, %z_1 #; %x_2 #, %y_2 #, %z_2 #; %x_3 #, %y_3 #, %z_3 #]^\top
  \cdot
  #[ %a_1 #, %a_2 #, %a_3 #; %b_1 #, %b_2 #, %b_3 #; %c_1 #, %c_2 #, %c_3 #]^\top
\end{eqnarray}
</div>


<div class="mathenv proposition_to_know">
<b>Fact:</b> If %A is invertible, then so is %A^\top. This can be proven using the fact directly above.

@
\forall %A,%B \in \R^(2 \times 2),
    `(%A) is invertible` 
  \implies
   %[
   (%A^(-1)) * %A & = &  [1 #, 0; 0 #, 1] \\
   %A^\t * (%A^(-1))^\t & = & ((%A^(-1)) * %A)^\t  \\
           `` & = & [1#,0;0#,1]^\t  \\
           `` & = & [1#,0;0#,1]
   %]
/@

</div>

<div class="mathenv proposition_to_know">
<b>Fact:</b> \det %A = \det %A^\top. We can see this easily in the %A \in \R^{2 \times 2} case.

@
\forall %a,%b,%c,%d \in \R,
  %[
  \det [%a#,%b;%c#,%d] & = & %a %d-%b %c \\
              `` & = & %a %d - %c %b  \\
              `` & = & \det [%a#,%c;%b#,%d]  \\
  \det [%a#,%b;%c#,%d] & = & \det [%a#,%c;%b#,%d]
  %]
/@

</div>






















<a name="lecture11b"></a>

<a name="3.10"></a>
<h3><span class="secn">3.10.</span> Orthogonal matrices</h3>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A matrix %M \in \R^{n \times n} is orthogonal iff %M^\top = %M^{-1}.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b>  The columns of orthogonal matrices are always orthogonal unit vectors. 
The rows of orthogonal matrices are always orthogonal unit vectors.
We can see this in the \R^{2 \times 2} case. For columns, we use the fact that
%M^\top \cdot %M = \I:
\begin{eqnarray}
  #[ %a #, %b #; %c #, %d #]^\top #[ %a #, %b #; %c #, %d #]  & = & #[ 1 #, 0 #; 0 #, 1 #] \\
  #[ %a #, %c #; %b #, %d #] #[ %a #, %b #; %c #, %d #]   & = & #[ 1 #, 0 #; 0 #, 1 #]\\
   #[ (%a,%c) \cdot (%a,%c) #, (%a,%c) \cdot (%b,%d) #; (%b,%d) \cdot (%a,%c) #, (%b,%d) \cdot (%b,%d) #] & = & #[ 1 #, 0 #; 0 #, 1 #].
\end{eqnarray}
For rows, we use that %M \cdot %M^\top = \I:
\begin{eqnarray}
  #[ %a #, %b #; %c #, %d #] #[ %a #, %b #; %c #, %d #]^\top  & = & #[ 1 #, 0 #; 0 #, 1 #] \\
  #[ %a #, %b #; %c #, %d #] #[ %a #, %c #; %b #, %d #]  & = & #[ 1 #, 0 #; 0 #, 1 #]\\
   #[ (%a,%b) \cdot (%a,%b) #, (%a,%b) \cdot (%c,%d) #; (%c,%d) \cdot (%a,%b) #, (%c,%d) \cdot (%c,%d) #] & = & #[ 1 #, 0 #; 0 #, 1 #].
\end{eqnarray}
</div>

Below, we provide a verifiable argument of the above fact for the \R^{2 \times 2} case.

@
\forall %a,%b,%c,%d \in \R,
    %[
    [%a#,%b;%c#,%d] * [%a#,%b;%c#,%d]^\t & = & [1#,0;0#,1]
    %]
  \implies
    %[
    [%a#,%b;%c#,%d] * [a#,c;b#,d] & = & [1#,0;0#,1]  \\
    [%a %a + %b %b#, %a %c + %b %d; %c %a + %d %b#, %c %c + %d %d] & = & [1#,0;0#,1]  \\

    %a %a + %b %b & = & 1  \\
    [%a;%b] * [%a;%b] & = & 1  \\
    ||[%a;%b]|| & = & 1  \\
    %]
    %[
    `([%a;%b]) is a unit vector`  \\
    %]
    %[
    %c %c + %d %d & = & 1  \\
    [%c;%d] * [%c;%d] & = & 1 \\
    ||[%c;%d]|| & = & 1  \\
    %]
    %[
    `([%c;%d]) is a unit vector`  \\
    %]
    %[
    %a %c + %b %d & = & 0  \\
    [%a;%b] * [%c;%d] & = & 0  \\
    %]
    %[
    `([%a;%b]) and ([%c;%d]) are orthogonal`
    %]
/@


<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Matrices representing rotations and reflections are orthogonal. We can show this for the general (counterclockwise) rotation matrix:

\begin{eqnarray}
  #[ \cos \theta #, \sin \theta #; -\sin \theta #, \cos \theta #] #[ \cos \theta #, \sin \theta #; -\sin \theta #, \cos \theta #]^\top 
          & = & #[ \cos \theta #, \sin \theta #; -\sin \theta #, \cos \theta #] #[ \cos \theta #, -\sin \theta #; \sin \theta #, \cos \theta #] \\
          & = & #[ \cos^2 \theta + \sin^2 \theta &nbsp;&nbsp;&nbsp;&nbsp;#, -\cos \theta \sin \theta + \sin \theta \cos \theta #; -\sin \theta \cos \theta + \cos \theta \sin \theta  &nbsp;&nbsp;&nbsp;&nbsp;#, \sin^2 \theta  + \cos^2 \theta #]  \\
          & = &  #[ 1 #, 0 #; 0 #, 1 #].
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose that the hour hand of an animated
12-hour clock face is represented using a vector [ %x ; %y ]. To
maintain the time, the coordinates [ %x ; %y ] must be updated once
every hour by applying a matrix %M to the vector representing
the hour hand. What is the matrix %M?

Since the hour hand must be rotated by 360/12 = 30 degrees in the clockwise direction while
the rotation matrix represents counterclockwise rotation, we actually want to find the
rotation matrix for 360 - (360/12) = 330 degrees.

Thus, %M must be the rotation matrix for \theta = 2\pi - (2\pi / 30), where \theta is specified
in radians. Thus, we have:
\begin{eqnarray}
  \cos (330/360 \cdot 2\pi) & = & (\sqrt 3)/2 \\
  \sin (330/360 \cdot 2\pi) & = & ---1/2 \\
  %M & = & #[ \cos (330/360 \cdot 2\pi) #, ---\sin (330/360 \cdot 2\pi) #; \sin (330/360 \cdot 2\pi) #, \cos (330/360 \cdot 2\pi) #] \\
     & = & #[ (\sqrt 3)/2 #, 1/2 #; ---1/2 #, (\sqrt 3)/2 #]
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> You are instructed to provide a simple algorithm for
drawing a spiral on the Cartesian plane. The spiral is obtained using
counterclockwise rotation, and for every 360 degree
turn of the spiral, the spiral arm's distance from the origin
should double. Provide a matrix %M \in \R^{2 \times 2} that will
take any point [ %x ; %y ] on the spiral and provide the next point
[ %x' ; %y' ] after \theta radians of rotation (your definition of
%M should contain \theta).

It is sufficient to multiply a rotation matrix by a scalar
matrix that scales a vector according to the angle of
rotation. If the angle of rotation is \theta, then the
scale factor %s should be such that:
\begin{eqnarray}
  %s<sup>((2\pi) / \theta)</sup> & = & 2 \\
  %s & = & 2<sup>1 / ((2\pi) / \theta)</sup> \\
  %s & = & 2<sup>\theta/(2\pi)</sup>
\end{eqnarray}
In the above, if %n = \pi/\theta then %s is the %nth root of 2.
Thus %M would be:
\begin{eqnarray}
  #[ 2<sup>\theta/(2\pi)</sup> #, 0 #; 0 #, 2<sup>\theta/(2\pi)</sup> #] 
    \cdot #[ \cos \theta #, ---\sin \theta #; \sin \theta #, \cos \theta #] 
    & = & #[ 2<sup>\theta/(2\pi)</sup> \cos \theta 
          #, --- (2<sup>\theta/(2\pi)</sup>) \sin \theta 
          #; 2<sup>\theta/(2\pi)</sup> \sin \theta
          #, 2<sup>\theta/(2\pi)</sup> \cos \theta #] \\
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Orthogonal matrices are closed under multiplication. For orthogonal %A, %B \in \R^{n \times n} we have:

\begin{eqnarray}
  (%A%B)^\top & = & %B^\top %A^\top & = & %B^{-1} %A^{-1} & = & (%A%B)^{-1}.
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Orthogonal matrices are closed under inversion and transposition. For orthogonal %A \in \R^{n \times n} we have:

\begin{eqnarray}
  (%A^{-1})^\top & = & (%A^\top)^{-1}.
\end{eqnarray}

Thus, we can show that both A^{-1} and A^\top are orthogonal.

\begin{eqnarray}
  (%A^{-1})^\top & = & (%A^{-1})^{-1} \\
  (%A^\top)^\top & = & %A.
\end{eqnarray}

\begin{eqnarray}
  (%A^\top)^\top & = & (%A^\top)^{-1} \\
                & = & %A.
\end{eqnarray}
</div>



We can summarize this by adding another row to our table of matrix subsets.


<table class="fig_table">
 <tr>
  <td><b>subset of \R^{n \times n}</b></td>
  <td><b>definition</b></td>
  <td><b>closed under<br/>matrix<br/>multiplication</b></td>
  <td><b>properties of<br/>matrix multiplication</b></td>
  <td><b>inversion</td>
 </tr>
 <tr>
  <td>orthogonal matrices</td>
  <td>%M^\top = %M^{-1}</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
</table>


<div class="mathenv example_to_know">
<b>Example:</b> Let %A \in \R^{2 \times 2} be an orthogonal matrix. Compute %A %A^\top %A.

We recall that because %A is orthogonal, %A^\top = %A^{-1}. Thus, we have that

  $$ %A %A^\top %A = (%A %A^{-1}) %A = \I %A = %A.$$
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose the last row of a matrix %M \in \R^{n \times n} consists of all zeroes. Show that %M is not invertible.

Suppose %M is invertible. Then %M^\top is invertible. But the last column of %M is all zeroes, which means it is a trivial linear
combination of the other column vectors of %M. Thus, we have a contradiction, so %M cannot be invertible.
</div>





<a name="3.11"></a>
<h3><span class="secn">3.11.</span> Matrix rank</h3>

Yet another way to characterize matrices is by considering their rank.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> We can define a the <i>rank</i> of a matrix %M, \rank %M, as the number of nonzero rows in \rref %M.
</div>

We will see in this course that \rank %M is related in many ways to the various characteristics of a matrix.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Matrix rank is preserved by elementary row operations. Why is this the case? Because \rref is idempotent and \rank is defined in
terms of it:

\begin{eqnarray}
  \rref %M & = & \rref (\rref %M) \\
  number of nonzero rows in \rref %M & = & number of nonzero rows in \rref (\rref %M) \\
  \rank (%M) & = & \rank (\rref %M)
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If %M \in \R^{n \times n} is invertible, \rank %M = n. How can we prove this? Because all invertible matrices
have \rref %M = \I, and \I has no rows with all zeroes. If \I \in \R^{n \times n}, then \I has %n nonzero rows.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For %A \in \R^{n \times n}, \rank %A = \rank (%A^\top).
</div>

We will not prove the above in this course in general, but typical proofs
involve first proving that for all %A \in \R^{n \times n}, \rank A \leq \rank (A^\top). Why is this sufficient to complete the proof? Because we
can apply this fact for both %A and %A^\top and use the fact that (%A^\top)^\top = %A:

\begin{eqnarray}
  \rank(%A) & \leq & \rank(%A^\top) \\
  \rank(%A^\top) & \leq & \rank((%A^\top)^\top) \\
  \rank(%A^\top) & \leq & \rank(%A) \\
  \rank(%A^\top) & = & \rank(%A) \\
\end{eqnarray}

To get some intuition about the previous fact, we might consider the following question: if the columns of a matrix %M \in \R^{2 \times 2}
are linearly independent, are the rows also linearly independent?

<!-- <b>Fact:</b> The inverse of an orthogonal matrix is orthogonal. How can this be proven using matrix ranks? -->

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The only matrix in \R^{n \times n} that has no nonzero rows in reduced row echelon form is \I.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If a matrix %M \in \R^{n \times n} has rank %n, then it must be that \rref %M = \I (and, thus, %M is invertible). This is derived from
the fact above.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Invertible matrices are closed under the transposition operation. In other words, if %M is invertible, then %M^\top is invertible. How
can we prove this using the rank operator? We know that \rank is preserved under transposition, so we have:

\begin{eqnarray}
  %n & = & \rank(%A) & = & \rank(%A^\top) & = & %n 
\end{eqnarray}

Thus, the rank of \rank(%A^\top) is %n, so it is invertible.
</div>

The following table summarizes the important facts about the \rank and \rref operators.


<table class="fig_table">
 <tr><td>\rank (\rref %M) = \rank %M</td></tr>
 <tr><td>\rref (\rref %M ) = \rref %M</td></tr>
 <tr><td>\rank (%M^\top) = \rank %M</td></tr>
 <tr><td>for %M \in \R^{n \times n}, %M is invertible iff \rank %M = %n</td></tr>
</table>





<!--assignment3-->

<br/><hr/>
<a name="3.12"></a>
<a name="assignment3"></a>
<h3><span class="secn">3.12.</span>
  <b>Assignment #3: Matrix Properties and Operations</b> <!--span class="btn_assignment">(<a href="materials.php?hw=3">show only this assignment</a>)</span-->
</h3>

<only142>

       <p>In this assignment you will perform step-by-step algebraic manipulations involving vector and matrix properties and operations.
       <b style="color:firebrick;">You may not add new lines or premises above an "implies" logical operator. If you add any
       new premises, you will recieve no credit.</b>
       </p>

<ol>
  <li><ol style="list-style-type:lower-alpha;">
      <li> Finish the argument below that shows that matrix multiplication of vectors in \R^3 preserves lines.

@
\forall %M \in \R^(3 \times 3), \forall %u,%v,%w \in \R^3, \forall %s \in \R,
   %[
    %w = %s(%u-%v) + %v
   %]
   %[
    `(%w) is on the line defined by (%u) and (%v)`
   %]
  \implies

    <comment> ... put proof steps here ... </comment>

    %[
    `(%M %w) is on the line defined by (%M %u) and (%M %v)`
    %]
/@

      </li>
      <li> Finish the argument below that shows that if multiplication by a matrix %M maps three vectors to [0; 0; 0], it
      maps any linear combination %v' of those vectors to [0; 0; 0].

@
\forall %M \in \R^(3 \times 3), \forall %u,%v,%w,%v' \in \R^3, \forall %a,%b,%c \in \R,
    %[
    %M %u & = & [0;0;0]  \\
    %M %v & = & [0;0;0]  \\
    %M %w & = & [0;0;0]  \\
    %v' & = & %a %u + %b %v + %c %w
    %]
  \implies

    <comment> ... put proof steps here ... </comment>

    %[
    %M %v' = [0;0;0]
    %]
/@

           <b>Extra credit:</b> if %u, %v, and %w are setwise linearly independent, what can you say about the matrix %M? Be as specific as
           possible.

      </li>
      <li> Show that if a matrix %M in \R^{2 \times 2} is invertible, there is a unique solution for %u given any equation %M %u = %v with %v \in \R^2.

@
\forall %M \in \R^(2 \times 2), \forall %u,%v \in \R^2,
    %[
    `(%M) is invertible`
    %]
    %[
     %M %u = %v
    %]
  \implies

    <comment> ... put proof steps here ... </comment>

    <comment> replace the right-hand side below </comment>
    <comment> with an expression in terms of M and v </comment>
    %[
    %u = ? 
    %]
/@

      </li>


      <li> Show that if the column vectors of a matrix in \R^{2 \times 2} are linearly dependent, then its determinant is 0.

@
\forall %a,%b,%c,%d \in \R,
    %[
    `([%a;%c]) and ([%b;%d]) are linearly dependent`
    %]
  \implies

    <comment> ... put proof steps here ... </comment>

    %[
    \det [%a#,%b; %c#,%d] = 0
    %]
/@

           <b>Extra credit:</b> you may include the opposite direction of this proof for extra credit (build a new, separate argument in which 
           <code>\det [a,c; c,d] = 0</code> is found above the <code>\implies</code> and <code>`([a;c]) and ([b;d]) are linearly dependent`</code>
           is at the end of the argument below it).
      </li>
    </ol>
  </li>

  <li> In this problem, you will define explicit matrices that correspond to elementary row operations on matrices in \R^{2 \times 2}.
    <ol style="list-style-type:lower-alpha;">
      <li> Find appropriate matrices %A, %B, %C, and %D in \R^{2 \times 2} to finish the following argument.

@
\forall %A,%B,%C,%D \in \R^(2 \times 2), \forall %a,%b,%c,%d,%t \in \R,
    %[
    %A & = & \?  \\
    %B & = & \?  \\
    %C & = & \?  \\
    %D & = & \?
    %]
  \implies

    <comment> ... put proof steps here ... </comment>

    %[
    %A * [%a#,%b;%c#,%d] & = & [%a+c#, %b+%d   ;   %c#, %d]    \\
    %B * [%a#,%b;%c#,%d] & = & [%a#, %b       ; %c+%a#, %d+%b]  \\
    %C * [%a#,%b;%c#,%d] & = & [%t*%a#, %t*%b   ;   %c#, %d]    \\
    %D * [%a#,%b;%c#,%d] & = & [%a#, %b       ; %t*%c#, %t*%d]
    %]
/@

      </li>
      <li> Use the matrices %A, %B, %C, and %D from part (a) with matrix multiplication to construct a matrix %E that can be
           shown to satisfy the last line in the following argument.

@
\forall %A,%B,%C,%D,%E \in \R^(2 \times 2), \forall %a,%b,%c,%d,%t \in \R,
    %[
    %t & = & \?  \\
    %A & = & \?  \\
    %B & = & \?  \\
    %C & = & \?  \\
    %D & = & \?  \\
    %E & = & \? 
    %]
  \implies

    <comment> ... put proof steps here ... </comment>

    %[
    %E * [%a#,%b; %c#,%d] = [%c#,%d; %a#,%b]
    %]
/@

      </li>
      <li> <b>Extra credit:</b> the row operations defined by %A, %B, %C, and %D are all invertible as long as %t \neq 0; 
      prove this for %t = -1 by showing that the matrices %A, %B, %C, and %D are invertible.

@
\forall %A,%B,%C,%D \in \R^(2 \times 2), \forall %t \in \R,
    %[
    %t & = & -1 \\
    %A & = & \?  \\
    %B & = & \? \\
    %C & = & \? \\
    %D & = & \?
    %]
  \implies

    <comment> ... put proof steps here ... </comment>

    %[
    `(%A) is invertible` \\
    `(%B) is invertible` \\
    `(%C) is invertible` \\
    `(%D) is invertible`
    %]
/@

      </li>
    </ol>
  </li>

</ol>
<hr/><br/>

</only142>


<only132>
       <p>In this assignment you will use Python to implement several of the vector and matrix operations and predicates we have introduced
       so far.
       <b>Please submit a single file <code>a3.py</code> containing your solutions.</b>

  <b style="color:firebrick;">Your file may not import any modules or employ any external library functions
  (unless the problem statement explicitly permits this). 
  You will be graded on the correctness, concision, and mathematical legibility of your code.
  The different problems and problem parts rely on each other; carefully consider whether you can use functions you define
  in one part within subsequent parts.</b>

<ol>
  <li> In this assignment, we will represent vectors as Python tuples.
       For example, the vector [1;2;3] would be represented in Python as <code>(1,2,3)</code>. 

    <ol style="list-style-type:lower-alpha;">
      <li> Define a Python function <code>scale(s,v)</code> that takes two arguments: a scalar <code>s</code> and a vector <code>v</code>.
           The function should return the result of multiplying the vector by the scalar.
           <pre class="snippet">scale(2, (3,4,5))     # Returns (6,8,10).</pre>
      </li>

      <li> Define a Python function <code>norm(v)</code> that takes a vector argument <code>v</code> and returns its norm (i.e., its length).<br/><br/>
      </li>
    
      <li> Define a Python function <code>dot(v,w)</code> that takes two vectors as arguments and returns their dot product.
           If the two vectors do not have the same number of components, the function should return <code>None</code>.
           <br/><br/>Include a comment explaining <b>exactly</b> how many addition operations and how many multiplication operations your
           implementation performs when the input is a pair of vectors that each have <code>n</code> components.
           <pre class="snippet">dot((1,2), (3,4))     # Returns 11.
dot((5,1,0), (1,1,2)) # Returns 6.
dot((5,1,0,4), (1,2)) # Returns None.</pre>
      </li>
      <li> Define a Python function <code>orthogonal(v,w)</code> that takes two vectors as arguments and returns <code>True</code> only
           if the two input vectors are orthogonal to one another; if they are not orthogonal, it returns <code>False</code>.
           If the two vectors do not have the same number of components, the function should return <code>None</code>.<br/><br/>
      </li>
      <li> Define a Python function <code>proj(v,w)</code> that takes two vectors as arguments and returns a vector that
           is the projection of <code>v</code> onto <code>w</code>.
      </li>
    </ol>
  </li>

  <li> In this assignment, we will represent matrices as tuples of tuples. For example, the matrix
\begin{eqnarray}
  #[ 1 #, 2 #; 3 #, 4 #]
\end{eqnarray}
would be represented as <code>((1,2),(3,4))</code>:

    <ol style="list-style-type:lower-alpha;">
      <li> Define a Python function <code>mult(A,B)</code> that takes two matrix arguments and returns the matrix that is
           the result of multiplying <code>A</code> by <code>B</code>. If either argument is not a valid matrix, or if
           the two matrices cannot be multiplied because the number of rows and/or columns does not match as necessary,
           the function should return <code>None</code>.
           <br/><br/>Include a comment explaining <b>exactly</b> how many addition operations and how many multiplication operations your
           implementation performs when the input is a pair of matrices that each have <code>n</code> rows and columns.
           <pre class="snippet">mult(((1,2),(3,4)), ((-5,-6),(7,8)))     # Returns ((9,10),(13,14)).</pre>
      </li>

      <li> Define a Python function <code>upper(A)</code> that takes a matrix argument and returns <code>True</code> only if
           the matrix <code>A</code> is upper triangular; otherwise, it returns <code>False</code>.
      </li>

      <li> Define a Python function <code>transpose(A)</code> that takes a matrix argument and returns the transpose of that matrix.
      </li>

      <li> Define a Python function <code>lower(A)</code> that takes a matrix argument and returns <code>True</code> only if
           the matrix <code>A</code> is lower triangular; otherwise, it returns <code>False</code>.
      </li>

      <li> Define a Python function <code>symmetric(A)</code> that takes a matrix argument and returns <code>True</code> only if
           the matrix <code>A</code> is symmetric; otherwise, it returns <code>False</code>.
      </li>

      <li> Define a Python function <code>orthogonal(A)</code> that takes a matrix argument and returns <code>True</code> only if
           the matrix <code>A</code> is orthogonal; otherwise, it returns <code>False</code>.
           <b style="color:firebrick;">You may name this function <code>orthogonal2()</code> if all your tests are at the
           end of your file
           and you don't want this definition to hide the definition of <code>orthogonal()</code> from Problem #1(d) above.</b>
      </li>
    </ol>
  </li>

  <li> In this problem you will implement functions that can be used to generate elementary matrices corresponding to the
       three types of elementary row operations. <b>In matrices, the first row is indexed as <code>i = 1</code> (unlike
       tuples and lists in Python). Thus, you will need to convert the <i>row indices</i> <code>i</code> and <code>j</code>
       into the corresponding Python tuple indices.</b>
       <b style="color:firebrick;">Hint: begin with the identity matrix and apply the row operation to that matrix.</b>
       <br/>
    <ol style="list-style-type:lower-alpha;">
      <li> Define a Python function <code>elementary_swap(r,c,i,j)</code>. This function should take four integer arguments:
        <ul>
          <li><code>r</code> is the number of rows</li>
          <li><code>c</code> is the number of columns;</li>
          <li><code>i</code> and <code>j</code> are the two rows to swap.</li>
        </ul>
        The function should return a single matrix that can then be used to swap row <code>i</code> with row <code>j</code>
        by using matrix multiplication. See example below.
        <pre class="snippet">mult(elementary_swap(3,3,1,3), ((1,2,3), (0,0,0), (7,8,9))) # Returns ((7,8,9), (0,0,0), (1,2,3)).</pre>
      </li>
      <li> Define a Python function <code>elementary_scale(r,c,i,s)</code>. This function should take four integer arguments:
        <ul>
          <li><code>r</code> and <code>c</code> are the row and column counts, respectively;</li>
          <li><code>i</code> is the row to scale;</li>
          <li><code>s</code> is the scalar to use.</li>
        </ul>
        The function should return a single matrix that can then be used to scale row <code>i</code> using the scalar <code>s</code>.
        See example below.
        <pre class="snippet">mult(elementary_scale(2,2,2,-1), ((1,2), (3,4))) # Returns ((1,2), (-3,-4)).</pre>
      </li>
      <li> Define a Python function <code>elementary_add(r,c,i,j,s)</code>. This function should take four integer arguments:
        <ul>
          <li><code>r</code> and <code>c</code> are the row and column counts, respectively;</li>
          <li><code>i</code> is the row to add to row <code>j</code>;</li>
          <li><code>s</code> is the scalar with which to scale row <code>i</code> before adding it to row <code>j</code>.</li>
        </ul>
        The function should return a single matrix that can then be used to add a multiple (by <code>s</code>) of row <code>i</code>
        to row <code>j</code>. See example below.
        <pre class="snippet">mult(elementary_add(3,3,1,2,-1), ((1,2,3), (0,0,0), (7,8,9))) # Returns ((1,2,3), (-1,-2,-3), (7,8,9)).</pre>
      </li>
    </ol>
  </li>

  <li> Define a Python function <code>is_rref(A)</code> that takes a matrix argument <code>A</code> and returns <code>True</code>
       only if <code>A</code> is in reduced row echelon form; otherwise, it returns <code>False</code>.
  </li>

</ol>
<hr/><br/>

</only132>

<!--/assignment3-->



<a name="lecture12"></a>

<a name="R.1"></a>
<hr style="margin-bottom:120px;"/>
<h2><span class="secn">Review 1.</span> Vector and Matrix Algebra and Applications</h2>

The following is a breakdown of what you should be able to do at this point in the course 
(and of what you may be tested on in an exam).
Notice that many of the tasks below can be composed.
This also means that many problems can be solved in more than one way.

<ul>
  <li>vectors
    <ul>
      <li>definitions and algebraic properties of scalar and vector operations (addition, multiplication, etc.)</li>
      <li>vector properties and relationships between vectors
        <ul>
          <li>dot product of two vectors</li>
          <li>norm of a vector</li>
          <li>unit vectors</li>
          <li>orthogonal projection of a vector onto another vector</li>
          <li>orthogonal vectors</li>
          <li>linear dependence of two vectors</li>
          <li>linear independence of two vectors</li>
          <li>linear combinations of vectors</li>
          <li>linear independence of three vectors</li>
        </ul>
      </li>
      <li>lines and planes
        <ul>
          <li>line defined by a vector and the origin ([0; 0])</li>
          <li>line defined by two vectors</li>
          <li>line in \R^2 defined by a vector orthogonal to that line</li>
          <li>plane in \R^3 defined by a vector orthogonal to that plane</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>matrices
    <ul>
      <li>algebraic properties of scalar and matrix multiplication and matrix addition</li>
      <li>collections of matrices and their properties (e.g., invertibility, closure)
        <ul>
          <li>identity matrix</li>
          <li>elementary matrices</li>
          <li>scalar matrices</li>
          <li>diagonal matrices</li>
          <li>upper and lower triangular matrices</li>
          <li>matrices in reduced row echcelon form</li>
          <li>determinant of a matrix in \R^{2 \times 2}</li>
          <li>inverse of a matrix and invertible matrices</li>
        </ul>
      </li>
      <li>other matrix operations and properties
        <ul>
          <li>determine whether a matrix is invertible
            <ul>
              <li>using the determinant for matrices in \R^{2 \times 2}</li>
              <li>using facts about rref for matrices in \R^{n \times n}</li>
            </ul>
          </li>
          <li>algebraic properties of matrix inverses with respect to matrix multiplication</li>
          <li>transpose of a matrix
            <ul>
              <li>algebraic properties of transposed matrices with respect to matrix addition, multiplication, and inversion</li>
            </ul>
          </li>
          <li>matrix rank</li>
        </ul>
      </li>
      <li>matrices in applications
        <ul>
          <li>solve an equation of the form %L%U = %w</li> 
          <li>matrices and systems of states
            <ul>
              <li>interpret partial observations of system states as vectors</li>
              <li>interpret relationships betweem dimensions in a system of states as a matrix</li>
              <li>given a partial description of a system state and a matrix of relationships, find the full description of the system state</li>
              <li>interpret system state transitions/transformations over time as matrices
                <ul>
                  <li>population growth/distributions over time
                    <ul>
                      <li>compute the system state after a specifieds amount of time</li>
                      <li>find the fixed point of a transition matrix</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

Below is a comprehensive collection of review problems going over the course material covered until this point. These problems are an
accurate representation of the kinds of problems you may see on an exam.

<div class="mathenv example_to_know">
<b>Example:</b> Find any %h \in \R such that the following two vectors are linearly independent.
\begin{eqnarray}
  #[ 5 #; -5 #; -2 #] , #[ 20 #; -20 #; %h #]
\end{eqnarray}
There are many ways to solve this problem. One way is to use the definition of linear dependence and find an %h that does not satisfy it.
\begin{eqnarray}
  %s #[ 5 #; -5 #; -2 #] & = & #[ 20 #; -20 #; %h #]
\end{eqnarray}
Then, we have that any %h such that %h \neq -8 is sufficient to contradict linear dependence (and, thus, imply linear independence):
\begin{eqnarray}
  %s \cdot 5 & = & 20 \\
  %s & = & 4\\
  %s \cdot (-2) & = &  %h \\
  -8 & = &  %h
\end{eqnarray}
Another solution is to recall that orthogonality implies linear independence. Thus, it is sufficient to find %h such that
the two vectors are orthogonal.
\begin{eqnarray}
  #[ 5 #; -5 #; -2 #] \cdot #[ 20 #; -20 #; %h #] & = & 0
\end{eqnarray}
This implies %h = 100.
\begin{eqnarray}
   5(20) + (-5)(-20) + (-2)%h & = & 0\\
  %h & = & 100
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we have a matrix %M such that the following three equations are true:
\begin{eqnarray}
  %M #[ 1 #; 0 #; 0 #] & = & #[ 3 #; -2 #] \\
  %M #[ 0 #; 1 #; 0 #] & = & #[ 3 #; 0 #] \\
  %M #[ 0 #; 0 #; 1 #] & = & #[ -3 #; 1 #] 
\end{eqnarray}
Compute the following:
\begin{eqnarray}
  %M #[ 2 #; 1 #; -1 #] & = & ? 
\end{eqnarray}

We should recall that multiplying a matrix by a canonical unit vector with 1 in the %ith row in the vector gives us the %ith column of the matrix.
Thus, we can immediately infer that:
\begin{eqnarray}
  %M & = & #[ 3 #, 3 #, -3 #; -2 #, 0 #, 1 #]
\end{eqnarray}
Thus, we have:
\begin{eqnarray}
  %M \cdot #[ 2 #; 1 #; -1 #] & = & #[ (3,3,-3) \cdot (2,1,-1) #; (-2,0,1) \cdot (2,1,-1) #] & = & #[ 12 #; -5 #]
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> List at least three properties of the following matrix:

  $$ #[ 0 #, 1 #, 0 #; 1 #, 0 #, 0 #; 0 #, 0 #, 1 #] $$

The matrix has many properties, such as:
<ul>
 <li>it is an elementary matrix</li>
 <li>it is an invertible matrix</li>
 <li>it is an orthogonal matrix</li>
 <li>it is a symmetric matrix</li>
 <li>it has rank %n</li>
 <li>its reduced row echelon form is the identity</li>
</ul>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Find the matrix %B \in \R^{2 \times 2} that is symmetric, has a constant diagonal
(i.e., all entries on the diagonal are the same real number), and satisfies the following
equation:
\begin{eqnarray}
  %B #[ 2 #; 1 #] & = & #[ 15 #; 6 #]
\end{eqnarray}

We know that %B is symmetric and has a constant diagonal, so we need to solve for %a and %b in:

\begin{eqnarray}
  #[ a #, b #; b #, a#] \cdot #[ 2 #; 1 #] & = & #[ 15 #; 6 #]
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Compute the inverse of the following matrix:

  $$ #[ 2 #, 3 #; 1 #, 2 #] $$

One approach is to set up the following equation and solve for %a,%b,%c, and %d:
\begin{eqnarray}
  #[ a #, b #; c #, d #] \cdot #[ 2 #, 3 #; 1 #, 2 #] & = & #[ 1 #, 0 #; 0 #, 1 #]
\end{eqnarray}
Another approach is to apply the formula for the inverse of a matrix in \R^{2 \times 2}:
\begin{eqnarray}
  #[ 2 #, 3 #; 1 #, 2 #]^{-1} & = & (1 / (2 \cdot 2 - 3 \cdot 1)) \cdot #[ 2 #, ---3 #; ---1 #, 2 #] & = & #[ 2 #, ---3 #; ---1 #, 2 #]
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Let %a \in \R be such that %a \neq 0. Compute the inverse of the following matrix:

  $$ #[ %a #, -%a #; -%a #, -%a #] $$

As in the previous problem, we can either solve an equation or apply the formula:
\begin{eqnarray}
  #[ %a #, -%a #; -%a #, -%a #]^{-1} & = & (1 / (-%a^2 - %a^2)) \cdot #[ -%a #, %a #; %a #, %a #]
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose %x \in \R is such that %x \neq 0. Compute the inverse of the following matrix:

  $$ #[ %x #, 0 #, -2%x #; 0 #, %x #, 0 #; 0 #, 0 #, 4%x #] $$

Because we have an upper triangular matrix, computing the inverse by solving the following equation is fairly efficient. Start by considering
the bottom row and its product with each of the columns. This will generate the values for %g, %h, and %i. You can then proceed to the other
rows.
\begin{eqnarray}
  #[ %x #, 0 #, -2%x #; 0 #, %x #, 0 #; 0 #, 0 #, 4%x #] #[ %a #, %b #, %c #; %d #, %e #, %f #; %g #, %h #, %i #] & = & #[ 1 #, 0 #, 0 #; 0 #, 1 #, 0 #; 0 #, 0 #, 1 #]
\end{eqnarray}
The solution is:

  $$ #[ 1/%x #, 0 #, 1/(2%x) #; 0 #, 1/%x #, 0 #; 0 #, 0 #, 1/4%x #] $$

</div>

<div class="mathenv example_to_know">
<b>Example:</b> Assume a matrix %M \in \R^{2 \times 2} is symmetric, has constant diagonal, and all its entries are nonzero. Show that %A cannot
be an orthogonal matrix.

Let %a \neq 0 and %b \neq 0, and let us define:

\begin{eqnarray}
%M & = & #[ %a #, %b #; %b #, %a #].
\end{eqnarray}

Suppose that %M is an orthogonal matrix; then, we have that:

\begin{eqnarray}
             %M^{-1} & = & %M^\top\\
%M \cdot %M^\top & = & \I \\
#[ %a #, %b #; %b #, %a #] \cdot #[ %a #, %b #; %b #, %a #] & = & #[ 1 #, 0 #; 0 #, 1 #].
\end{eqnarray}

Thus, we have %b%a + %a%b = 0, so 2%a%b = 0. This implies that either %a = 0 or %b = 0. But this contradicts the assumptions, so %M cannot be
orthogonal.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose the Earth is located at the origin and your spaceship is in space at the location corresponding to the vector
[ -5 ; 4; 2 ]. Earth is sending transmissions along the vector [ \sqrt(3)/3 ; \sqrt(3)/3 ; \sqrt(3)/3 ]. What is the shortest distance
your spaceship must travel in order to hear a transmission from Earth?

By the triangle inequality, for any vector %v \in \R^3, the closest point to %v on a line is the orthogonal projection
of %v onto that line. We need to find the distance from the spaceship's current position to that point.

We first compute the orthogonal projection of [ -5 ; 4; 2 ] onto the line %L
defined as follows:
\begin{eqnarray}
  %L & = & { %a \cdot #[ \sqrt(3)/3 #; \sqrt(3)/3 #; \sqrt(3)/3 #] &nbsp;|&nbsp; %a \in \R}
\end{eqnarray}
We can compute the orthogonal projection using the formula for an orthogonal projection of one vector onto another.
Notice that the vector specifying the direction of transmission is already a unit vector:
\begin{eqnarray}
  ||#[ \sqrt(3)/3 #; \sqrt(3)/3 #; \sqrt(3)/3 #]|| & = & 1 \\
  #[ -5 #; 4#; 2 #] \cdot #[ \sqrt(3)/3 #; \sqrt(3)/3 #; \sqrt(3)/3 #] & = & \sqrt(3)/3 \\
  (%v \cdot %u/||%u||) \cdot %u/||%u|| & = & \sqrt(3)/3 \cdot #[ \sqrt(3)/3 #; \sqrt(3)/3 #; \sqrt(3)/3 #] \\
                         & = & #[ 1/3 #; 1/3 #; 1/3 #]
\end{eqnarray}
Now, we must compute the distance between the destination above and the spaceship's current position:
\begin{eqnarray}
  ||#[ 1/3 #; 1/3 #; 1/3 #] - #[ -5 #; 4#; 2 #]|| & = & ||#[ 16/3 #; -11/3 #; -5/3 #]|| \\
                                                  & = & \sqrt((16/3)^2 + (-11/3)^2 + (-5/3)^2) \\
                                                  & = & \sqrt(256/9 + 121/9 + 25/9) \\
                                                  & = & \sqrt(402)/3
\end{eqnarray}
Thus, the shortest distance is \sqrt(402)/3.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Two communication towers located at %v \in \R^2 and %u \in \R^2 are sending directed signals to each other (it is only possible to hear
the signal when in its direct path). You are at the origin and want to travel the shortest possible distance to
intercept their signals. What vector specifies the distance and direction you must travel to intercept their signals?

We can solve this problem by recognizing that the closest point to the origin on the line along which the signals can be intercepted is the vector that is orthogonal to the line (i.e., it is the orthogonal projection of the origin onto the
line). The definition of the line is:
\begin{eqnarray}
  %L & = & { %a \cdot (%u - %v) + %v &nbsp;|&nbsp; %a \in \R}
\end{eqnarray}
Thus, the point %p \in \R^2 to which we want to travel must be both <i>on the line</i> and <i>orthogonal</i> to the
line (i.e., its slope must be orthogonal to any vector that represents the slope of the line, such as %u - %v). In other words, it must satisfy the following two equations:
\begin{eqnarray}
  %p & = & %a \cdot (%u - %v) + %v \\
  %p \cdot (%u - %v) & = & 0
\end{eqnarray}
Thus, it is sufficient to solve the above system of equations for %p.
</div>


<a name="lecture13"></a>
<a name="4"></a>
<hr style="margin-bottom:120px;"/>
<h2><span class="secn">4.</span> Vector Spaces</h2>

We are interested in studying <i>sets of vectors</i> because they can be used to model sets of system states, observations
and data that might be obtained about systems, geometric shapes and regions, and so on. We can then represent real-world problems
(e.g., given some observations, what is the actual system state) as equations of the form %M \cdot %v = %w, and sets of vectors as
the collections of possible solutions to those equations. But what <i>is</i> exactly the set of possible solutions to %M \cdot %v = %w?
Can we characterize it precisely? Can we define a succinct notation for it? Can we say anything about it beyond simply solving the equation?
Does this tell us anything about our system?

In some cases, it may make more sense to consider only a finite set of system states, or an infinite set of discrete states (i.e., only
vectors that contain integer components); for example, this occurs if vectors are used to represent the number of atoms, molecules, cows, chickens,
power plants, single family homes, and so on. However, in this course, we make the assumption that our our sets
of system states (our <i>models</i> of systems) are infinite and continuous (i.e., not finite and not discrete); in this context, this simply
means that the entries in the vectors we use to represent system states are real numbers.

Notice that the assumption of continuity means that, for example, if we are looking for a particular state (e.g., a state corresponding
to some set of observations), we allow the possibility that the state we find will not correspond exactly to a state that "makes sense". Consider
the example problem involving the barn of cows and chickens. Suppose we observe 4 heads and 9 legs. We use the matrix that
represents the relationships between the various dimensions of the system to find the number of cows and chickens:
\begin{eqnarray}
       #[ 1 #, 1 #; 2 #, 4 #] 
 \cdot #[ %x <i style="color:gray;">chickens</i> #; %y <i style="color:gray;">cows</i> #]
 & = & #[ 4 <i style="color:gray;">heads</i> #; 9 <i style="color:gray;">legs</i> #] \\
 %x & = & 3.5 \\
 %y & = & 0.5
\end{eqnarray}
Notice that the solution above is not an integer solution; yet, it is a solution to the equation we introduced because the set of system states
we are allowing in our solution space (the <i>model</i> of the system) contains all vectors in \R^2, not just those with integer entries.

As we did with vectors and matrices, we introduce a succinct language (consisting of symbols, operators, predicates, terms, and formulas) for
infinite, continuous sets of vectors. And, as with vectors and matrices, we study the algebraic laws that govern these symbolic expressions.

<a name="4.1"></a>
<h3><span class="secn">4.1.</span> Sets of vectors and their notation</h3>

We will consider three kinds of sets of vectors in this course; they are listed in the table below.

<table class="fig_table">
 <tr>
  <td><b>kind of set (of vectors)</b></td>
  <td><b>maximum<br/>cardinality<br/>("quantity of<br/>elements")</b></td>
  <td><b>solution space of a...</b></td>
  <td><b>examples</b></td>
 </tr>
 <tr>
  <td>finite set of vectors</td>
  <td>finite</td>
  <td></td>
  <td>
   <ul style="padding-left:14px;">
    <li>{(0,0)}</li>
    <li>{(2,3),(4,5),(0,1)}</li>
   </ul>
  </td>
 </tr>
 <tr>
  <td>vector space</td>
  <td>infinite</td>
  <td>homogenous system of<br/>linear equations:<br/>&nbsp;&nbsp;%M \cdot %v = \0</td>
  <td>
   <ul style="padding-left:14px;">
    <li>{(0,0)}</li>
    <li>\R</li>
    <li>\R^2</li>
    <li>\span{(1,2),(2,3),(0,1)}</li>
    <li>any point, line, or plane<br/>intersecting the origin</li>
   </ul>
  </td>
 </tr>
 <tr>
  <td>affine space</td>
  <td>infinite</td>
  <td>nonhomogenous<br/>system of<br/>linear equations:<br/>&nbsp;&nbsp;%M \cdot %v = %w</td>
  <td>
   <ul style="padding-left:14px;">
    <li>{ %a + %v | %v \in %V} where<br/>%V is a vector space and %a is a vector</li>
    <li>any point, line, or plane</li>
   </ul>
  </td>
 </tr>
</table>

To represent finite sets of vectors symbolically, we adopt the convention of simply listing the vectors between a pair of braces (as with any
set of objects). However, we need a different convention for symbolically representing vector spaces and affine spaces. This is because we
must use a symbol of finite size to represent a vector space or affine space that may contain an infinite number of vectors.

If the solution spaces to equations of the form %M \cdot %v = %w are infinite, continuous sets of vectors, in what way can be characterize them?
Suppose that %M \in \R^{2 \times 2} and that is %M invertible. Then we have that:
\begin{eqnarray}
  %M & = & #[ %a #, %b #; %c #, %d #] \\
  %w & = & #[ %s #; %t #] \\
  %M \cdot %v & = & %w \\
  #[ %a #, %b #; %c #, %d #] \cdot %v & = & #[ %s #; %t #] \\
           %v & = & (1/(%a%d-%b%c)) \cdot #[ %d #, ---%b #; ---%c #, %a #] \cdot #[ %s #; %t #] \\
           %v & = & (%s/(%a%d---%b%c)) \cdot #[ %d #; ---%c #] + (%t/(%a%d---%b%c)) \cdot #[ ---%b #; %a #]
\end{eqnarray}
Notice that the set of possible solutions %v is a <i>linear combination</i> of two vectors in \R^2. In fact, if a collection of solutions
(i.e., vectors) to the equation %M \cdot %v = \0 exists, it must be a set of linear combinations (in the more general case of
%M \cdot %v = %w, it is a set of linear combinations with some specified offset). Thus, we introduce a succinct notation
for a collection of linear combinations of vectors.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A <i>span</i> of a set of vectors { %v_1, ..., %v_%n } is the set of all linear combinations of vectors in { %v_1, ..., %v_%n }:
\begin{eqnarray}
  \span { %v_1, ..., %v_%n } = { %a_1 \cdot %v_1 + ... + %a_%n \cdot %v_%n &nbsp;|&nbsp; %a_1 \in \R, ..., %a_%n \in \R }
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> For each of the following spans, expand the notation into the equivalent set comprehension and determine if the set
of vectors is a point, a line, a plane, or a three-dimensional space.
  <ol style="list-style-type:lower-alpha;">
    <li> The set defined by:
\begin{eqnarray}
    \span { #[ 1 #; 2#] } & = & {%a \cdot #[ 1 #; 2#] | %a \in \R}
\end{eqnarray}
      The above span is a line.
    </li>
    <li> The set defined by:
\begin{eqnarray}
    \span { #[ 1 #; -2#], #[ ---2 #; 4 #] } & = & \span { #[ 1 #; ---2#] } & = & {%a \cdot #[ 1 #; ---2#] | %a \in \R}
\end{eqnarray}
      The above span is a line.
    </li>
    <li> The set defined by:
\begin{eqnarray}
    \span { #[ 0 #; 1 #], #[ 1 #; 0 #], #[ 1 #; 1 #] } 
      & = & \span { #[ 0 #; 1 #], #[ 1 #; 0 #] } 
      & = & {%a \cdot #[ 1 #; 0#] + %b \cdot #[ 0 #; 1#] | %a,%b \in \R}
      & = & \R^2
\end{eqnarray}
      The above span is a plane.
    </li>
    <li> The set defined by:
\begin{eqnarray}
    \span { #[ 0 #; 0 #] } & = & { #[ 0 #; 0 #] }
\end{eqnarray}
      The above span is a set containing a single point (the origin).
    </li>
    <li> The set defined by:
\begin{eqnarray}
    \span { #[ 0 #; 1 #; 0 #], #[ 1 #; 0 #; 0 #], #[ 0 #; 0 #; 1 #] }
      & = & {%a \cdot #[ 0 #; 1 #; 0#] + %b \cdot #[ 1 #; 0 #; 0 #] + %c \cdot #[ 0 #; 0 #; 1#] | %a,%b,%c \in \R}
      & = & \R^3
\end{eqnarray}
    The above span is a three-dimensional space.
    </li>
  </ol>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Using span notation, describe the set of vectors that are orthogonal to the vector [ 1 ; 2 ] \in \R^2.

We know that the set of vectors orthogonal to [ 1 ; 2 ] can be defined as the line %L where:
\begin{eqnarray}
  %L & = & { #[ %x #; %y #] &nbsp;|&nbsp; #[ %x #; %y #] \cdot #[ 1 #; 2 #] = 0 }
\end{eqnarray}
It suffices to find a vector on the the line %L. The equation for the line is:
\begin{eqnarray}
  #[ %x #; %y #] \cdot #[ 1 #; 2 #] & = & 0 \\
  1 \cdot %x + 2 \cdot %y & = & 0 \\
                       %y & = & (-1/2) %x
\end{eqnarray}
We can choose any point on the line %y = (-1/2) %x; for example, [ 2 ; -1 ]. Then, we have that:
\begin{eqnarray}
  %L & = & \span { #[ 2 #; -1 #] }
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Using span notation, describe the set of solutions to the following matrix equation:

\begin{eqnarray}
  #[ 1 #, 2 #; 2 #, 4 #] \cdot #[ %x #; %y #] & = & #[ 0 #; 0 #]
\end{eqnarray}
Notice that the above equation implies two equations:
\begin{eqnarray}
  #[ 1 #; 2 #] \cdot #[ %x #; %y #] & = & 0 \\
  #[ 2 #; 4 #] \cdot #[ %x #; %y #] & = & 0
\end{eqnarray}
In fact, the second equation provides no additional information because 2 \cdot [ 1 ; 2 ] = [ 2 ; 4 ]. Thus, the solution space
is the set of vectors:
\begin{eqnarray}
  %L & = & { #[ %x #; %y #] &nbsp;|&nbsp; #[ 1 #; 2 #] \cdot #[ %x #; %y #] = 0 }
\end{eqnarray}
We can now use our solution to the previous problem to find a span notation for the solution space:
\begin{eqnarray}
  %L & = & \span { #[ 2 #; -1 #] }
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose also that system states are described by vectors %v \in \R^3 with the following units for each dimension:
\begin{eqnarray}
  %v & = & #[ %x <i style="color:gray;"># carbon atoms </i> #; %y <i style="color:gray;"># hydrogen atoms </i> #; %z <i style="color:gray;"># oxygen atoms </i> #]
\end{eqnarray}
Individual molecules are characterized using the following vectors:
\begin{eqnarray}
  C_3H_8: #[ 3 #; 8 #; 0 #] ,\~ O_2:  #[ 0 #; 0 #; 2 #] ,\~ CO_2: #[ 1 #; 0 #; 2 #] ,\~ H_2O: #[ 0 #; 2 #; 1 #]
\end{eqnarray}

<ul>
  <li>
  Suppose that a mixture contains <i>only</i> molecules of water (H_2O) and carbon dioxide (CO_2).
  Using the span notation, specify the possible set of system states that satisfy these criteria.

  The possible set of system states for the mixture is:
\begin{eqnarray}
  %V & = & \span { #[ 1 #; 0 #; 2 #] , #[ 0 #; 2 #; 1 #] }
\end{eqnarray}
  Recall that the above set is continuous. In this particular problem, only vectors with integer components
  would be of interest; thus, the above set is at least guaranteed to contain all possible system states that satisfy
  the specified criteria. 

Then, if we wanted to solve a more constrained problem that satisfies the above criteria, we would know that we should only
consider vectors %v \in %V as possible solutions.
  </li>
  <li>
  Suppose we observe the following system state (in terms of the number of each kind of atom):
\begin{eqnarray}
  %v & = & #[ 100 #; 400 #; 400 #]
\end{eqnarray}
  How can we determine whether the above mixture <i>could</i> contain only H_20 and CO_2?
  </li>
  <li>
Suppose we want to know if a mixture of only C_3H_8 and O_2 can ever react to produce a mixture of only CO_2 and H_2O.
How can we represent this as an equation of spans? This problem can be represented as follows:
\begin{eqnarray}
  \span { #[ 3 #; 8 #; 0 #],  #[ 0 #; 0 #; 2 #] } & = & \span { #[ 1 #; 0 #; 2 #] ,  #[ 0 #; 2 #; 1 #] }
\end{eqnarray}
  </li>
</ul>
</div>

Recall the <a href="#2.2">definition for what constitutes a vector space</a> (there are many equivalent definitions).

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A <i>vector space</i> is a set of vectors that contains \0, is closed under vector addition and scalar multiplication, and is
such that all the elements in the set satisfy the vector space <a href="#2.2">axioms</a> governing vector addition and scalar multiplication.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Any set of linear combinations of a collection of vectors is closed under vector addition and scalar multiplication, contains
\0, and satisfies the vector space axioms. In other words, for any collection of vectors %v_1, ..., %v_%n, \span{%v_1, ..., %v_%n} is a vector space.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any set of vectors %V \subset \R^n that satisfies the vector space axioms, there exists a finite set of at most %n vectors
 %v_1, ..., %v_%k (where %k \leq %n) such that \span{%v_1, ..., %v_%k} = %V.
</div>

Given the two facts above, we can safely adopt \span{%v_1, ..., %v_%n} as a standard notation for vector spaces. In addition to this notation,
we will also often use the notation \R^n for specific values of %n (e.g., \R^2 is equivalent to \span{[1;0],[0;1]}), and {\0} for specific
vector \0 (e.g., {[0;0]} is equivalent to \span{[0;0]}).

<a name="4.2"></a>
<h3><span class="secn">4.2.</span> Membership and equality relations involving sets of vectors</h3>

Recall that many of the algebraic laws we saw governing operators on vectors and matrices involved equality of vectors and matrices. In fact, one
can view these laws as collectively <i>defining</i> the semantic equality of the symbols (i.e., they specify when two symbols refer to the same
<i>object</i>).

The <i>meaning</i> of symbols is closely tied to the equality relation we define over them. In the case of the span notation, one potential
problem is that there is more than one way to describe a set using span notation. For example:
\begin{eqnarray}
  \span { #[ 2 #; 3 #] } & = & \span { #[ 4 #; 6 #] } \\
  \span { #[ 1 #; 0 #], #[ 0 #; 1 #] } & = & \span { #[ 2 #; 1 #], #[ 0 #; -1 #] }
\end{eqnarray}
How can we determine if two spans are equivalent? We must define the equality relation (i.e., the relational operator) that applies
to sets of vectors (including infinite sets containing all linear combinations of vectors). We first recall the definition of equality
for our vector notation.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Recall that two vectors %v, %w \in \R^n are equal if and only if their components are equivalent:
\begin{eqnarray}
  #[ %a_1 #; %a_2 #; \vdots #; %a_%n #] &nbsp; = &nbsp; #[ %b_1 #; %b_2 #; \vdots #; %b_%n #]
  \~ & iff & \~
  %a_1 = %b_1 &nbsp;and&nbsp; %a_2 = %b_2 &nbsp;and&nbsp; ... &nbsp;and&nbsp; %a_%n = %b_%n
\end{eqnarray}
</div>

With the equality of pairs of vectors defined, it is possible to provide a definition of equality for sets of vectors (both finite and infinite).
However, we will build the definition gradually. First, let us define when a vector is a member of a finite set of vectors using
vector equality.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> A vector %v \in \R^n is a member of the finite set of vectors { %w_1, ..., %w_%k } \subset \R^n if it is equivalent to one of the
vectors in the set:
\begin{eqnarray}
  %v \in { %w_1, ..., %w_%k }
  \~ & iff & \~
  \exists %w \in { %w_1, ..., %w_%k } s.t. %v = %w
\end{eqnarray}
</div>

Notice that the above defines membership of %v in the set using an equation (in this case, %v = %w_%i where %i \in {1,...,%k}). We can
take the same approach in the case of an infinite set of vectors.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> A vector %v \in \R^n is a member of the set of vectors \span { %w_1, ..., %w_%k } \subset \R^n if it is equivalent to one of the
vectors in the set:
\begin{eqnarray}
  %v \in \span { %w_1, ..., %w_%k } \~ 
    & iff & \~ \exists %w \in \span { %w_1, ..., %w_%k } &nbsp;s.t.&nbsp; %v = %w \\
    & iff & \~ \exists <span style="color:red;"><b>%w</b></span> \in { %a_1 \cdot %w_1 + ... + %a_%k \cdot %w_%k &nbsp;|&nbsp; %a_1 \in \R, ..., %a_%n \in \R } &nbsp;s.t.&nbsp; %v = <span style="color:red;"><b>%w</b></span> \\
    & iff & \~ \exists <span style="color:red;"><b>%a_1 \cdot %w_1 + ... + %a_%k \cdot %w_%k</b></span> \in \span { %w_1, ..., %w_%k } &nbsp;s.t.&nbsp; %v = <span style="color:red;"><b>%a_1 \cdot %w_1 + ... + %a_%k \cdot %w_%k</b></span> \\
    & iff & \~ \exists %a_1 \in \R ,..., %a_%n \in \R &nbsp;s.t.&nbsp; %v = %a_1 \cdot %w_1 + ... + %a_%n \cdot %w_%n \\
    & iff & \~ \exists #[ %a_1 #; \vdots #; %a_%k #] \in \R^k &nbsp;s.t.&nbsp; #[ \uparrow #, #, \uparrow #; %w_1 #, ... #, %w_%k #; \downarrow #, #, \downarrow #] \cdot #[ %a_1 #; \vdots #; %a_%k #] = %v\\
\end{eqnarray}
Thus, we can determine membership of a vector in a span by solving a matrix equation of the form %M \cdot %u = %v.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Determine whether the following formula is true:
\begin{eqnarray}
  #[ -1 #; 6 #] & \in & \span { #[ 1 #; 4 #], #[ 1 #; -1 #] }
\end{eqnarray}
We proceed as in the fact above:
\begin{eqnarray}
    #[ -1 #; 6 #] \in \span { #[ 1 #; 4 #], #[ 1 #; -1 #] } \~ 
    & iff & \~ \exists %w \in \span { #[ 1 #; 4 #], #[ 1 #; -1 #] } &nbsp;s.t.&nbsp; #[ -1 #; 6 #] = %w \\
    & iff & \~ \exists %w \in { %a \cdot #[ 1 #; 4 #] + %b \cdot #[ 1 #; -1 #] &nbsp;|&nbsp; %a \in \R, %b \in \R } &nbsp;s.t.&nbsp; #[ -1 #; 6 #] = %w \\
    & iff & \~ \exists %a \cdot #[ 1 #; 4 #] + %b \cdot #[ 1 #; -1 #] \in \span { #[ 1 #; 4 #], #[ 1 #; -1 #] } &nbsp;s.t.&nbsp; #[ -1 #; 6 #] = %a \cdot #[ 1 #; 4 #] + %b \cdot #[ 1 #; -1 #] \\
    & iff & \~ \exists %a \in \R, %b \in \R &nbsp;s.t.&nbsp; #[ -1 #; 6 #] = %a \cdot #[ 1 #; 4 #] + %b \cdot #[ 1 #; -1 #] \\
    & iff & \~ \exists #[ %a #; %b #] \in \R^2 &nbsp;s.t.&nbsp; #[ 1 #, 1 #; 4 #, -1 #] \cdot #[ %a #; %b #] = #[ -1 #; 6 #] \\
\end{eqnarray}
Since a solution to the matrix equation exists, the formula is true:
\begin{eqnarray}
  #[ 1 #, 1 #; 4 #, -1 #] \cdot #[ %a #; %b #] & = & #[ -1 #; 6 #] \\
  %a & = & 1 \\
  %b & = & -2
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Solve each of the following problems.
<ol style="list-style-type:lower-alpha;">
  <li>Determine whether the following formula is true or false:
\begin{eqnarray}
  #[ 2 #; 5 #; -1 #] \in \span { #[ 4 #; 0 #; -2 #], #[ 0 #; 1 #; 0 #] }
\end{eqnarray}
  </li>
  <li>Define the following line using span notation:
\begin{eqnarray}
  %L & = & { #[ %x #; %y #] &nbsp;|&nbsp; %y = -3 \cdot %x }
\end{eqnarray}
  </li>
</ol>
</div>

Now suppose we want to determine if a <i>finite set</i> of vectors is a subset of a span. Let us first consider how we would check if a
finite set of vectors is a subset of another finite set of vectors.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> A finite set of vectors { %v_1, ..., %v_%j } \subset \R^n is a subset of the finite set of vectors { %w_1, ..., %w_%k } \subset \R^n if every
member of { %v_1, ..., %v_%j } is a member of { %w_1, ..., %w_%k }:
\begin{eqnarray}
  { %v_1, ..., %v_%j } \subset { %w_1, ..., %w_%k }
  \~ & iff & \~
  \forall %v \in { %v_1, ..., %v_%j }, \exists %w \in { %w_1, ..., %w_%k } &nbsp;s.t.&nbsp; %v = %w
\end{eqnarray}
</div>

Thus, we can generalize the above by replacing the second finite set of vectors with a span.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> A finite set of vectors { %v_1, ..., %v_%j } \subset \R^n is a subset of the set of vectors \span { %w_1, ..., %w_%k } \subset \R^n if every
member of { %v_1, ..., %v_%j } is a member of \span { %w_1, ..., %w_%k }:
\begin{eqnarray}
  { %v_1, ..., %v_%j } \subset \span { %w_1, ..., %w_%k }
  \~ & iff & \~ \forall %v \in { %v_1, ..., %v_%j }, \exists %w \in \span { %w_1, ..., %w_%k } &nbsp;s.t.&nbsp; %v = %w \\
    & iff & \~ \forall %v \in { %v_1, ..., %v_%j }, \exists #[ %a_1 #; \vdots #; %a_%k #] \in \R^k &nbsp;s.t.&nbsp; #[ \uparrow #, #, \uparrow #; %w_1 #, ... #, %w_%k #; \downarrow #, #, \downarrow #] \cdot #[ %a_1 #; \vdots #; %a_%k #] = %v\\
    & iff & \~ \exists #[ %a<sub>11</sub> #, ... #, %a<sub>1%j</sub> #; \vdots #, #, \vdots #; %a<sub>1%k</sub>#, ... #, %a<sub>%k%j</sub> #] 
                \in \R<sup>%k \times %j</sup> 
       &nbsp;s.t.&nbsp; 
             #[ \uparrow #, #, \uparrow #; %w_1 #, ... #, %w_%k #; \downarrow #, #, \downarrow #] 
       \cdot #[ %a<sub>11</sub> #, ... #, %a<sub>1%j</sub> #; \vdots #, #, \vdots #; %a<sub>1%k</sub>#, ... #, %a<sub>%k%j</sub> #] 
       = #[ \uparrow #, #, \uparrow #; %v_1 #, ... #, %v_%j #; \downarrow #, #, \downarrow #]\\
\end{eqnarray}
</div>

<a name="lecture14"></a>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any finite set of vectors { %v_1, ..., %v_%j } \subset \R^n, for any real number scalars %a_1 \in \R, ..., %a_%j \in \R,
we have that:
\begin{eqnarray}
  { %v_1, ..., %v_%j } \subset \span { %w_1, ..., %w_%k } \~ implies \~ %a_1 \cdot %v_1 + ... + %a_%j \cdot %v_%j \in \span { %w_1, ..., %w_%k }
\end{eqnarray}
We can see the above is true, because:
\begin{eqnarray}
  %v_1 & = & %s<sub>11</sub> \cdot %w_1 + ... + %s<sub>%k1</sub> \cdot %w_%k \\
       & \vdots &  \\
  %v_%j & = & %s<sub>1%j</sub> \cdot %w_1 + ... + %s<sub>%k%j</sub> \cdot %w_%k \\
  %a_1 \cdot %v_1 & = & (%a_1 %s<sub>11</sub>) \cdot %w_1 + ... + (%a_1 %s<sub>%k1</sub>) \cdot %w_%k \\
       & \vdots &  \\
  %a_%j \cdot %v_%j & = & (%a_%j %s<sub>1%j</sub>) \cdot %w_1 + ... + (%a_%j %s<sub>%k%j</sub>) \cdot %w_%k \\
  %a_1 \cdot %v_1 + ... + %a_%j \cdot %v_%j & = & (%a_1 %s<sub>11</sub> + ... + %a_%j %s<sub>1%j</sub>) %w_1 + ... + (%a_1 %s<sub>%k1</sub> + ... + %a_%j %s<sub>%k%j</sub>) %w_%k
\end{eqnarray}  
Thus, we have that:
\begin{eqnarray}
  { %v_1, ..., %v_%j } \subset \span { %w_1, ..., %w_%k } \~ implies \~ \span { %v_1, ..., %v_%j } \subset \span { %w_1, ..., %w_%k }
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Using facts about sets,
we can then specify the following definition of equality between two spans for two finite sets of vectors %V \subset \R^n and
%W \subset \R^n:
\begin{eqnarray}
 \span %W = \span %V \~ & iff & \~ \span %W \subset \span %V and \span %V \subset \span %W
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Determine whether the following formula is true:
\begin{eqnarray}
  \span { #[ 1 #; 2 #], #[ 2 #; 1 #] } & \in & \span { #[ 9 #; 3 #], #[ 3 #; 1 #], #[ -6 #; -2 #] }
\end{eqnarray}
It suffices to set up two matrix equations and determine if solutions %A \in \R<sup>2 \times 3</sup> and %B \in \R<sup>3 \times 2</sup>
exist:
\begin{eqnarray}
  #[ 1 #, 2 #; 2 #, 1 #] \cdot %A & = & #[ 9 #, 3 #, -6 #; 3 #, 1 #, ---2 #] \\
  #[ 9 #, 3 #, -6 #; 3 #, 1 #, ---2 #] \cdot %B & = & #[ 1 #, 2 #; 2 #, 1 #] \\
\end{eqnarray}
Alternatively, we can check each vector individually (this is analogous to considering the above equations for %A and %B one column at a time).
\begin{eqnarray}
  Is #[ 1 #; 2 #] a linear combination of #[ 9 #; 3 #], #[ 3 #; 1 #], and #[ -6 #; -2 #]? \\
  Is #[ 2 #; 1 #] a linear combination of #[ 9 #; 3 #], #[ 3 #; 1 #], and #[ -6 #; -2 #]? \\
  Is #[ 9 #; 3 #] a linear combination of #[ 1 #; 2 #] and #[ 2 #; 1 #]? \\
  Is #[ 3 #; 1 #] a linear combination of #[ 1 #; 2 #] and #[ 2 #; 1 #]? \\
  Is #[ -6 #; -2 #] a linear combination of #[ 1 #; 2 #] and #[ 2 #; 1 #]?
\end{eqnarray}
If all of the above are true, then the two spans are equivalent. If <i>any</i> of the above is false, then the two spans are not equivalent.
</div>

The above implies that a matrix can be used to represent a particular vector space. We will see in other sections
further below that a given vector space can be represented in more than one way by a matrix, and that more than one matrix can be used
to represent a vector space.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Given two vector spaces %W and %V, we have:

  $$ %W is a (vector) subspace of %V \~ iff \~ %V \subset %W.$$
</div>

<a name="lecture14b"></a>

<a name="4.3"></a>
<h3><span class="secn">4.3.</span> Vector spaces as abstract structures</h3>

Our definitions of a vector space so far have explicitly referenced sets of concrete vectors as we usually understand them. However, this is not
necessary.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A <i>vector space</i> is a set of objects %X that satisfies the following conditions:
<ul>
 <li>addition \oplus: %X \times %X \rightarrow %X is an operation on elements of %X such that:
   <ul>
     <li>%X is closed under \oplus, so for any %x,%y \in %X:
\begin{eqnarray}
  %x \oplus %y & \in & %X
\end{eqnarray}
     </li>
     <li>\oplus is commutative and associative; for any %x,%y,%z \in %X, we have:
\begin{eqnarray}
  %x \oplus %y & = & %y \oplus %x \\
  (%x \oplus %y) \oplus %z & = & %x \oplus (%y \oplus %z)
\end{eqnarray}
     </li>
     <li>there is a unique additive identity \0 \in %X for the operation \oplus where for any %x \in %X:
\begin{eqnarray}
  %x \oplus \0 & = & %x \\
  \0 \oplus %x & = & %x
\end{eqnarray}
     </li>
     <li>every element %x \in %X has an additive inverse -%x where:
\begin{eqnarray}
  %x \oplus (-%x) & = & \0 \\
  (-%x) \oplus %x & = & \0
\end{eqnarray}
     </li>
   </ul>
 </li>
 <li>scalar multiplication \otimes: \R \times %X \rightarrow %X is an operation on elements of %X such that:
   <ul>
     <li>%X is closed under \otimes, so for any %s \in \R, %x \in %X:
\begin{eqnarray}
  %s \otimes %x & \in & %X
\end{eqnarray}
     </li>
     <li>1 \in \R is an identity with \otimes, so for any %x \in %X:
\begin{eqnarray}
  1 \otimes %x & = & %x
\end{eqnarray} 
     </li>
     <li>\otimes is associative, so for any %s, %t \in \R, %x \in %X:
\begin{eqnarray}
  %s \otimes (%t \otimes %x) & = & (%s \otimes %t) \otimes %x
\end{eqnarray} 
     </li>
     <li>\otimes distributes across \oplus, so for any %x,%y \in %X:
\begin{eqnarray}
  %s \otimes (%x \oplus %y) & = & %s \otimes %x \oplus %s \otimes %y
\end{eqnarray} 
     </li>
   </ul>
 </ul>
</div>

The above definition specifies conditions under which any set of objects %S can be studied as a vector space. 

We have encountered many sets that satisfy the above properties (thus making them vector spaces); below is a table of vector spaces,
including vector spaces we have already encountered and vector spaces we will encounter later in the course.

<table class="fig_table">
 <tr>
  <td><b>vector space</b></td>
  <td><b>addition<br/>operation</b></td>
  <td><b>additive<br/>identity</b></td>
  <td><b>scalar<br/>multiplication<br/>operation</b></td>
 </tr>
 <tr>
  <td>\R</td>
  <td>addition of<br/>real numbers</td>
  <td>0</td>
  <td>multiplication of<br/>real numbers</td>
 </tr>
 <tr>
  <td>\R^2</td>
  <td>vector addition:<br/>
  [ %a ; %b ] + [ %c ; %d ] = [ %a + %c ; %b + %d ]
  </td>
  <td>[ 0 ; 0 ] </td>
  <td>scalar multiplication:<br/>
 %s \cdot [ %a ; %b ] = [ %s \cdot %a ; %s \cdot %b ]
  </td>
 </tr>
 <tr>
  <td>\R^3</td>
  <td>vector addition:<br/>
  [ %a ; %b ; %c ] + [ %d ; %e ; %f ] = <br/> \~ \~ [ %a + %d ; %b + %e ; %c + %f ]
  </td>
  <td>[ 0 ; 0 ; 0] </td>
  <td>scalar multiplication:<br/>
 %s \cdot [ %a ; %b ; %c ] = [ %s \cdot %a ; %s \cdot %b ; %s \cdot %c ]
  </td>
 </tr>
 <tr>
  <td>\R^n</td>
  <td>vector addition:<br/>
  [ %a_1 ; ... ; %a_%n ] + [ %b_1 ; ... ; %b_%n ] = <br/> \~ \~ [ %a_1 + %b_1 ; ... ; %a_%n + %b_%n ]
  </td>
  <td>[ 0 ; ... ; 0 ] </td>
  <td>scalar multiplication:<br/>
 %s \cdot [ %a_1 ; ... ; %a_%n ] = [ %s ... %a_1 ; ... ; %s \cdot %a_%n ]
  </td>
 </tr>
 <tr>
  <td>\span { [0;0] } = { [0;0] }</td>
  <td>vector addition
  </td>
  <td>[0;0]</td>
  <td>scalar multiplication<br/>of vectors in \R^2
  </td>
 </tr>
 <tr>
  <td>\span { %v_1 , ... , %v_%k } \subset \R^n</td>
  <td>vector addition
  </td>
  <td>[ 0 ; ... ; 0 ] </td>
  <td>scalar multiplication<br/>of vectors in \R^n
  </td>
 </tr>
 <tr>
  <td>\R^{2 \times 2}</td>
  <td>matrix addition
  </td>
  <td>[ 0 , 0 ; 0 , 0 ] </td>
  <td>scalar multiplication<br/>of a matrix
  </td>
 </tr>
 <tr>
  <td>\R^{n \times n}</td>
  <td>matrix addition
  </td>
  <td>[ 0, ..., 0 ; ... ; 0, ..., 0 ] </td>
  <td>scalar multiplication<br/>of a matrix
  </td>
 </tr>
 <tr>
  <td>affine space with<br/>origin at %a \in \R^2</td>
  <td>%v \oplus %w = (%v - %a) + (%w - %a) + %a
  </td>
  <td>%a</td>
  <td>%s \otimes %v = %s \cdot (%v - %a) + %a
  </td>
 </tr>
 <tr>
  <td>set of lines<br/>through the origin<br/>%f(%x) = %a %x</td>
  <td>%f \oplus %g = %h<br/>where %h(%x) = %f(%x) + %g(%x)
  </td>
  <td>%f(%x) = 0 \cdot %x</td>
  <td>%s \otimes %f = %h<br/>where %h(%x) = %s \times %f(%x)
  </td>
 </tr>
 <tr>
  <td>set of polynomials<br/>of degree 2<br/>%f(%x) = %a %x^2 + %b %x + %c</td>
  <td>%f \oplus %g = %h<br/>where %h(%x) = %f(%x) + %g(%x)
  </td>
  <td>%f(%x) = 0</td>
  <td>%s \otimes %f = %h<br/>where %h(%x) = %s \times %f(%x)
  </td>
 </tr>
 <tr>
  <td>set of polynomials<br/>of degree %k<br/>%f(%x) = %a_%k %x^k + ... + %a_0</td>
  <td>%f \oplus %g = %h<br/>where %h(%x) = %f(%x) + %g(%x)
  </td>
  <td>%f(%x) = 0</td>
  <td>%s \otimes %f = %h<br/>where %h(%x) = %s \times %f(%x)
  </td>
 </tr>
</table>

<div class="mathenv example_to_know">
<b>Example:</b> The affine space %A = {%a + %v | %v \in %V} is a vector space for appropriate definitions of addition, scalar multiplication, and identity.
<ul>

 <li>addition (\oplus) can be defined as follows. It is an operation on elements of %A under which %A is closed, and which satisfies the vector space axioms:

  $$%v \oplus %w = %u \~ where \~ %u = (%v - %a) + (%w - %a) + %a = %v + %w - 2%a + %a = %v + %w - %a.$$

 </li>
 <li>scalar multiplication (\otimes) can be defined as follows. It is an operation on elements of %A under which %A is closed, 
     and which satisfies the vector space axioms:

  $$%s \otimes %v = %u \~ where \~ %u = (%s \cdot (%v - %a)) + %a = %s%v - %s%a  + %a = %s%v + (1-%s)%a.$$

 </li>
 <li>there is a unique additive identity in %A; it is the vector %a:

  $$%v \oplus %a = %v + %a - %a = %v.$$

 </li>
</ul>
</div>

An abstract definition of vector spaces is useful because it also allows
us to study by analogy and learn about other objects that are not necessarily sets of vectors. All the properties we can derive about vector
spaces using the above definition will apply to other sets of objects that also satisfy the above definition.

<div class="mathenv example_to_know">
<b>Example:</b> We consider the set of functions %F = {%f | %f(%x) = %c%x, %c \in \R}. How do we show that %F is a vector
space?
<ul>
 <li>there is a unique additive identity in %F:  %f(%x) = 0%x</li>
 <li>addition (+) is an operation on elements of %F under which %F is closed, and which satisfies the vector space axioms:

  $$%f + %g = %h \~ where \~ %h(%x) = %f(%x) + %g(%x)$$

 </li>
 <li>scalar multiplication (\cdot) is an operation on elements of %F under which %F is closed, and which satisfies the vector space axioms:

  $$%s \cdot %f = %h \~ where \~ %h(%x) = %s \cdot %f(%x)$$

 </li>
</ul>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Show that %F = {%f | %f(%x) = %b%x^2 + %c%x, %b,%c \in \R} is a vector space.

For our purposes, it is sufficient to show that there is an additive identity, that the set is closed under addition, and that the set is closed under
scalar multiplication. There is an additive identity:
\begin{eqnarray}
  %f(%x) = 0 \cdot %x^2 + 0 \cdot %x
\end{eqnarray}
The set is closed under the usual addition operation on polynomial curves; for any %f, %g \in %F we have %f + %g = %h where:
\begin{eqnarray}
  %f(%x) & = & %b%x^2 + %c%x \\
  %g(%x) & = & %b'%x^2 + %c'%x \\
  %f(%x) + %g(%x) & = & (%b%x^2 + %c%x) + (%b'%x^2 + %c'%x) \\
                  & = & (%b + %b') \cdot %x^2 + (%c + %c') \cdot %x \\
  %h(%x) & = & (%b + %b') \cdot %x^2 + (%c + %c') \cdot %x \\
  %h & \in & %F   
\end{eqnarray}
For any %f \in %F and %s \in \R we have %h = %s \cdot %f where:
\begin{eqnarray}
  %f(%x) & = & %b%x^2 + %c%x \\
  %s \cdot %f(%x) & = & %s \cdot (%b%x^2 + %c%x) \\
  %s \cdot %f(%x) & = & (%s%b)%x^2 + (%s%c)%x \\
  %h(%x) & = &(%s%b)%x^2 + (%s%c)%x \\
  %h & \in & %F   
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Find two elements in %F = {%f | %f(%x) = %b%x^2 + %c%x + %d, %b,%c,%d \in \R} that are linearly independent. What does it mean for
two functions to be linearly independent?

Two vectors %g, %h \in \F are linearly independent if they are not linearly dependent. This means there exists no scalar such that
%s \cdot %g = %h or %s \cdot %h = %g. One example of such a pair would be:
\begin{eqnarray}
  %g(%x) & = & %x^2 \\
  %h(%x) & = & 1
\end{eqnarray}
There is no single scalar by which we can multiply the curve %h(%x) = 1 to obtain a parabola, and there is no single scalar by which we can
multiply the parabola to obtain a non-zero flat line.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> What is a finite set of functions that spans {%f | %f(%x) = %b%x^2 + %c%x + %d, %b,%c,%d \in \R}? One such set is:

  $${%f,%g,%h} \~ where \~ %f(%x) = 1, \~ %g(%x) = %x, \~ %h(%x) = %x^2$$
</div>

<div class="mathenv example_to_know">
<b>Example:</b> We consider the set of functions {%f | %f(%x) = x^k, %k \in \R}. The following definitions of addition and scalar multiplication
make this set of functions a vector space.
<ul>
 <li>there is a unique additive identity:  %f(%x) = %x^0</li>
 <li>addition (+) is defined as:

  $$%f + %g = %h \~ where \~ %h(%x) = %f(%x) \cdot %g(%x)$$

 </li>
 <li>scalar multiplication (\cdot) is an operation on elements of %S under which %S is closed, and which satisfies the vector space axioms:

  $$%s \cdot %f = %h \~ where \~ %h(%x) = %f(%x)^s$$

 </li>
</ul>
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Once we start thinking of curves as a vector space, we can rewrite curve-fitting problems as equations involving linear combinations of
curves. For example, suppose we have some curve \phi for which we do not know the formula...
\begin{eqnarray}
  \phi(%x) = ... 
\end{eqnarray}
...that represents some observable phenomenon. We can hypothesize that 
the curve \phi(%x) is a linear combination of simpler curves, such as %f(%x) = %x and %g(%x) = %x^2. This is equivalent to
hypothesizing that \phi is in the following vector space:
\begin{eqnarray}
  \span {%f, %g} = { %f &nbsp;|&nbsp; %f(%x) = %a %x^2 + %b %x where %a,%b \in \R }
\end{eqnarray}
We can then set up the following
equation:
\begin{eqnarray}
  %a \cdot %f + %b \cdot %g & = & \phi
\end{eqnarray}
However, the above equation cannot directly be solved for %a,%b \in \R because we do not know the formula for \phi. Suppose we can <i>sample</i>
\phi on some finite collection of input values { %x_1,...,%x_%m } \subset \R. 
We can then write an equation that <i>approximates</i> the above equation at those
points:
\begin{eqnarray}
  %a \cdot #[ %f(%x_1) #; \vdots #; %f(%x_%m) #] + %b \cdot #[ %g(%x_1) #; \vdots #; %g(%x_%m) #] & = & #[ \phi(%x_1) #; \vdots #; \phi(%x_%m) #]
\end{eqnarray}
The above is equivalent to the following system of equations:
\begin{eqnarray}
  %a \cdot %f(%x_1) + %b \cdot %g(%x_1) & = & \phi(%x_1) \\
                                        & \vdots &  \\
  %a \cdot %f(%x_%m) + %b \cdot %g(%x_%m) & = & \phi(%x_%m) 
\end{eqnarray}
It can also be rewritten as the following matrix equation:
\begin{eqnarray}
  #[ %f(%x_1) #, %g(%x_1) #; \vdots #; %f(%x_%m) #, %g(%x_%m) #] \cdot #[ %a #; %b #] & = & #[ \phi(%x_1) #; \vdots #; \phi(%x_%m) #]
\end{eqnarray}
Notice that because we know %f and %g exactly, the matrix above has all constant entries. Furthermore, the right-hand side of the equation also
consists of a vector with all constants as entries because we assumed we can sample \phi on those inputs. Thus, we now have a matrix equation of
the form %M \cdot %v = %w. 

We can now use \rref computation or, if the matrix is invertible, any other method for computing the matrix inverse to solve
the above equation for %a,%b \in \R. This would give us the exact coefficients of a curve in \span {%f, %g} that fits \phi at the %x values { %x_1,...,%x_%m }.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any two natural numbers %k, %m \in \N where %k \neq %m, the curves %f and %g defined below are linearly independent:
\begin{eqnarray}
  %f(%x) = %x<sup>%k</sup> \\
  %g(%x) = %x<sup>%m</sup> 
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Corollary:</b> For any natural number %k \in \N, the curves %f and %g defined below are linearly independent:
\begin{eqnarray}
  %f(%x) = %x<sup>%k</sup> \\
  %g(%x) = %x<sup>%k+1</sup> 
\end{eqnarray}
We can see this by choosing any set of real number inputs { %x_1,...,%x<sub>%k+2</sub> }; the equation below then has no solution %s \in \R:
\begin{eqnarray}
  %f & = & %s \cdot %g \\
  #[ %f(%x_1) #; \vdots #; %f(%x<sub>%k + 2</sub>) #] & = & %s \cdot #[ %g(%x_1) #; \vdots  #; %g(%x<sub>%k + 2</sub>)#]
\end{eqnarray}
Since no such %s exists, %f and %g must be linearly independent.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we have the following two curves:
\begin{eqnarray}
  %f(%x) = %x \\
  %g(%x) = %x^2
\end{eqnarray}
If we choose any three inputs, e.g., {1,2,3}, we will find that %g cannot be a linear combination of %f because the three points will not be collinear.
\begin{eqnarray}
  %f & = & %s \cdot %g \\
  #[ %f(1) #; %f(2) #; %f(3) #] & = & %s \cdot #[ %g(1) #; %g(2) #; %g(3) #] \\
  #[ 1 #; 2 #; 3 #] & = & %s \cdot #[ 1 #; 4 #; 9 #] \\
  %s & = & 1 \\
  %s & = & 1/2 \\
  1 & = & 1/2
\end{eqnarray}
Since we derived a contradiction above, no such %s exists, so %f and %g must be linearly independent.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any collection of exactly %k points in \R^2:
\begin{eqnarray}
  { #[ %x_1 #; %y_1 #], ..., #[ %x_%k #; %y_%k #] }
\end{eqnarray}
there exists a polynomial %f(%x) = %a<sub>%k-1</sub>%x<sup>%k-1</sup> + ... + %a_1 %x + %a_0 that fits those points exactly.

We know the above must be true because the columns of the following matrix in \R<sup>%k \times %k</sup> must be linearly independent:
\begin{eqnarray}
  #[ (%x_1)<sup>%k-1</sup> #, ... #, (%x_1) #, 1 #;
     \vdots #,                    #, \vdots #, \vdots #;
     (%x_%k)<sup>%k-1</sup> #, ... #, (%x_%k) #, 1 #]
\end{eqnarray}
This means that the equation below must have a unique solution:
\begin{eqnarray}
  #[ (%x_1)<sup>%k-1</sup> #, ... #, (%x_1) #, 1 #;
     \vdots #,                    #, \vdots #, \vdots #;
     (%x_%k)<sup>%k-1</sup> #, ... #, (%x_%k) #, 1 #] \cdot #[ %a<sub>%k-1</sub> #; \vdots #; %a_0 #]
     & = &
     #[ %y_1 #; \vdots #; %y_%k #]
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Just as with any vector space, we can set up and solve problems involving linear combinations of vectors. Suppose we want to find a
function in the space {%f | %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d, %a,%b,%c,%d \in \R} that fits a certain collection of points, such as:

  $${(1,3), (-1,13), (2,1), (-2,33)}$$

We can interpret each point as a pair (%x, %f(%x)). We can then set up the following system of equations:

\begin{eqnarray}
 %f(1) & = & 3 \\
 %f(-1) & = & 13 \\
 %f(2) & = & 1 \\
 %f(-2) & = & 33
\end{eqnarray}

Expanding, we have:

\begin{eqnarray}
 %a(1)^3 + %b(1)^2 + %c(1) + %d & = & 3 \\
 %a(-1)^3 + %b(-1)^2 + %c(-1) + %d & = & 13 \\
 %a(2)^3 + %b(2)^2 + %c(2) + %d & = & 1 \\
 %a(-2)^3 + %b(-2)^2 + %c(-1) + %d & = & 33 \\
\end{eqnarray}

Notice that we can rewrite this in the form of an equation %M %v = %w:

\begin{eqnarray}
 #[ (1)^3 #, (1)^2 #, (1) #, 1 #; (-1)^3 #, (-1)^2 #, (-1) #, 1 #; (2)^3 #, (2)^2 #, (2) #, 1 #; #; (-2)^3 #, (-2)^2 #, (-2) #, 1 #] 
  \cdot  #[ %a #; %b #; %c #; %d #] & = & #[ 3 #; 13 #; 1 #; 33 #] 
\end{eqnarray}

This shows us that we can interpret individual objects in {%f | %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d, %a,%b,%c,%d \in \R} as vectors in \R^4.
We can also view this problem in terms of systems, system states, and observations. If the function %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d that solves
the above system is a state of the system, then (3, 13, 1, 33) is a <i>partial</i> observation of that state along four distinct "dimensions".

Note that finding the equation for a line that crosses two points is just a special case of the above.
</div>

<div class="mathenv example_to_know">
<b>Example:</b>  We want to find a function %f(%x) = %c%x + %d such that the points (2,3) and (5,-4) fall on the line the equation represents. Thus,
we have:

\begin{eqnarray}
 #[ (2) #, 1 #; (5) #, 1 #] \cdot #[ %c #; %d #] & = & #[ 3 #; -4 #] 
\end{eqnarray}

We can consider many spaces of functions beyond spaces of polynomial functions. For example, suppose \exp(%x) = %e^x.
Then we can define the following vector space:

  $$ \span{ \exp, \cos, \sin }.$$

We can also generalize this approach to families of related functions.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following set of vectors of functions, where %f' denotes the derivative of a function %f:

  $$ %F = { (%f, %f, %f') \~ | \~ %f(%x) = %b%x^2 + %c%x + %d }$$

The set %F is a vector space. Now, suppose we have the following problem. Find the function for a parabola in %F such that 
(1,4) and (2,-3) lie on the parabola, and the maximum or minimum point on the parabola is at %x = 1. Such a function is
represented by the solution to the following equation:

\begin{eqnarray}
 #[ (1)^2 #, (1) #, 1 #; (2)^2 #, (2) #, 1 #; 2(1) #, 1 #, 0 #] \cdot #[ %b #; %c #; %d #] & = & #[ 4 #; -3 #; 0 #]
\end{eqnarray}

Note that the number of rows in the matrix (and result vector) corresponds to the number of data points for which
a model is being sought, and is not bounded.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Find the order 5 polynomial that exactly fits the following points in \R^2:

\begin{eqnarray}
  #[-1#;45#], #[8#;4#], #[6#;5#], #[2#;-8#], #[-10#;8#], #[15#;6#]
\end{eqnarray}

We must first compute the vectors that approximate each of the following curves that span our space of possible polynomials
at each of the %x components of the points above: 
\begin{eqnarray}
  { %f &nbsp;|&nbsp %f(%x) = %a_5 %x^5 + %a_4 %x^4 + %a_3 %x^3 + %a_2 %x^2 + %a_1 %x + %a_0 + % where %a_%i \in \R }
   & = & \span {%f,%g,%h,%i,%j,%l}
\end{eqnarray}
where
\begin{eqnarray}
   %f(%x) & = & %x^5 \\
   %g(%x) & = & %x^4 \\
   %h(%x) & = & %x^3 \\
   %i(%x) & = & %x^2 \\
   %j(%x) & = & %x \\
   %l(%x) & = & 1 \\
\end{eqnarray}
Thus, we want to solve for the coefficients of the following linear combination of curves:
\begin{eqnarray}
  %a_5 %f + %a_4 %g + %a_3 %h + %a_2 %i + %a_1 %j + %a_0 %l & = & curve represented by data points
\end{eqnarray}
Thus, we would have the following approximation of the above linear combination at the specific %x values in { -1, 8, 6, 2, -10, 15 }:
\begin{eqnarray}
  %a_5 #[ %f(-1) #; %f(8) #; %f(6) #; %f(2) #; %f(-10) #; %f(15) #] +
  %a_4 #[ %g(-1) #; %g(8) #; %g(6) #; %g(2) #; %g(-10) #; %g(15) #] +
  %a_3 #[ %h(-1) #; %h(8) #; %h(6) #; %h(2) #; %h(-10) #; %h(15) #] +
  %a_2 #[ %i(-1) #; %i(8) #; %i(6) #; %i(2) #; %i(-10) #; %i(15) #] +
  %a_1 #[ %j(-1) #; %j(8) #; %j(6) #; %j(2) #; %j(-10) #; %j(15) #] +
  %a_0 #[ %l(-1) #; %l(8) #; %l(6) #; %l(2) #; %l(-10) #; %l(15) #]
  & = &
  #[45#;4#;5#;-8#;8#;6#]
\end{eqnarray}
We can convert the above into the following matrix equation:
\begin{eqnarray}
  #[ %f(-1) #, %g(-1) #, %h(-1) #, %i(-1), #, %j(-1)#, %l(-1) #;
     %f(8) #, %g(8) #, %h(8) #, %i(8), #, %j(8)#, %l(8) #;
     %f(6) #, %g(6) #, %h(6) #, %i(6), #, %j(6)#, %l(6) #;
     %f(2) #, %g(2) #, %h(2) #, %i(2), #, %j(2)#, %l(2) #;
     %f(-10) #, %g(-10) #, %h(-10) #, %i(-10) #, %j(-10)#, %l(-10) #;
     %f(15) #, %g(15) #, %h(15) #, %i(15) #, %j(15)#, %l(15) #]
     \cdot
     #[ %a_5 #; %a_4 #; %a_3 #; %a_2 #; %a_1 #; %a_0 #]
  & = &
  #[45#;4#;5#;-8#;8#;6#]
\end{eqnarray}
It is then sufficient to solve the above equation to obtain the ceofficients, and thus the degree 5 polynomial that exactly fits the points
above.
</div>

The disadvantage to using the approach presented in this subsection to fitting functions to data points is that the data must
match an actual function perfectly (or, equivalently, the space of functions being considered must be rich 
enough to contain a function that can fit the data points exactly). We will see further below
how to find functions that have the "best" fit (for one particular definition of "best") without necessarily matching the points exactly.

<div class="mathenv example_to_know">
<b>Example:</b> Let polynomials in %F = {%f | %f(%t) = %a %t^2 + %b %t + %c } represent a space of possible radio signals.
    To send a vector %v \in \R^3 to Bob, Alice sets her device to generate the signal corresponding to the polynomial in %F whose coefficients
    are represented by %v. 
    Bob can then let his radio receiver sample the radio signals in his environment at multiple points in time %t to retrieve the message.

Suppose Alice wants to transmit the following vector %v \in \R^3 to Bob:
\begin{eqnarray}
  %v & = & #[ 5 #; -2 #; 1 #]
\end{eqnarray}
       Alice sets her radio transmitter to generate a signal whose amplitude as a function of time is:
\begin{eqnarray}
  %f(%t) & = & 5 %t^2 - 2 %t + 1
\end{eqnarray}
  How can Bob recover the message Alice is sending?
  
  Bob can recover the message by having his device sample the signal at three points, e.g., %t_1 = 1, %t_2 = 2, and %t_3 = 3.
  Once Bob does this, he can set up an equation to recover the curve Alice used to generate the signal:
  \begin{eqnarray}
  #[ (1)^2 #, (1) #, 1 #; (2)^2 #, (2) #, 1 #; (3)^2 #, (3) #, 1 #] \cdot %v & = & #[ %f(1) #; %f(2) #; %f(3) #] \\
  #[ 1 #, 1 #, 1 #; 4 #, 2 #, 1 #; 9 #, 3 #, 1 #] \cdot %v & = & #[ 4 #; 17 #; 40 #]
  \end{eqnarray}
  The above equation can then be solved to obtain Alice's message %v.
</div>

<!--assignment4-->

<br/><hr/>
<a name="4.4"></a>
<a name="assignment4"></a>
<h3><span class="secn">4.4.</span>
  <b>Assignment #4: Vector Spaces and Polynomials</b> <!--span class="btn_assignment">(<a href="materials.php?hw=4">show only this assignment</a>)</span-->
</h3>

<only142>

       <p>In this assignment you will solve problems involving vector spaces and vector spaces of polynomials.
       <b>Please submit a single file <code>a4.*</code> containing your solutions. The file extension may be anything you choose;
       please ask before submitting a file in an exotic or obscure file format (plain text or PDF are preferred).</b>
       <p>You can use the following form to compute the \rref using WolframAlpha:      
       <form method="post" target="_blank" action="warref.php"><textarea name="xs"></textarea><input type="submit" value="rref"></form>

<ol>

  <li><ol style="list-style-type:lower-alpha;">
      <li> Explain in detail why the following formula is true for any %a \in \R and %b \in \R if %a \neq 0 and %b \neq 0:
\begin{eqnarray}
  \span { #[ %a #; %b #], #[ %a+1 #; %b #] } & = & \span { #[ 1 #; 0 #], #[ 0 #; 1 #] }
\end{eqnarray}
        <solution>
        This can be done in multiple ways.
        
        One way is to show that the two vectors on the left-hand side are linearly independent. Suppose
        they are linearly dependent. Then there exists %s \in \R such that:
\begin{eqnarray}
  #[ %a #; %b #] & = & %s \cdot #[ %a+1 #; %b #] \\
  %a & = & %s \cdot (%a + 1) \\
  %s & = & %a / (%a + 1) \\
  %b & = & %s \cdot %b \\
  %s & = & 1 \\
  %a / (%a + 1) & = & 1 \\
  %s & \neq & %s
\end{eqnarray}
        Since assuming that they are linearly dependent leads to the contradiction above, they must be
        linearly independent.
        
        Alternatively, we can show that the following matrix is invertible:
\begin{eqnarray}
  \det #[ %a #, %a + 1 #; %b #, %b #] & = & %a \cdot %b - %b \cdot (%a+1) \\
                                      & = & %b ( %a - %a - 1) \\
                                      & = & %b \cdot (-1) \\
  %b & \neq & 0 \\
   \det #[ %a #, %a + 1 #; %b #, %b #] & \neq & 0
\end{eqnarray}
        Thus, the span of its columns is all of \R^2.
        </solution>
      </li>
      <li> Assume addition and scalar multiplication of curves in \R^2 are defined in the usual way as follows:
\begin{eqnarray}
  %f + %g = %h & \~ & where \~ %h(%x) = %f(%x) + %g(%x) \\
  %s \cdot %f = %h & \~ & where \~ %h(%x) = %s \cdot %f(%x)
\end{eqnarray}
      Find a basis of the following vector space:
\begin{eqnarray}
  { %f &nbsp;|&nbsp; %f(%x) = %a \cdot 2<sup>%x+2</sup> + %b \cdot 2^{%x} + %c %x where %a,%b,%c \in \R }
\end{eqnarray}

        <solution>
        The space of curves defined above can be defined as the set of linear combinations of the following three curves:
\begin{eqnarray}
  %f(%x) & = & 2<sup>%x+2</sup> \\
  %g(%x) & = & 2^{%x} \\
  %h(%x) & = & %x
\end{eqnarray}
        However, {%f, %g, %h} is not a basis because %f and %g are linearly dependent:
\begin{eqnarray}
  %f & = & 4 \cdot %g \\
  %f(%x) & = & 4 \cdot %g(%x) \\
  2<sup>%x+2</sup> & = & 4 \cdot 2^{%x} \\
  2<sup>%x+2</sup> & = & 2^2 \cdot 2^{%x}
\end{eqnarray}
        </solution>
      </li>
    </ol>
  </li>

  <li> Find an orthonormal basis of each of the following spaces. Please show your work.

    <ol style="list-style-type:lower-alpha;">
      <li> The line %L defined by:
\begin{eqnarray}
  %L & = & { #[ %x #; %y #] &nbsp;|&nbsp; %y = -3 %x }
\end{eqnarray}

        <solution>
        Since %L is a line, \dim %L = 1, so it is sufficient to find a single unit vector that spans %L.
        Any vector on the line sufficies. If %x = 1, then %y = -3, so we can say:
\begin{eqnarray}
  %L & = & \span { #[ 1 #; -3 #] } \\
   ||#[ 1 #; -3 #]||  & = & \sqrt(1^2 + (-3)^2) \\
                      & = & \sqrt(10)
\end{eqnarray}
        Thus, the orthonormal basis for %L is:
\begin{eqnarray}
  %L & = & \span { #[ 1/\sqrt(10) #; -3/\sqrt(10) #] }
\end{eqnarray}
        </solution>
      </li>

      <li> The space %V defined by:
\begin{eqnarray}
  %V & = & \span { #[ 1 #; 2 #; 0 #; 3 #], #[ 1 #; 0#; 3 #; 1 #], #[ 2 #; 0 #; 0 #; 0 #], #[ 1 #; -2 #; 0 #; -3 #]  }
\end{eqnarray}

        <solution>
        First, we note that the last vector is a linear combination of two other vectors:
\begin{eqnarray}
  #[ 1 #; -2 #; 0 #; -3 #] & = & #[ 2 #; 0 #; 0 #; 0 #] --- #[ 1 #; 2 #; 0 #; 3 #]
\end{eqnarray}
        None of the remaining three vectors are a linear combination of the other two, so
        we have found a basis:
\begin{eqnarray}
  basis %V & = & { #[ 1 #; 2 #; 0 #; 3 #], #[ 1 #; 0#; 3 #; 1 #], #[ 2 #; 0 #; 0 #; 0 #] }
\end{eqnarray}
        Alternatively, we could have found a basis by computing the following \rref and keeping only
        the non-zero rows as the basis:
\begin{eqnarray}
  \rref #[ 1 #, 2 #, 0 #, 3 #; 1 #, 0#, 3 #, 1   #; 2 #, 0 #, 0 #, 0 #; 1 #, -2 #, 0 #, -3 #]
\end{eqnarray}

        We can now apply the Gram-Schmidt process to find an orthonormal basis:
\begin{eqnarray}
  %u_1 & = & #[ 2 #; 0 #; 0 #; 0 #] \\
  %e_1 & = & #[ 1 #; 0 #; 0 #; 0 #] \\
  %u_2 & = & #[ 1 #; 2 #; 0 #; 3 #] --- (#[ 1 #; 2 #; 0 #; 3 #] \cdot #[ 1 #; 0 #; 0 #; 0 #]) \cdot #[ 1 #; 0 #; 0 #; 0 #]\\
       & = & #[ 0 #; 2 #; 0 #; 3 #] \\
  %e_2 & = & #[ 0 #; 2/\sqrt(13) #; 0 #; 3/\sqrt(13) #] \\
  %u_3 & = & #[ 1 #; 0#; 3 #; 1 #] 
              --- (#[ 1 #; 0#; 3 #; 1 #] \cdot #[ 1 #; 0 #; 0 #; 0 #]) \cdot #[ 1 #; 0 #; 0 #; 0 #]
              --- (#[ 1 #; 0#; 3 #; 1 #] \cdot #[ 0 #; 2/\sqrt(13) #; 0 #; 3/\sqrt(13) #]) \cdot #[ 0 #; 2/\sqrt(13) #; 0 #; 3/\sqrt(13) #] \\
       & = & #[ 0 #; 0#; 3 #; 1 #] --- #[ 0 #; 6/13 #; 0 #; 9/13 #] \\
       & = & #[ 0 #; ---6/13#; 3 #; 4/13 #] \\
  %e_3 & = & \sqrt(1573/169) \cdot #[ 0 #; ---6/13#; 3 #; 4/13 #]
\end{eqnarray}
        </solution>
      </li>

      <li> The plane in \R^3 orthogonal to the vector %v where:
\begin{eqnarray}
  %v & = & #[ 1 #; 0 #; 2 #]
\end{eqnarray}

        <solution>
        We must first find a basis for the plane %P, which we can define as:
\begin{eqnarray}
  %P & = & { %w | %w \cdot %v = 0 } \\
     & = & { #[ %x #; %y #; %z #] | 1 \cdot %x + 0 \cdot %y + 2 \cdot %z = 0 }
\end{eqnarray}
        To find one vector on the plane, we simply use the equation in the definition above:
\begin{eqnarray}
   %x + 2 \cdot %z & = & 0 \\
   %x & = & ---2 \cdot %z \\
   %x & = & ---2 \~ (we choose %x as the independent variable and assign a value to it)\\
   %y & = & 2 \~ (since %y can be anything, we choose something convenient) \\
   %z & = & 1 \~ (this is determined by the equation)
\end{eqnarray}
        Next, we want to find another vector in the plane. Since we are looking for an orthonormal basis,
        we look for a vector that is orthogonal to the one we already found above. Thus, we must solve
        a system of two equations:
\begin{eqnarray}
   #[ %x #; %y #; %z #] \cdot  #[ 1 #; 0 #; 2 #] & = & 0 \\
   #[ %x #; %y #; %z #] \cdot  #[ ---2 #; 2 #; 1 #] & = & 0 \\
   %x + 2 \cdot %z & = & 0 \\
    ---2 %x + 2 %y + %z & = & 0 \\
    %x & = & 2 \~ (we choose %x as the independent variable and assign a value to it)\\
    %y & = & 5/2 \\
    %z & = & ---1
\end{eqnarray}
        Thus, we now have two orthogonal vectors. It suffices to normalize both of them:
\begin{eqnarray}
    %V & = & \span { #[ ---2 #; 2 #; 1 #], #[ 2 #; 5/2 #; ---1 #] } \\
       & = & \span { #[ ---2/3 #; 2/3 #; 1/3 #], #[ 4/(3 \sqrt(5)) #; 5/(3\sqrt(5)) #; ---2/(3/\sqrt(5)) #] }
\end{eqnarray}
        </solution>
      </li>

    </ol>
  </li>

  <li> For each of the following, find the exact polynomial (i.e., the natural number representing the degree of the polynomial and its real number coefficients).

    <ol style="list-style-type:lower-alpha;">
      <li> The input box below takes as input a space-separated list of real numbers. Clicking <b>evaluate</b> outputs the result of computing
           %f(%x) for each of the numbers %x in the list.
           <form method="post" target="_blank" action="a43a.php"><input type="text" name="xs"><input type="submit" value="evaluate"></form>
           Suppose that the following is true:
\begin{eqnarray}
  %f & \in & { %f &nbsp;|&nbsp; %f(%x) = %a %x^2 + %b %x + %c where %a,%b,%c \in \R }
\end{eqnarray}
           Find the coefficients of the polynomial %f being computed by the box above. Explain how you obtained your result (write out the equations).
        <solution>
        It is sufficient to sample the polynomial at three points and then to solve for its coefficients:
\begin{eqnarray}
  #[ (0)^2 #, (0) #, 1 #; (1)^2 #, (1) #, 1 #; (2)^2 #, (2) #, 1 #] \cdot #[ %a #; %b #; %c #] & = & #[ %f(0) #; %f(1) #; %f(2) #] \\
  & = & #[ 20 #; 19 #; 12 #] \\
  #[ %a #; %b #; %c #] & = & #[ -3 #; 2 #; 20 #]  
\end{eqnarray}
        Any valid method or tool can be used to solve the equation, but the equation must be specified.
        Note that the sampled points may vary; this is fine as long as the coefficients match the points
        that were being used to fit the curve (that is, solutions with different coefficients should be
        accepted as long as the initial points and curve oefficients and mutually consistent).
        </solution>
      </li>

      <li> The input box below takes as input a space-separated list of real numbers. Clicking <b>evaluate</b> outputs the result of computing
           %f(%x) for each of the numbers %x in the list. The only information available to you is that %f is a polynomial; it has some degree %k,
           but that degree is unknown to you.
           <form method="post" target="_blank" action="a43b.php"><input type="text" name="xs"><input type="submit" value="evaluate"></form>
           Find all the coefficients of the polynomial %f being computed by the box above.
           Explain how you obtained your result (justify your answer by appealing to properties of polynomials and write out the equations).
        <solution>
          This problem can be approached by repeatedly falsifying hypotheses about the polynomial until a hypothesis
          that cannot be falsified is reached.

          First, suppose that the polynomial for the curve has degree %k = 1 (i.e., it is of the form %a %x + %b). Then
          it should be possible to fit a line (a polynomial of the form %a %x + %b) exactly to three points
          on the curve. If this is not possible, then the curve's polynomial has degree greater than %k = 1.
          
          By following this process, we find that an exact solution exists for 6 points and %k = 5.
        </solution>
      </li>
    </ol>
  </li>
  <li>
    Let polynomials in %F = {%f | %f(%t) = %a %t^2 + %b %t + %c } represent a space of possible radio signals.
    To send a vector %v \in \R^3 to Bob, Alice sets her device to generate the signal corresponding to the polynomial in %F whose coefficients
    are represented by %v. Bob then has his receiver sample the radio signals in his environment at multiple points in time %t to retrieve the message.
    <ol type="a">
     <li> Suppose Alice wants to transmit the following vector %v \in \R^3 to Bob:
\begin{eqnarray}
  %v & = & #[ -2 #; 1 #; 3 #]
\end{eqnarray}
       Alice sets her radio transmitter to generate a signal whose amplitude as a function of time is:
\begin{eqnarray}
  %f(%t) & = & -2 %t^2 + %t + 3
\end{eqnarray}
  Suppose Bob wants to recover the message sent by Alice. At how many values of %t should Bob sample the signal to
           recover Alice's message? Write out the computation Bob would perform to recover the message %v from Alice.
       <solution>
       Bob would need to sample the signal at three (3) distinct points, e.g. %t \in {0,1,2}. Bob would then solve the following equation:
\begin{eqnarray}
  #[ (0)^2 #, (0) #, 1 #; (1)^2 #, (1) #, 1 #; (2)^2 #, (2) #, 1 #] \cdot #[ %a #; %b #; %c #] & = & #[ %f(0) #; %f(1) #; %f(2) #] \\
  #[ 0 #, 0 #, 1 #; 1 #, 1 #, 1 #; 4 #, 2 #, 1 #] \cdot #[ %a #; %b #; %c #] & = & #[ 3 #; 2 #; ---3 #] \\
  #[ %a #; %b #; %c #] & = & #[ ---2 #; 1 #; 3 #]
\end{eqnarray}
       </solution>
     </li>
     <li>
     Suppose Bob samples the signals at %t \in {1,2,3} and obtains the vectors listed below. What vector in \R^3 did Alice send? Show your work.
\begin{eqnarray}
  { #[1#;1#], #[2#;---3#], #[3#;---11#] }
\end{eqnarray}
       <solution>
       Bob would need to solve the following equation:
\begin{eqnarray}
  #[ (1)^2 #, (1) #, 1 #; (2)^2 #, (2) #, 1 #; (3)^2 #, (3) #, 1 #] \cdot #[ %a #; %b #; %c #] & = & #[ 1 #; ---3 #; ---11 #] \\
  #[ %a #; %b #; %c #] & = & #[ ---2 #; 2 #; 1 #] \\
\end{eqnarray}
       The solution can be obtained by solving a system of equations, by computing the \rref of an augmented matrix,
       or any other valid method       
       </solution>
     </li>
     <li>
     Suppose the environment contains noise from other communication devices; the possible signals in this noise are always from the span
     of the following polynomials:
\begin{eqnarray}
  %g(%t) & = & 2 %t --- 2 \\
  %h(%t) & = & %t^2 + 3 %t
\end{eqnarray}
     
     Alice and Bob agree that Alice will only try to communicate to Bob one scalar %r \in \R at a time. 
     They agree on a unit vector %u \in \R^3 ahead of time. Any time Alice wants to send some %r \in \R to Bob, she will 
     have her device generate the signal corresponding to the polynomial represented by
     %r \cdot %u.
     
     If the vectors below represent the samples collected by Bob, what scalar was Alice sending to Bob?
\begin{eqnarray}
  { #[1#;3#], #[2#;9#], #[3#;13#] }
\end{eqnarray}

        <solution>
        To allow Bob to distinguish Alice's signal from the noise, Alice must choose a signal that is linearly independent
        from the two noise signals. While Alice and Bob could theoretically agree on any linearly independent vector (as long
        as it is linearly independent, a correct solution is possible), the computions are easiest if the vector is orthogonal
        to the span of the two noise vectors.
        
        The two noise polynomials can be represented as vectors in \R^3, and the noise plane %P can be defined as their span:
\begin{eqnarray}
  %P & = & \span { #[0 #; 2 #; ---2 #], #[ 1 #; 3 #; 0 #] }
\end{eqnarray}
        We want to find a unit vector orthogonal to the plane %P. It is sufficient to find a solution to the following system of
        equations:
\begin{eqnarray}
  #[0 #; 2 #; ---2 #] \cdot #[ %x #; %y #; %z #] & = & 0 \\
  #[ 1 #; 3 #; 0 #] \cdot #[ %x #; %y #; %z #] & = & 0 \\
  ||#[ %x #; %y #; %z #]|| & = & 1
\end{eqnarray}
        Solving the above yields the vector:
\begin{eqnarray}
  %u & = & (1/\sqrt(11)) \cdot #[ ---3 #; 1 #; 1 #]
\end{eqnarray}
        Next, we must determine the coefficients of the curve Bob observed. 
        This curve is a linear combination of the
        noise polynomials and Alice's polynomial (Alice's polynoial is represented by %u).
\begin{eqnarray}
   #[ (1)^2 #, (1) #, 1 #; (2)^2 #, (2) #, 1 #; (3)^2 #, (3) #, 1 #] \cdot #[ %a #; %b #; %c #] & = & #[ 3 #; 9 #; 13 #] \\
   #[ %a #; %b #; %c #] & = & #[ ---1 #; 9 #; ---5 #]
\end{eqnarray}
        Finally, we must remove the contribution of the noise vectors from the observed vector in order
        to isolate Alice's signal. There are two ways to approach this (they are equivalent).
        One approach is to find the orthogonal projection of the observed vector onto the span of the noise vectors,
        and then to subtract this projection from the observed vector. Another approach is to project the observed
        vector directly onto \span {%u}; we use the latter.
\begin{eqnarray}
   ( #[ ---1 #; 9 #; ---5 #] \cdot (1/\sqrt(11)) \cdot #[ ---3 #; 1 #; 1 #]) \cdot (1/\sqrt(11)) \cdot #[ ---3 #; 1 #; 1 #]
     & = & #[ ---21/11 #; 7/11 #; 7/11 #]
\end{eqnarray}
        Since %u is a unit vector, the above vector's length represents the scalar by which %u was multiplied,
        so its length is the scalar %s in %s \cdot %u.
\begin{eqnarray}
   %s & = & ||#[ ---21/11 #; 7/11 #; 7/11 #]|| \\
      & = & \sqrt(539)/11
\end{eqnarray}
        </solution>
     </li>
     <li>
     Suppose there is no noise in the environment. Alice, Bob, and Carol want to send individual scalars to one another at the same time:
     all three would be sending a signal simultaneously, and all three would be listening at the same time:
     <ul>
       <li>Alice's device would generate a signal corresponding to a scalar multiple of %f(%x) = 2 %x + 1;</li>
       <li>Bob's device would generate a signal corresponding to a scalar multiple of %g(%x) = -%x^2 + 1;</li>
       <li>Carol's device would generate a signal corresponding to a scalar multiple of %h(%x) = %x^2 - 3 %x.</li>
     </ul>
     Suppose you sample the radio signals at a given moment and obtain the vectors below. Determine which scalars
     Alice, Bob, and Carol are each transmitting. Your answer can be in the form of a vector, but you must specify which scalar corresponds
     to which sender.
\begin{eqnarray}
   { #[0#;5#], #[1#;7#], #[2#;7#] }
\end{eqnarray}

        <solution>
        We can set up the following equation, where %a, %b, and %c are the scalars being transmitted by
        Alice, Bob, and Carol, respectively:
\begin{eqnarray}
   %a \cdot #[ %f(0) #; %f(1) #; %f(2) #] + %b \cdot #[ %g(0) #; %g(1) #; %g(2) #] + %b \cdot #[ %h(0) #; %h(1) #; %h(2) #] & = & #[ 5 #; 7 #; 7 #] \\
   #[ %f(0) #, %g(0) #, %h(0) #; %f(1) #, %g(1) #, %h(1) #; %f(2) #, %g(2) #, %h(2) #] \cdot #[ %a #; %b #; %c #] & = & #[ 5 #; 7 #; 7 #] \\
   #[ 1 #, 1 #, 0 #; 3 #, 0 #, ---2 #; 5 #, ---3 #, ---2 #] \cdot #[ %a #; %b #; %c #] & = & #[ 5 #; 7 #; 7 #] \\
\end{eqnarray}
        Using any valid method for solving the equation above, we get:
\begin{eqnarray}
  #[ %a #; %b #; %c #] & = & #[ 3 #; 2 #; 1 #] 
\end{eqnarray}
        An alternative approach is to first determine the polynomial being observed by setting up an equation,
        finding the exact curve that fits the three points provided, and then setting up a second matrix equation
        to determine what linear combination of the three signal polynomials adds up to the curve that fits the
        three points. 
        </solution>
     </li>
    </ol>
  </li>
</ol>
<hr/><br/>


</only142>

<only132>
       <p>In this assignment you will use Python to implement several functions that operate on vector spaces and polynomials.
       <b>Please submit a single file <code>a4.py</code> containing your solutions.</b>

  <b style="color:firebrick;">Your file may not import any modules or employ any external library functions
  (unless the problem statement explicitly permits this). 
  You will be graded on the correctness, concision, and mathematical legibility of your code.
  The different problems and problem parts rely on each other; carefully consider whether you can use functions you define
  in one part within subsequent parts.</b>

  In this assignment, we will represent vectors in \R^n and matrices in \R^{n \times n} using Python tuples and nested tuples, respectively
  (as we did in <a href="#assignment3">Assignment #3</a>).
  A helper function <code>solve()</code> is provided below. This function takes a <span style="color:firebrick;"><b>square</b></span> matrix <code>M</code> as its first argument and a vector
  <code>w</code> as its second argument. If there is a solution <code>v</code> to the equation <code>M * v = w</code>, it returns the vector <code>v</code>;
  otherwise, it returns <code>None</code>.
  <b style="color:firebrick;">The <code>solve()</code> method has been updated. Please use the implementation below, which should handle
  zero rows and columns correctly.</b>
  <pre class="snippet">def solve(M, w, epsilon = 1.0/(10**10)):
    rows, cols = len(M)+1, len(M[0])+1
    if rows != cols: return None
    I = [[1 if c==r else 0 for c in range(cols)] for r in range(rows)]
    M = [([M[c][r] for c in range(cols-1)] if r < rows-1 else list(w))+[0]+I[r] for r in range(rows)]
    def soln(M):
        if len([1 for i in range(cols) if abs(M[-1][i]) > epsilon]) > 0: return None   
        return tuple([-1*M[-1][cols+j] for j in range(cols-1)])
    for r in range(rows):
        lead = 0
        if lead >= cols: return soln(M)
        i = r
        while M[i][lead] == 0:
            i += 1
            if i == rows:
                i, lead = r, lead + 1
                if cols == lead: return soln(M)
        while abs(M[r][lead]) < epsilon: lead += 1
        if abs(M[r][lead]) > epsilon:
            M[r] = [ mrx / M[r][lead] for mrx in M[r]]
            for i in [j for j in range(rows) if j != r]:
                if r < rows-1:
                    M[i] = [ iv - M[i][lead]*rv for rv,iv in zip(M[r],M[i])]
    return soln(M)</pre>
  Below is an example of <code>solve()</code> being invoked:
  <pre class="snippet">solve( ((1,3),(2,1)), (5,6) ) # Returns (2.6, 0.8).</pre>
<ol>
  <li> A definition of the <code>Span</code> class is provided below. Objects of this class represent spans; the <code>vectors</code> field
       of each object is the finite set of vectors { %v_1, ..., %v_{%k} } in the notation \span { %v_1, ..., %v_{%k} }.
  <pre class="snippet">class Span():
    def __repr__(self): return "span " + str(self.vectors)
    def __str__(self):  return "span " + str(self.vectors)
    def __init__(self, vectors): self.vectors = vectors
 
    def contains(self, v):
        pass # Implement for Problem #1, part (a).
 
    def __eq__(span1, span2):
        pass # Implement for Problem #1, part (b).
 
    def basis(self):
        pass # Implement for Problem #1, part (c).
 
    def dim(self):
        pass # Implement for Problem #1, part (d).
 
    def orthonormal(self):
        pass # Implement for Problem #1, part (e).</pre>

    <ol style="list-style-type:lower-alpha;">
      <li> Define the <code>Span</code> class's <code>contains()</code> method, which returns <code>True</code> if the vector argument
           <code>v</code> is in the vector space defined by the span, and <code>False</code> otherwise.
           <pre class="snippet">V = Span( { (1,0), (0,1) } )
V.contains((2,-3)) # Returns True.</pre>
      </li>
      <li> Define the <code>Span</code> class's <code>__eq__()</code> method, which takes two arguments that are both <code>Span</code>
      objects. The method should return <code>True</code> if the two spans are equivalent; it should return <code>False</code> otherwise.
      This method is invoked if you apply the <code>==</code> operator to two <code>Span</code> objects:
      <pre class="snippet">Span( { (1,0), (0,1) } ) == Span( { (2,3) } ) # Returns False.</pre>
      </li>
      <li> Define the <code>Span</code> class's <code>basis()</code> method, which returns a finite set of vectors corresponding to a basis
      of the vector space represented by the <code>Span</code> object.
      </li>
      <li> Define the <code>Span</code> class's <code>dim()</code> method, which returns a positive integer corresponding to the dimension
      of the vector space represented by the <code>Span</code> object.
      </li>
      <li> Define the <code>Span</code> class's <code>orthonormal()</code> method, which returns a finite set of vectors corresponding to an
      orthonormal basis of the vector space represented by the <code>Span</code> object.
      </li>
    </ol>
  </li>
 <li> Define a function <code>exactFit()</code> that takes as its input a set of vectors in \R^2.
      If there are %k + 1 vectors in the input, the function should find a
      polynomial of the form %f(%x) = %a_{%k} %x^{%k} + ... + %a_1 %x + %a_0 that exactly fits these vectors.
      The function should return a single vector (%a_{%k}, ..., %a_1, %a_0) containing the coefficients of the polynomial.
      <pre class="snippet">
      exactFit( { \ 
        (-1,31), (-4,  268327), ( 1,  17), (-5, -1520701), ( 3,  389147), \
        (-3,  78869), (-6, -19979509), ( 5,  29571949), ( 0, -1), (-2,  5027) \
        } )
      # The above should return (8, 36, -1, -2, 0, -9, -4, 0, -10, -1) where k is 9.</pre>
 </li>
 <li> In this problem, input polynomials are represented as Python functions. Below are a few examples:
      <pre class="snippet">def f(x): return 2 * x**2 + 3 * x - 5
def g(x): return x**3 + 4 * x**2 - 2 * x + 8</pre>
      Implement the following functions.

    <ol style="list-style-type:lower-alpha;">
      <li> Define a function <code>findK()</code> that takes two inputs, a function <code>f</code> and a positive integer
           <code>k</code>. If there exists a polynomial of the form %f(%x) = %a_{%k} %x^{%k} + ... + %a_1 %x + %a_0 that represents
           the function <code>f</code>, the function should return a vector (%a_{%k}, ..., %a_1, %a_0) containing the coefficients of the polynomial.
           Otherwise, it should return <code>None</code>.
      <pre class="snippet">def f(x): return x**3 + 4 * x**2 - 2 * x + 8
findK(f, 3) # Should return (1, 4, -2, 8).</pre>
      </li>
      <li> Define a function <code>find()</code> that takes a single input, a function <code>f</code>.
           If for any %k there exists a polynomial of the form %f(%x) = %a_{%k} %x^{%k} + ... + %a_1 %x + %a_0 that represents
           the function <code>f</code>, the function should return a vector (%a_{%k}, ..., %a_1, %a_0) containing the coefficients of the polynomial.
           Otherwise, it should return <code>None</code>.
      <pre class="snippet">def f(x): return x**3 + 4 * x**2 - 2 * x + 8
find(f) # Should return (1, 4, -2, 8).</pre>
           Try to make your implementation of
           <code>find()</code> as efficient as possible.
      </li>
    </ol>
  </li>
</ol>
<hr/><br/>

</only132>

<!--/assignment4-->









<a name="lecture15"></a>

<a name="4.5"></a>
<h3><span class="secn">4.5.</span> Basis, dimension, and orthonormal basis of a vector space</h3>

We have adopted \span{%v_1,...,%v_%n} as our notation for vector spaces. However, this makes it possible to represent a given vector space
in a variety of ways: 

  $$\span{(0,1),(1,0)} = \span{(0,1),(1,0),(1,0)} = \span{(0,1),(1,0),(1,0),(1,0)}, and so on.$$

We might naturally ask: given
a vector space %V, what is the <i>smallest</i> possible size for a finite set of vectors %W such that %V = \span %W?


<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A finite set of vectors %B is a <i>basis</i> of the vector space %V if \span %B = %V and for all finite 
sets of vectors %W such that \span %W = %V, |%B| \leq |%W|.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> A finite set of vectors %v_1,...,%v_%n is a basis for %V iff \span {%v_1,...,%v_%n} = %V and the vectors %v_1,...,%v_%n
are setwise linearly independent.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given a finite set of vectors %v_1,...,%v_%n, we can find the basis for \span {%v_1,...,%v_%n} by creating a matrix %M in which the
rows are the vectors %v_1,...,%v_%n and computing \rref %M. The set of distinct nonzero rows in \rref %M is a basis for \span {%v_1,...,%v_%n}.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> We can find the basis of %V = \span {[-3;0;7;0], [-9;0;21;0], [0;0;2;8], [0;0;1;4], [3;0;13;4], [0;0;1;0]} by computing
the reduced row echelon form of the following matrix. We can use a third-party tool to do so (e.g.,
<a target="_blank" href="http://www.wolframalpha.com/input/?i=rref+%28%28-3%2C0%2C7%2C0%29%2C%28-9%2C0%2C21%2C0%29%2C%280%2C0%2C2%2C8%29%2C%280%2C0%2C1%2C4%29%2C%283%2C0%2C13%2C4%29%2C%280%2C0%2C1%2C0%29">WolframAlpha</a>).
\begin{eqnarray}
 %M & = & #[ -3#,0#,7#,0#; -9#,0#,21#,0#; 0#,0#,2#,8#; 0#,0#,1#,4#; 3#,0#,13#,4#; 0#,0#,1#,0 #] \\
 \rref %M & = & #[ 1#,0#,0#,0#; 0#,0#,1#,0#; 0#,0#,0#,1#; 0#,0#,0#,0#; 0#,0#,0#,0#; 0#,0#,0#,0 #] \\
\end{eqnarray}
The rows of the matrix \rref %M will correspond to the vectors in a basis for %V. Thus, we have that:
\begin{eqnarray}
 %V & = & \span { #[-3#;0#;7#;0#], #[-9#;0#;21#;0#], #[0#;0#;2#;8#], #[0#;0#;1#;4#], #[3#;0#;13#;4#], #[0#;0#;1#;0#]} \\
    & = & \span { #[1#;0#;0#;0#], #[0#;0#;1#;0#], #[0#;0#;0#;1#] }
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The <i>dimension</i> of a vector space %V, which we write as \dim %V, is the size of any basis of %V (recall that every basis
is the same size as every other basis, since they all must have the smallest size of any set of vectors that spans %V).
</div>

<div class="mathenv example_to_know">
<b>Example:</b> What is the dimension of {%f | %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d, %a,%b,%c,%d \in \R}?

The dimension of a space is the size of its basis. 
A basis must consist of vectors that are linearly independent from one another, and it must span the space. We know that the following set %B
spans the space:
\begin{eqnarray}
 %B & = & { %f, %g, %h, %l } where \\
  %f(%x) & = & %x^3 \\
  %g(%x) & = & %x^2 \\ 
  %h(%x) & = & %x \\ 
  %l(%x) & = & 1 \\ 
\end{eqnarray}
We also know that the curves %f, %g, %h, and %l are linearly independent. Thus, %B is a basis. Since |%B| = 4 and \span %B = %V, then we
have that:
\begin{eqnarray}
 \dim %V & = & |%B| \\
 & = & 4 
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Consider the vector space %V consisting of finite sets of real numbers:
<ul>
 <li>the empty set is the unique additive identity:  \emptyset</li>
 <li>addition (+) is defined as follows: for any two objects in our vectors space %P \in %V and %T \in %V, we have that

  $$%P + %T = %Q \~ where \~ %Q = (%P \cup %T) - (%P \cap %T)$$

 </li>
 <li>scalar multiplication is defined as:

  $$%s \cdot %T = %Q \~ where \~ %Q = {%s \cdot %r | %r \in %T}$$

 </li>
</ul>
What are the basis and dimension of this vector space?

The basis of the space is %B = { {1} } since you can obtain any finite set of real numbers
by taking linear combinations of the set {1} \in %V. Since \span { {1} } = %V and |{ {1} }| = 1, \dim %V = 1.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A finite set of vectors %B is an <i>orthonormal basis</i> of the vector space %V if %B is a basis of %V,
all the vectors in %B are unit vectors, and the vectors in %B are setwise orthogonal.
</div>

Recall that for any vector %v \in \R^n and any unit vector %u \in \R^n, (%v \cdot %u) \cdot %u is the projection of %v onto the line parallel to
%u (i.e., the vector space {%a %u | %a \in \R}). We can use this fact to define a process for turning an arbitrary basis into an orthonormal basis.

<a name="algorithmgramschmidt"></a>
<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given a basis %B = {%v_1,...,%v_%n}, it is possible to turn it into an orthonormal basis {%e_1,...,%e_%n} by using the Gram-Schmidt process. This
algorithm can be summarized as follows.

\begin{eqnarray}
 %u_1 & = & %v_1 & \~ & %e_1 = %u_1 / ||%u_1 || \\
 %u_2 & = & %v_2 --- ((%v_2 \cdot %e_1) \cdot %e_1) & \~ & %e_2 = %u_2 / || %u_2 || \\
 %u_3 & = & %v_3 --- ((%v_3 \cdot %e_1) \cdot %e_1) --- ((%v_3 \cdot %e_2) \cdot %e_2) & \~ & %e_3 = %u_3 / || %u_3 || \\
 %u_4 & = & %v_4 --- ((%v_4 \cdot %e_1) \cdot %e_1) --- ((%v_4 \cdot %e_2) \cdot %e_2) --- ((%v_4 \cdot %e_3) \cdot %e_3)  & \~ & %e_4 = %u_4 / || %u_4 || \\
      & \vdots &    & \~ &  \\
 %u_%n & = & %v_%n --- &Sigma;_{i=1}^{%n-1} ((%v_%n \cdot %e_%i) \cdot %e_%i) & \~ & %e_%n = %u_%n / || %u_%n ||
\end{eqnarray}

Intuitively, for each vector %v_%i \in %B, the algorithm substracts out the contributions of the already-computed orthogonal unit vectors from
%v_%i, obtaining %u_%i, and then rescales %u_%i to make it into a unit vector %e_%i.

At the end of the above process, {%e_1,...,%e_%n} is the orthonormal basis (and {%e_1,...,%e_%n} is a basis of orthogonal vectors that are not
necessarily unit vectors).
</div>

Many computational environments for mathematics that support linear algebra provide an operator for turning a basis into an orthonormal basis.

Why is an orthonormal basis useful? Once again, we appeal to the fact that (%v \cdot %u) \cdot %u is the projection of %v onto %u, where
%v \cdot %u is the length of that projection. Suppose we want to determine how to express an arbitrary vector %v using an orthonormal
basis %u_1,...,%u_%n.

<a name="lecture16"></a>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given a vector %v \in \R^n, and an orthonormal basis %B =  {%u_1,...,%u_%n} for \R^n, we can determine what linear combination of the vectors
%u_1,...,%u_%n yields %v by computing:

  $$%a = (%M_B)^\top \cdot %v.$$

Notice that the components of the vector %a, which we will call %a_1,...,%a_%n, are the scalars needed to compute %v as a 
linear combination of the vectors %u_1,...,%u_%n:

  $$%v = %a_1 %u_1 + ... + %a_%n %u_%n = %M_B %a.$$

</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given a vector %v \in \R^n, some %k < %n, and an orthonormal basis %B = {%u_1,...,%u_%k} for a subspace %W \subset \R^n,
the product (%M_B \cdot (%M_B)^\top \cdot %v) is the <i>projection</i> of %v onto %W.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Notice the relationship of the above fact to projection onto the span of a single unit vector %u = (%x,%y) in \R^2.
Suppose we have a basis %B = {%u} for a subspace {%s \cdot %u | %s \in \R^2} \subset \R^2. Then we have:

\begin{eqnarray}
  %M_B & = & #[ %x #; %y#]
\end{eqnarray}
  
Thus, for an arbitrary %v \in \R^2 not necessarily in {%s \cdot (%x,%y) | %s \in \R^2} we have:

\begin{eqnarray}
 %M_B \cdot (%M_B)^\top \cdot %v & = & #[ %x #; %y#] \cdot #[ %x #; %y#]^\top \cdot %v \\
                                     & = &   #[ %x #; %y#] \cdot ( #[ %x #; %y#] \cdot %v )  \\
                                     & = & ( #[ %x #; %y#] \cdot %v ) \cdot #[ %x #; %y#] \\
                                     & = & ( %v            \cdot #[ %x #; %y#] ) \cdot #[ %x #; %y#] \\
                                     & = & ( %v \cdot %u ) \cdot %u
\end{eqnarray}

Thus, we see that the projection of a vector onto another unit vector in \R^2 is just a special case of the general approach to projection.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we want to project the vector %v \in \R^2 onto the space %W \subset \R^2 where:
\begin{eqnarray}
  %v & = & #[2 #; 4 #] \\
  %W & = & \span { #[ 1/\sqrt(2) #; 1/\sqrt(2) #]} \\
  %B & = & { #[ 1/\sqrt(2) #; 1/\sqrt(2) #] }
\end{eqnarray}
We compute the matrix %M_B and its transpose. %M_B is a 2 \times 1 matrix in this case, and (%M_B)^\top is a 1 \times 2 matrix:
\begin{eqnarray}
  %M_B & = &  #[ 1/\sqrt(2) #; 1/\sqrt(2) #] \\
  (%M_B)^\top & = &  #[ 1/\sqrt(2) #, 1/\sqrt(2) #]
\end{eqnarray}
We can now apply the formula. Notice that it is equivalent to the formula for projection of %v onto the single vector that spans %W:
\begin{eqnarray}
  %M_B \cdot (%M_B)^\top \cdot %v & = &  #[ 1/\sqrt(2) #; 1/\sqrt(2) #] \cdot #[ 1/\sqrt(2) #, 1/\sqrt(2) #] \cdot  #[2 #; 4 #] \\
                                  & = &  #[ 1/\sqrt(2) #; 1/\sqrt(2) #] \cdot (#[ 1/\sqrt(2) #; 1/\sqrt(2) #] \cdot  #[2 #; 4 #]) \\
                                  & = &  #[ 1/\sqrt(2) #; 1/\sqrt(2) #] \cdot (6/\sqrt(2)) \\
                                  & = &  #[ 3 #; 3 #]
\end{eqnarray}
Thus, the projection of %v onto %W is [3; 3].
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we want to project the vector %v \in \R^3 onto the space %W \subset \R^3 where:
\begin{eqnarray}
  %v & = & #[2 #; 4 #; 5 #] \\
  %W & = & \span { #[ 1/\sqrt(2) #; 1/\sqrt(2) #; 0 #], #[ 0 #; 0 #; 1 #] } \\
  %B & = & { #[ 1/\sqrt(2) #; 1/\sqrt(2) #; 0 #], #[ 0 #; 0 #; 1 #] }
\end{eqnarray}
We compute the matrix %M_B and its transpose. %M_B is a 3 \times 2 matrix in this case, and (%M_B)^\top is a 2 \times 3 matrix:
\begin{eqnarray}
  %M_B & = &  #[ 1/\sqrt(2) #, 0 #; 1/\sqrt(2) #, 0 #; 0 #, 1 #] \\
  (%M_B)^\top & = &  #[ 1/\sqrt(2) #, 1/\sqrt(2) #, 0 #; 0 #, 0 #, 1  #]
\end{eqnarray}
We can now apply the formula:
\begin{eqnarray}
  %M_B \cdot (%M_B)^\top \cdot %v & = &  #[ 1/\sqrt(2) #, 0 #; 1/\sqrt(2) #, 0 #; 0 #, 1 #] \cdot #[ 1/\sqrt(2) #, 1/\sqrt(2) #, 0 #; 0 #, 0 #, 1  #] \cdot #[2 #; 4 #; 5 #] \\
                                  & = &  #[ 1/\sqrt(2) #, 0 #; 1/\sqrt(2) #, 0 #; 0 #, 1 #] \cdot #[6/\sqrt(2) #; 5 #] \\
                                  & = &  #[ 3 #; 3 #; 5 #]
\end{eqnarray}
Thus, the projection of %v onto %W is [3; 3; 5].
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we want to project the vector %v \in \R^3 onto the space %W \subset \R^3 where:
\begin{eqnarray}
  %v & = & #[3 #; 2 #; 4 #] \\
  %W & = & \span { #[ 1/\sqrt(2) #; 0 #; 1/\sqrt(2) #], #[ 0 #; 1 #; 0 #] }
\end{eqnarray}
What is the orthogonal projection of %v onto %W?

Since the two vectors that span %W are already an orthonormal basis, we can simply project %v onto the two individual vectors in the
orthonormal basis and take the sum to obtain the orthogonal projection of %v onto %W:
\begin{eqnarray}
  (#[3 #; 2 #; 4 #] \cdot #[ 1/\sqrt(2) #; 0 #; 1/\sqrt(2) #]) \cdot #[ 1/\sqrt(2) #; 0 #; 1/\sqrt(2) #] + (#[3 #; 2 #; 4 #] \cdot #[ 0 #; 1 #; 0 #]) \cdot #[ 0 #; 1 #; 0 #] 
    & = & (7/\sqrt(2)) \cdot #[ 1/\sqrt(2) #; 0 #; 1/\sqrt(2) #] + 2 \cdot #[ 0 #; 1 #; 0 #] \\
    & = & #[ 7/2 #; 2 #; 7/2 #]
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we want to project the vector %v \in \R^3 onto the space %W \subset \R^3 where:
\begin{eqnarray}
  %v & = & #[-1 #; 2 #; 1 #] \\
  %W & = & \span { #[ 3 #; 4 #; 0 #], #[ 0 #; 0 #; 2 #] }
\end{eqnarray}
What is the orthogonal projection of %v onto %W?

Since the two vectors that span %W are already orthogonal, it is sufficient to turn them into unit vectors in order to obtain an
orthonormal basis of %W. We can then project %v onto the two individual vectors in the
orthonormal basis and take the sum to obtain the orthogonal projection of %v onto %W:
\begin{eqnarray}
  \span {  #[ 3 #; 4 #; 0 #], #[ 0 #; 0 #; 2 #] } & = & \span { #[ 3/5 #; 4/5 #; 0 #], #[ 0 #; 0 #; 1 #] }
  (#[-1 #; 2 #; 1 #] \cdot #[ 3/5 #; 4/5 #; 0 #]) \cdot #[ 3/5 #; 4/5 #; 0 #] + (#[-1 #; 2 #; 1 #] \cdot #[ 0 #; 0 #; 1 #]) \cdot #[ 0 #; 0 #; 1 #]
    & = & 1 \cdot #[ 3/5 #; 4/5 #; 0 #] + 1 \cdot #[ 0 #; 0 #; 1 #] \\
    & = & #[ 3/5 #; 4/5 #; 1 #]
\end{eqnarray}
</div>

<a name="4.6"></a>
<h3><span class="secn">4.6.</span> Homogenous, non-homogenous, overdetermined, and underdetermined systems</h3>

While we have usually considered %n \times %n matrices, in real-world applications (especially those involving data), system states usually have
a shorter description than the corpus of data (e.g., observational data) being used to determine the system state. Thus, problems in such applications
usually involve matrices in which the number of rows is very different from the number of columns. Furthermore, the data may be noisy,
which means that there may be no exact system state that matches the data. Informally, this might mean that the system of equations being
used to determine a system state is <i>overdetermined</i>. On the other hand, if the amount of data is not sufficient to determine a single system
state, a system is <i>underdetermined</i>.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> For any matrix %M \in \R^{n \times m} and vector %v \in \R^m, the system %M %x = %v is <i>overdetermined</i> if there exist 
no solutions to the equation %M %x = %v.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> For any matrix %M \in \R^{n \times m} and vector %v \in \R^m, the system %M %x = %v is <i>undetermined</i> if there exist 
two or more solutions for %x that satisfy the equation %M %x = %v.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> For any matrix %M \in \R^{n \times m}, the system %M %x = \0 is <i>homogenous</i>.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> For any matrix %M \in \R^{n \times m} and vector %v \in \R^m where %v \neq \0, the system %M %x = %v is <i>nonhomogenous</i>.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> A homogenous system %M %x = \0 has at least one solution.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> A homogenous system %M %x = \0 is never overdetermined.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The solution space of a homogenous system is a vector space:
<ul>
 <li>there is a unique additive identity: \0 is a solution</li>
 <li>adding two solutions yields a solution:
\begin{eqnarray}
 %M %x & = & \0 \\
 %M %y & = & \0 \\
 %M (%x + %y) & = & %M %x + %M %y \\
              & = & \0 
\end{eqnarray}
 </li>
 <li>multiplying a solution by a scalar yields a solution:
\begin{eqnarray}
 %M %x & = & \0 \\
 %M (%s %y) & = & %s (%M %y) \\
            & = & %s \0\\
            & = & \0 
\end{eqnarray}
 </li>
</ul>
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The solution space of a nonhomogenous system is an affine space.
</div>

<div class="mathenv example_to_know">
<b>Motivating Example:</b> Suppose we have a nonhomogenous system %M %x = %v where %v \in \R^{1000000000000}. 

We may not know ahead of time whether or
not it is overdetermined. Suppose we do not want to incur the cost of attempting to solve %M %x = %v exactly. This might be because we only want to make
one pass of the data to reduce costs, but it might also be because this is a data stream that is too voluminous to store, so the data <i>will be
gone</i> unless we solve the system right away (e.g., data from a telescope, a particle collider, or Twitter). 

In such cases, we can typically assume the data will have noise, so the system is overwhelmingly likely to be overdetermined and have no solution.
Thus, we are okay with finding an approximate solution.

We do know, however, that %M %x = \0 has at least one solution, and that the solution space is a vector space (so, for example, we
could try to "search" for a better solution using some other technique involving addition and scalar multiplication
once we find at least one non-trivial one). Thus, we might instead
choose to solve the system %M %x = \0, knowing that even in the worst case, a very poor approximate solution of \0 will always be found
in the worst case.

Is there a better compromise between these two strategies of keeping %v or changing it to \0? Can we find some other space of
solutions that won't be overdetermined, but will
still allow us to find a non-trivial (nonzero) approximate solution? One possibility is to <i>project</i> the vector %v onto a subspace
of the solution space, ensuring that the system is no longer overdetermined. This approach is covered in the section below.
</div>

<a name="lecture17"></a>
<a name="4.7"></a>
<h3><span class="secn">4.7.</span> Application: approximating overdetermined systems</h3>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose we have an overdetermined system where %M \in \R<sup>%m \times %n</sup>, %v \in \R^{%n}, and %w \in \R^{%m}:
\begin{eqnarray}
 %M \cdot %v & = & %w
\end{eqnarray}
Let us rewrite %M in terms of its columns %c_%i \in \R^{%m}:
\begin{eqnarray}
  %M & = & #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #] \\
  %v & = & #[ %x_1 #; \vdots #; %x_%n #]
\end{eqnarray}
Let %B = {%c_1,...,%c_%n} be a basis for \span %B \subset \R^{%m}. To be more general, we will use %W = \R^{%m}  and %W' = \span %B:
\begin{eqnarray}
  %B & = & {%c_1,...,%c_%n} \\
  %W & = & \R^{%m} \\
  %W' & = & \span %B \\
  \span %B' & \subset & \R^{%m} \\
  %W' & \subset & %W \\
\end{eqnarray}
Now, it is possible to restate the fact that %M \cdot %v = %w has no solution in a different way:
\begin{eqnarray}
  %M \cdot %v & = & %w \\
  #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #] \cdot #[ %x_1 #; \vdots #; %x_%n #] & = & %w \\
  %x_1 \cdot %c_1 + ... + %x_%n \cdot %c_%n & = & %w \\
  %x_1 \cdot %c_1 + ... + %x_%n \cdot %c_%n & \in & span %B \\
  %M \cdot %v & \in & span %B \\
  %w & \not\in & \span %B
\end{eqnarray}
The above tells us that the real problem is that %w \not\in \span %B. It also tells us that for any %w' in the span of the columns of %M (which we call \span %B), %M \cdot %v = %w' <i>will have a solution</i>:
\begin{eqnarray}
 %M & = & #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #] \\
 %w' & \in & \span {%c_1, ..., %c_%n} \\
 %M \cdot %v & = & %w' has at least one solution
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Suppose we have an overdetermined system where %v \in \R^{%n}, and %w \in \R^{%m}:
\begin{eqnarray}
  #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #] \cdot %v & = & %w
\end{eqnarray}
We can find an <i>approximate solution</i> %v' by picking some %w' \in \span {%c_1, ..., %c_%n} and solving the new equation:
\begin{eqnarray}
  #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #] \cdot %v' & = & %w'
\end{eqnarray}
The <i>error</i> of the approximate solution %v' is defined to be ||%w' --- %w||.
</div>

We see that we may have many choices for %w' in changing an overdetermined system %M \cdot %v = %w to a system with at least one solution %M \cdot %v' = %w'.
Above, we have introduced a notion of <i>error</i> so that we can compare the quality of different choices of %w'. How can we pick the %w' that <i>minimizes</i> the
error ||%w - %w'||? Let us first define what the vector leading to the minimal error is.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Given %w \in %W and a subspace %W' \subset %W, the vector %w' within the subspace %W' that is <i>closest</i> to %w is
a vector %w^\ast \in %W' such that ||%w - %w^\ast|| is minimal. In other words, for all %w' \in W,

  $$||%w - %w^\ast|| \leq ||%w - %w'||.$$

</div>

How do we find %w^\ast?

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> We first review the triangle inequality in the case of a right triangle. Suppose we have a triangle with a height %a, a base length of %b, and
a hypotenuse of length %c. Then we have:
\begin{eqnarray}
 %a^2 + %b^2 & \geq & %a^2 \\
 %c^2 & \geq & %a^2 \\
 %c & \geq & %a
\end{eqnarray}
This implies that the orthogonal projection of a vector %v \in %V onto a subspace %W \subset %V is the closest point in %W to %v.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given %w \in %W and a subspace %W' \subset %W, the orthogonal projection %w^\ast of %w onto %W' is the <i>closest</i> vector in %W' to %w.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose we are given an overdetermined system:
\begin{eqnarray}
 %M \cdot %v & = & %w
\end{eqnarray}
We assume the matrix can be written in terms of its columns:
\begin{eqnarray}
  %M & = & #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #]
\end{eqnarray}
The <i>least-squares</i> approximate solution to %M \cdot %v = %w is the solution %v^\ast to the equation:
\begin{eqnarray}
 %M \cdot %v^\ast & = & %w^\ast
\end{eqnarray}
where %w^\ast is the orthogonal projection of %w onto \span {%c_1, ..., %c_%n}.
The solution %v^\ast is the one that leads to the smallest possible error for any possible vector %v that we can multiply by %M:

  $$||%w - %M %v^\ast|| \leq ||%w - %M_B %v||$$

</div>

Why is it called a "least-squares" approximate solution? In the table below, we summarize the correspondences used in the above facts.

<table class="fig_table">
 <tr>
  <td><b>concept related to<br/>solving %M_B %x = %v</b></td>
  <td align="center"><b>notation</b></td>
  <td align="center"><b>relationship</b></td>
  <td align="center"><b>notation</b></td>
  <td><b>geometric concept</b></td>
 </tr>
 <tr>
  <td>the space of values %W' with<br/>which we can replace %w in<br/>the overdetermined system %M %v = %w<br/>to make a system %M %v = %w'<br/>that has solutions</td>
  <td>{%M \cdot %v | %v \in \R^{%n}}</td>
  <td>the span of the<br/>columns of %M</td>
  <td>\span %B</td>
  <td>the subspace %W' of %W<br/>spanned by %B</td>
 </tr>
 <tr>
  <td>the error of an approximate<br/>solution %v'</td>
  <td>||%w - %M %v' ||</td>
  <td>%M %v' = %w'</td>
  <td>||%w - %w'||</td>
  <td>the distance between<br/>%w \in %W and %w' \in \span %B</td>
 </tr>
 <tr>
  <td>%M \cdot %v^\ast where %v^\ast is<br/>the minimum error solution</td>
  <td>for all %v',<br/>||%w - %M %w^\ast || \leq ||%w - %M %w' ||</td>
  <td>%M %v^\ast = %w^\ast</td>
  <td>for all %w' \in \span %B,<br/>||%w - %w^\ast|| \leq ||%w - %w'||</td>
  <td>the orthogonal projection<br/>%w^\ast of %w \in %W onto \span %B<br/>(the closest vector in \span %B<br/>to %w)</td>
 </tr>
</table>

Notice that we know a solution to %M %v^\ast = %w^\ast exists, and that we can show this in two different ways. Since %w^\ast \in \span %B, it
must be that %w is a linear combination of the vectors in %B, so it is a linear combination of the columns in %M. Alternatively, if %M is
a square matrix and we know that %B is a basis, then the columns of %M are linearly independent, which means %M is invertible, so

  $$%v^\ast = %M^{-1} %w^\ast.$$

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> We can compute the least-squares approximate solution %v^\ast to any equation %M \cdot %v = %w by using the following process:
<ol>
  <li>break %M up into a set of its column vectors</li>
  <li>find an orthonormal basis %B of the span of the column vectors of %M to make %M_B</li>
  <li>use %M_B to compute the projection %w^\ast of %w onto \span %B</li>
  <li>solve the system %M %v^\ast = %w^\ast (e.g., by finding the \rref of an augmented matrix)</li>
</ol>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Find the least-squares approximate solution %v^\ast to the following equation:
\begin{eqnarray}
  #[ 3 #, 0 #; 4 #, 0 #; 0 #, 2 #] \cdot %v & = & #[-1 #; 2 #; 1 #]
\end{eqnarray}

We have an overdetermined system (i.e., an equation with no solution) of the form %M \cdot %v = %w. We must turn it into an equation %M \cdot %v = %w*
that does have a solution by replacing %w with %w*, the orthogonal projection of %w that is in the span of the columns of %M.

Let %W be the span of the columns of the matrix in the equation above:
\begin{eqnarray}
  %W & = & \span { #[ 3 #; 4 #; 0 #], #[ 0 #; 0 #; 2 #] }
\end{eqnarray}
We first need to find an orthonormal basis of %W. Since the two vectors above are already orthogonal, it is sufficient to normalize them.
More generally, we would need to apply the Gram-Schmidt process (this is technically what we are doing here, it is just that the terms
we need to subtract are always 0 in this case). Thus, we have an orthonormal basis:
\begin{eqnarray}
  %W & = & \span { #[ 3/5 #; 4/5 #; 0 #], #[ 0 #; 0 #; 1 #] }
\end{eqnarray}
We now compute the orthogonal projection of %w to find %w* \in %W:
\begin{eqnarray}
  %w* & = & (#[-1 #; 2 #; 1 #] \cdot #[ 3/5 #; 4/5 #; 0 #]) \cdot #[ 3/5 #; 4/5 #; 0 #] + (#[-1 #; 2 #; 1 #] \cdot #[ 0 #; 0 #; 1 #]) \cdot #[ 0 #; 0 #; 1 #]\\
      & = & 1 \cdot #[ 3/5 #; 4/5 #; 0 #] + 1 \cdot #[ 0 #; 0 #; 1 #] \\
      & = & #[ 3/5 #; 4/5 #; 1 #]
\end{eqnarray}
Thus, we now have a solvable equation %M \cdot %v = %w*:
\begin{eqnarray}
  #[ 3 #, 0 #; 4 #, 0 #; 0 #, 2 #] \cdot %v & = & #[ 3/5 #; 4/5 #; 1 #] \\
  %v & = & #[ 1/5 #; 1/2 #]
\end{eqnarray}
The error of the approximate solution is ||%w --- %w*||:
\begin{eqnarray}
  error of the approximate solution #[ 1/5 #; 1/2 #] & = & ||%w --- %w*|| \\
                                                     & = & || #[-1 #; 2 #; 1 #] --- #[ 3/5 #; 4/5 #; 1 #] || \\
                                                     & = & || #[-8/5 #; 6/5 #; 0 #] || \\
                                                     & = & 2
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose a matrix %M is such that %M \cdot %v = \0 has exactly one solution. What can we say about the error of the least-squares 
approximation of any system %M \cdot %v = %w?

Since %M is invertible (because there is exactly one solution to %M \cdot %v = \0), then there is always an exact solution %v to %M \cdot %v = %w:
\begin{eqnarray}
  %M \cdot %v & = & %w \\
  %v & = & %M^{-1} \cdot %w
\end{eqnarray}
Thus, the columns of %M are linearly indepenent and span the space of all possible vectors that can appear on the right-hand side of the equation.
This means that %w^\ast = %w, so the error is ||%w --- %w^\ast|| = ||%w --- %w||| = 0.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose a matrix %M is such that %M \cdot %v = \0 has \R^n as its space of solutions. What can we say about any system %M \cdot %v = %w?
What can we say about the error of any least-squares approximation of a solution for %M \cdot %v = %w?

This means that %M is the matrix consisting of all zeroes. Thus, the span of its columns is { \0 }, the set consisting of only the origin. Thus, the error of
any approximate solution will be ||%w - \0|| = ||%w||.
</div>

<!-- Notice that in the above discussion, %W is a vector space, while {%v} is an affine space. 
More generally, the set of solutions to %M%x = %v is always an affine space. It is always possible to find a least-squares approximation within
the vector space {%x | %M%x = 0}. -->

<div class="mathenv example_to_know">
<b>Example:</b> We saw that if %M is the matrix consisting of all zero entries, %w^\ast must be the zero vector (i.e,. the origin). Are there other
matrices %M for which %w^\ast will be \0?

Yes, if all the columns of %M are orthogonal to %w, then the orthogonal projection of %w onto the span of the columns of %M will be the origin, \0.
For example, consider the following overdetermined system:
\begin{eqnarray}
  #[ 1 #, 2 #; 2 #, 4 #] \cdot %v & = & #[ ---2 #; 1 #]
\end{eqnarray}
Since [---2; 1] \cdot [1; 2] = 0 and [---2; 1] \cdot [2; 4] = 0, the orthogonal projection of [---2; 1] onto the span of the two columns will be [0; 0].
</div>

<h4>Relationship to curve fitting and linear regressions in statistics</h4>

The method described above does overlap with standard curve fitting and linear regression techniques you see
in statistics. There are many variants to these approaches, and the one considered above corresponds to a fairly
basic variant that has no specialized characteristics (i.e., it makes no special assumptions about the data points, 
the relative importance of the data points, their distribution, or the way they can be interpreted).
<ul>
 <li>
  The approach above is known as <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">
  ordinary least squares</a> and as <a href="http://en.wikipedia.org/wiki/Linear_least_squares">
  linear least squares</a>.
 </li>
 <li> 
  The case in which the function space is {%f | %f(%x) = %a%x + %b, %a,%b \in \R}
  corresponds to a <a href="http://en.wikipedia.org/wiki/Simple_linear_regression">simple linear regression</a>.
  This involves fitting a line to a collection of points while minimizing the sum of the squares of the
  distances <i>parallel to the %y-axis</i> between all the data points and the approximate line.
  <a href="http://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Motivational_example">This diagram</a>
  is an illustration of this. The method in these notes is more general in that it is not restricted to
  polynomials of order 1, but can be used to fit polynomials of any order to data.
 </li>
 <li>
  We can restate the least squares method described in these notes as finding %x^\ast such that
  the vector &epsilon; in %M %x^\ast = %v - &epsilon; is minimized. Notice that &epsilon; + %w = %v.
  The length of the vector &epsilon; can be viewed as the sum of squares of %y-axis-aligned distances from the
  estimate curve to the actual data points:

  $$ ||&epsilon;|| = ||%v - %M %x^\ast||. $$

 </li>
 <li>
  <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares#Geometric_approach">This diagram</a> illustrates the
  projection method being used.
 </li>
 <li>
 The matrix %M in %M %x = %v is sometimes called the
 <a href="http://en.wikipedia.org/wiki/Design_matrix">design matrix</a>.
 The same term is used to refer to the 
 <a href="http://en.wikipedia.org/wiki/Design_matrix#Simple_Regression">corresponding
 matrix in a simple linear regression.</a>
 </li>
</ul>

<a name="4.8"></a>
<h3><span class="secn">4.8.</span> Application: approximating a model of system state dimension relationships</h3>

The approach described in the previous section can be used to approximate a solution to the equation %M %v = %w. Usually, if %M describes
a model of a system (in this sense, <i>system</i> refers to a system of states that are described by values along some number of dimensions),
solving %M %v = %w corresponds to determining a system state %v given some other information about a system state %w.

What if we instead have a large collection of pairs of observations of system states of the form (%v,%w) (or, more generally, any collection of
observations of a system along multiple dimensions). Can we approximate a model of a set of relationships between some two subsets of the
dimensions in the system? In other words, if we take all our observations (%x_1, ..., %x_%n) and divide them into pairs of descriptions
%x = (%x_1,...,%x_%k) and %v = (%x_{k+1},...,%x_%n), can we find the best %M that approximates the relationship between incomplete
system state descriptions %x and %v?

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we have the following dimensions in our system:

<ul>
 <li>number of red dwarf stars</li>
 <li>number of G type stars</li>
 <li>number of pulsars</li>
 <li>units of infrared (IR) radiation observed</li>
 <li>units of ultraviolet (UV) radiation observed</li>
 <li>units of gamma (&gamma;) radiation observed</li>
</ul>

Suppose that we already have some set of confirmed (but possibly noisy) observations of systems along <i>all</i> the dimensions (the dimensions
of the quantities in each point correspond to the list of dimensions above):
\begin{eqnarray}
  #[ 4#;  5#;  1#; 243#; 3341#;  700#], 
  #[ 3#;  6#; 21#; 125#; 1431#; 1465#], 
  #[ 4#;  2#; 13#; 533#; 3432#;  334#], 
  #[16#;  4#;  4#; 334#;  143#;  762#], 
  #[13#;  8#; 13#; 235#; 1534#;  513#], 
  #[34#; 16#; 17#; 333#; 3532#;  450#]
\end{eqnarray}

For the above observations, we want to find a matrix that relates the first three dimensions (the number of each type of star) to the last
three dimensions (the amount of each kind of radiation). In other words, we are looking for a matrix %M \in \R^{3 \times 3} of the
form:

  $$#[ %a <i style="color:gray;">units IR/red dwarf</i> #, %b <i style="color:gray;">units IR/G type</i> #, %c <i style="color:gray;">units IR/pulsar</i> #; 
       %d <i style="color:gray;">units UV/red dwarf</i> #, %e <i style="color:gray;">units UV/G type</i> #, %f <i style="color:gray;">units UV/pulsar</i> #; 
       %g <i style="color:gray;">units &gamma;/red dwarf</i> #, %h <i style="color:gray;">units &gamma;/G type</i> #, %i <i style="color:gray;">units &gamma;/pulsar</i> #] $$

Multiplication by the above matrix represents a system state description transformation with the following units:

  $$ (<i style="color:gray;"># red dwarf stars</i>, <i style="color:gray;"># G type stars</i>, <i style="color:gray;"># pulsars</i>) 
     \to (<i style="color:gray;">units IR</i>, <i style="color:gray;">units UV</i>, <i style="color:gray;">units &gamma;</i>) $$

We can split the data into two collections of points: those that specify the number of each type of star (the inputs to the above transformation),
and those that specify the amount of each kind of radiation (the output of the above transformation). Below, we call these %P and %Q. We then
turn these two sets of data points into matrices. In order to turn this problem into the familiar form %M %v = %w, we can transpose the two
matrices.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we have four dimensions in our system. Also, suppose that we already have some set of 
confirmed (but possibly noisy) observations of systems along <i>all</i> the dimensions (the dimensions
of the quantities in each point correspond to the list of dimensions above):
\begin{eqnarray}
  #[ 1 #; 2#; 1#; 0#], 
  #[ 0 #; 3#; 0#; 1#], 
  #[ 2 #; 4#; 2#; 0#], 
  #[ 0 #; 2#; 0#; 1#], 
\end{eqnarray}

For the above observations, we want to find a matrix that relates the first two dimensions to the last
two dimensions. We would then set up the following equations:
\begin{eqnarray}
  #[ %a #, %b#; %c#, %d #] \cdot #[1 #; 0#] & = & #[1 #; 2#] \\
  #[ %a #, %b#; %c#, %d #] \cdot #[0 #; 1#] & = & #[0 #; 3#] \\
  #[ %a #, %b#; %c#, %d #] \cdot #[2 #; 0#] & = & #[2 #; 4#] \\
  #[ %a #, %b#; %c#, %d #] \cdot #[0 #; 1#] & = & #[0 #; 2#]
\end{eqnarray}
Unfortunately, there is no single solution %a,%b,%c,%d \in \R to the above collection of equations.
To find an approximate solution, we can convert the above into a single matrix equation:
\begin{eqnarray}
  #[1 #, 0 #; 0 #, 1 #; 2 #, 0 #; 0 #, 1 #] \cdot #[ %a #, %c#; %b#, %d #] & = & #[1 #, 2 #; 0 #, 3 #; 2 #, 4 #; 0 #, 2#]
\end{eqnarray}
We can go further and turn this into an equation of the form %M \cdot %v = %w:
\begin{eqnarray}
  #[1 #, 0 #, 0 #, 0 #; 0 #, 1 #, 0 #, 0 #; 2 #, 0 #, 0 #, 0 #; 0 #, 1 #, 0 #, 0 #;
    0 #, 0 #,1 #, 0 #; 0 #, 0 #,0 #, 1 #; 0 #, 0 #,2 #, 0 #; 0 #, 0 #,0 #, 1   #] 
  \cdot #[ %a #; %b#; %c#; %d #] 
  & = & #[ 1 #; 0 #; 2 #; 0 #; 2 #; 3 #; 4 #; 2 #]
\end{eqnarray}
In either case, we can now approach the problem by finding the <i>closest</i> vector or matrix that is in the span of the matrix on the left-hand side of the equation.
We do this by finding the orthogonal projection of the right-hand side onto the span of the matrix.
\begin{eqnarray}
  #[ 1 #; 0 #; 2; #; 0 #] & \in & \span {#[ 1 #; 0 #; 2; #; 0#], #[ 0 #; 1 #; 0; #; 1 #]} \\
  #[ 2 #; 3 #; 4; #; 2 #] & \not\in & \span {#[ 1 #; 0 #; 2; #; 0#], #[ 0 #; 1 #; 0; #; 1 #]}
\end{eqnarray}
The above tells us that it is sufficient to project the vector [2; 3; 4; 2] onto the span. The projection is:
\begin{eqnarray}
  (#[ 2 #; 3 #; 4 #; 2 #] \cdot #[ 1 #; 0 #; 2 #; 0#] \cdot (1/\sqrt(3)))  \cdot #[ 1 #; 0 #; 2 #; 0#] \cdot (1/\sqrt(3)) 
  + (#[ 2 #; 3 #; 4 #; 2 #] \cdot #[ 0 #; 1 #; 0 #; 1 #] \cdot (1/\sqrt(2)))  \cdot #[ 0 #; 1 #; 0 #; 1 #] \cdot (1/\sqrt(2))
\end{eqnarray}
The above yields:
\begin{eqnarray}
10/3 \cdot #[ 1 #; 0 #; 2 #; 0 #] + 5/2 \cdot #[ 0 #; 1 #; 0 #; 1 #]  & = & #[ 10/3 #; 5/2 #; 20/3 #; 5/2 #]
\end{eqnarray}
We can now set up an equation that has a solution:
\begin{eqnarray}
  #[1 #, 0 #; 0 #, 1 #; 2 #, 0 #; 0 #, 1 #] \cdot #[ %a #, %c#; %b#, %d #] & = & #[1 #, 10/3 #; 0 #, 5/2 #; 2 #, 20/3 #; 0 #, 5/2#] \\
  #[ %a #, %b #; %c#, %d #] & \approx &  #[ 1 #, 0 #; 10/3 #, 5/2 #]
\end{eqnarray}
</div>




















<a name="4.9"></a>
<h3><span class="secn">4.9.</span> Application: distributed computation of least-squares approximations</h3>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> We assume there are %n distinct computing devices {%C_0, ..., %C_%n} (e.g., servers, virtual machines, motes, mobile devices, etc.), that
any device can communicate with a small number (e.g., two) of other devices at any given moment in time, and that all devices can compute or
communicate simultaneously (i.e., independently of one another). Then the following are true:
<ul>
  <li>if %C_%i knows some small piece of information (e.g., a real number %r \in \R), the devices can distribute %r to all the devices in about \log %n time steps;</li>
  <li>if every device %C_%i has some real number %r_%i, the devices can collectively compute %r_1 + ... + %r_%n in about \log %n time steps.</li>
</ul>
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> The process of finding an approximate solution %v^\ast to an overdetermined system %M \cdot %v = %w can be broken down into the following basic operations:
<ul>
  <li>addition and subtraction of vectors;</li>
  <li>projection of one vector onto another vector;</li>
  <li>solving a system %M \cdot %v^\ast = %w^\ast that must have a solution.</li>
</ul>
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If two vectors [%a_1; ...; %a_%n] and [%b_1; ...; %b_%n] are stored in a distributed manner across multiple devices {%C_0, ..., %C_%n} such that device %C_%i stores only %a_%i and %b_%i, then the devices can collective compute the sum of these two vectors in one time step: each device %C_%i simply computes %a_%i + %b_%i
and stores the result internally.
</div>

<a name="lecture18a"></a>

<a name="4.10"></a>
<h3><span class="secn">4.10.</span> Orthogonal complements and algebra of vector spaces</h3>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The orthogonal complement %W^\bot of a vector subspace %W \subset %V is the set of all vectors in %V that are orthogonal
to all the vectors in %W:

  $$%W^\bot = {%v | %v \in %V, \forall %w \in %W, %v \cdot %w = 0}$$
</div>

Notice that the orthogonal complement of a subspace is always taken within the context of some specified space %V.
We consider vector spaces, their orthogonal complements, and common set operations: set union, set intersection, and set product.

<div class="mathenv example_to_know">
<b>Example:</b> Given any two vectors spaces %V and %W, is %V \cup %W a vector space? <b>No.</b> For example, consider

  $$%V = \span{[1;0]} \cup \span{[0;1]}.$$

We have that [1;0] \in %V and [0;1] \in %V, but not [1;0] + [0;1] = [1;1] \in %V.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Given any two vectors spaces %V and %W, is %V \cap %W a vector space? <b>Yes.</b>
<ul>
 <li>there is a unique additive identity:  \0 \in %V and \0 \in %W, so \0 \in %V \cap %W</li>
 <li>%V \cap %W is closed under addition (+): if %v,%w \in %V \cap %W then we have that %v \in %V and %w \in %V, and we also have that
     %v \in %W and %w \in %W, so we have that

  $$%v + %w \in %V \~ and \~ %v + %w \in %W, \~ so \~ %v + %w \in %V \cap %W.$$
  
  Since addition satisfies the vector space axioms for elements %V and %W, it also does so for elements in %V \cap %W.
 </li>
 <li>%V \cap %W is closed under scalar multiplication (\cdot): if %v \in %V \cap %W then we have that %v \in %V and %w \in %V, so we have that

  $$%s%v \in %V \~ and \~ %s%v \in %W, \~ so \~ %s%v \in %V \cap %W.$$

  Since scalar multiplication satisfies the vector space axioms for elements %V and %W, it also does so for elements in %V \cap %W.
 </li>
</ul>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Given any two vectors spaces %V and %W, is %V \times %W a vector space? <b>Yes.</b>
<ul>
 <li>there is a unique additive identity:  (\0_V,\0_W) where \0_V \in %V and \0_W \in %W</li>
 <li>there is an addition operation (+):

  $$ (%v,%w) + (%v',%w') = (%v+%v', %w+%w') $$

 </li>
 <li>there is a scalar multiplication (\cdot) operation:

  $$ %s (%v,%w) = (%s%v, %s%w) $$

 </li>
</ul>
</div>

<div class="mathenv example_to_know">
<b>Exercise:</b> For a vector space %W \subset %V, compute %W \cap %W^\bot.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For a vector subspace %W \subset %V,

  $$\dim %W + \dim %W^\bot = \dim %V.$$

</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For a vector subspace %W \subset %V,

  $$%W \times %W^\bot = %V.$$

</div>


<a name="lecture19"></a>

<a name="5"></a>
<hr style="margin-bottom:120px;"/>
<h2><span class="secn">5.</span> Linear Transformations</h2>

<a name="5.1"></a>
<h3><span class="secn">5.1.</span> Set products, relations, and maps</h3>

We consider the following sets of vectors and their properties.

<table class="fig_table">
 <tr>
  <td><b>construct</b></td>
  <td><b>definition</b></td>
  <td><b>example</b></td>
  <td><b>graphical example</b></td>
 </tr>
 <tr>
  <td>%V \times %W</td>
  <td>{ (%v,%w) | %v \in %V, %w \in %W }</td>
  <td>{1,2,3} \times {4,5,6} <br/>=<br/>{(1,4),(1,5),(1,6),<br/>(2,4),(2,5),(2,6),<br/>(3,4),(3,5),(3,6)}</td>
  <td></td>
 </tr>
 <tr>
  <td>%R is a relation between %V and %W</td>
  <td>%R \subset %V \times %W</td>
  <td>{(1,D), (2,B), (2,C)} <br/>is a relation between<br/> {1,2,3,4} and {A,B,C,D}</td>
  <td><img src="images/relation.png"></td>
 </tr>
 <tr>
  <td>%f is a function from %V to %W<br/>%f is a map from %V to %W</td>
  <td>
    %f is a relation between %V and %W and<br/>
    \forall %v \in %V, there is at most one<br/>%w \in %W s.t. %f relates %v to %w
  </td>
  <td>{ (%x,%f(%x)) | %f(%x) = %x^2 }</td>
  <td><img src="images/function.png"></td>
 </tr>
 <tr>
  <td>%R^{-1} is the inverse of %R</td>
  <td>{ (%w,%v) | (%v,%w) \in %R }</td>
  <td></td>
  <td></td>
 </tr>
 <tr>
  <td>%f: %X \to %Y is injective</td>
  <td>
  </td>
  <td></td>
  <td><img src="images/injection.png"></td>
 </tr>
 <tr>
  <td>%f: %X \to %Y is surjective</td>
  <td>
  </td>
  <td></td>
  <td><img src="images/surjection.png"></td>
 </tr>
 <tr>
  <td>%f: %X \to %Y is bijective</td>
  <td>
  </td>
  <td></td>
  <td><img src="images/bijection.png"></td>
 </tr>
</table>

Notice that we may have %f such that %f is a function, but %f^{-1} is not a function.

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If we say that a map %f is a relation between %V and %W, we can denote this as %f : %V \to %W or %f \in %V \to %W.
Then, %V is the <i>domain</i> of %f and %W is the <i>codomain</i>.
</div>

Notice that the definition of %f does not uniquely determine its codomain. For example,
%f(%x) = %x^2 could have a codomain of \R if %f : \R \to \R, or it could have a codomain of \R^{+} if %f : \R \to \R^{+}.

<div class="mathenv example_to_know">
<b>Example:</b> Describe with as much detail as you can the domains, codomains, and images of the following maps (there is more than one right
answer for these, as the definitions do not specify the domains of the functions). Determine whether each map is injective, surjective, or both
given the domains and codomains you chose.

\begin{eqnarray}
 %f(%x) & = & 3%x & \~ & \\
 %f(%x) & = &  %x^2 & \~ & \\
 %f(%x) & = &  -%x & \~ & \\
 %f(%x) & = &  %x \cdot \sqrt(-1) & \~ & \\
 %f(%x) & = &  #[%x #, 0 #; 0 #, %x#] & \~ & \\
 %f( %M, %N ) & = &  %M \cdot %N & \~ & where %M and %N are matrices
\end{eqnarray}
\begin{eqnarray}
 %f( #[%x #; %y #] ) & = &  #[1 #, 2 #; 3 #, 4#] #[%x #; %y #]
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given some %f:%V \to %W, the following process can be used to show a map %f is injective:
<ol>
 <li>assume %v,%v' \in %V and %v \neq %v';</li>
 <li>it is necessary to show that %f(%v) \neq %f(%v'):
  <ol type="a">
   <li>expand %f(%v) and %f(%v') using the definition of %f;</li>
   <li>use algebraic properties or other facts to derive %f(%v) \neq %f(%v').</li>
  </ol>
 </li>
</ol>
Alternatively, we can use the following approach:
<ol>
 <li>assume %v,%v' \in %V;</li>
 <li>assume %f(%v) = %f(%v'):
  <ol type="a">
   <li>expand %f(%v) and %f(%v') using the definition of %f;</li>
   <li>use algebraic properties or other facts to derive %v = %v'.</li>
  </ol>
 </li>
</ol>
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given some %f: %V \to %W, the following process can be used to determine whether a map %f is surjective:
<ol>
 <li>let %w \in %W;</li>
 <li>solve %f(%v) = %w:</li>
  <ul>
   <li>if you can solve the equation (or derive an equation %v = %g(%w)) such that %v \in %V, %f:%V \to %W is surjective;</li>
   <li>if the equation has no solutions in %V, %f:%V \to %W is not surjective.</li>
  </ul>
 </li>
</ol>
</div>

Notice that the above does not mean that there is not some %V' where %V \subset %V' that does have a solution
to the equation %f(%v) = %w. For example, %f(%x) = 3 %x is surjective if %f \in \R \to \R but not surjective
if %f \in \Z \to \Z.

<div class="mathenv example_to_know">
<b>Example:</b> Show that %f: \R \to \R where %f(%x) = %x + 1 is injective.

We can show this as follows by first adding 1 to both sides, then applying the definition of %f:
\begin{eqnarray}
 %x & \neq & %x' \\
 %x+1 & \neq & %x'+1 \\
 %f(%x) & \neq & %f(%x')
\end{eqnarray}
Alternatively, we could use the other method by first apply the definition of %f, then subtracting 1 from both sides.
\begin{eqnarray}
 %f(%x) & = & %f(%x')\\
 %x+1 & = & %x'+1 \\
 %x & = & %x' 
\end{eqnarray}

</div>

<div class="mathenv example_to_know">
<b>Example:</b> Show that %f: \R \to \R where %f(%x) = %x + 1 is surjective.

Suppose any value in the codomain %r \in \R is chosen. Then we can solve the following equation for a corresponding value %x in the domain:
\begin{eqnarray}
 %f(%x) & = & %r \\
 %x+1 & = & %r \\
 %x & = & %r - 1
\end{eqnarray}

</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A map %f: %V \to %W is a <i>bijection</i> or is <i>bijective</i> if %f is an injection from %V to %W and %f is a surjection from %V to %W.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Show that %f: \R \to \R where %f(%x) = %x + 1 is bijective.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Given a map %f: %V \to %W, we call { %f(%v) | %v \in %V } the <i>image</i> of %f:
\begin{eqnarray}
 \im(%f) & = & { %f(%v) | %v \in %V }
\end{eqnarray}
</div>

<a name="5.2"></a>
<h3><span class="secn">5.2.</span> Homomorphisms and isomorphisms</h3>

The previous section defined a number of properties of functions %f: %A \to %B that only address the equality of elements in %A and %B. However,
what if %A and %B have relations and operators? For example, what if %A and %B are vector spaces with an identity, an addition operation, and a
scalar multiplication operation?

We can restrict our notion of a map or function to maps or functions %f: %A \to %B that somehow preserve or consistently transform the properties 
of operators and relations we may have already defined on %A and %B. For example, suppose we have %A = {%a_1, %a_2, %a_3}, %B = {%b_1, %b_2, %b_3},
an operator \oplus such that %a_1 \oplus %a_2 = %a_3, and an operator \oplus over elements of %B such that %b_3 \oplus %b_2 = %b_1. A map %f from
%A to %B that preserves (or respects, or consistently transforms) \oplus would be such that

\begin{eqnarray}
 %f(%a_3) = %b_1 \\
 %f(%a_2) = %b_2 \\
 %f(%a_1) = %b_3
\end{eqnarray}

Notice that the above map is such that the operation \oplus is respected by the map %f:

\begin{eqnarray}
 %f(%a_3) \~ = \~ %f(%a_1 \oplus %a_2) \~ = \~ %f(%a_1) \oplus %f(%a_2) \~ = \~ %b_3 \oplus %b_2 \~ = \~ %b_1 \\
\end{eqnarray}

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Given a binary operator \otimes over %A and another operator \oplus over %B, we say that a map %f: %A \to %B is a <i>homomorphism</i>
if we have that

  $$\forall %a,%a' \in A, \~ %f(%a \otimes %a') = %f(%a) \oplus %f(%a').$$

Notice that a homomorphism might have, but does not necesserily have, any of the properties we introduced for maps: it could be injective, surjective,
and so on.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> A bijective homomorphism %f: %A \to %B is called an <i>isomorphism</i>.
</div>

<a name="5.3"></a>
<h3><span class="secn">5.3.</span> Linear transformations</h3>

In this course we are interested in a specific kind of homomorphism.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> For any two vector spaces %V and %W, a map %f: %V \to %W is a <i>linear transformation</i> iff we have that for all %v,%v' \in V and
scalars %s \in \R,

\begin{eqnarray}
 %f(%v + %v') & = & %f(%v) + %f(%v') \\
 %f(%s \cdot %v) & = & %s \cdot %f(%v)
\end{eqnarray}

In other words, a linear transformation is a homomorphism between vector spaces that preserves vector addition and scalar multiplication.
If a map %f does not satisfy the above properties, it is <i>not</i> a linear transformation.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> For any matrix %M \in \R^{n \times m}, the map %f : \R^{%m} \rightarrow \R^{%n} defined as %f(%v) = %M \cdot %v is a linear transformation.

We can confirm that any such %f satisfies the properties of a linear transformation:
\begin{eqnarray}
 %f(%v + %v') & = & %M \cdot (%v + %v') \\
              & = & %M \cdot %v + %M \cdot %v' \\
              & = & %f(%v) + %f(%v') \\
 %f(%s \cdot %v) & = & %M \cdot (%s \cdot %v) \\
              & = & %s \cdot (%M \cdot %v) \\
              & = & %s \cdot %f(%v)
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Representation of a polynomial curve %f as a vector of its %y values on some fixed number of inputs %x_1,...,%x_%n is a linear transformation.

Suppose we have the transformation \phi defined as follows:
\begin{eqnarray}
 \phi(%f) & = & #[ %f(%x_1) #; \vdots #; %f(%x_%n) #]
\end{eqnarray}
We can confirm that \phi is a linear transformation. Suppose that %h(%x) = %f(%x) + %g(%x). Then we have:
\begin{eqnarray}
 \phi(%f + %g) & = & \phi(%h) \\
              & = & #[ %h(%x_1) #; \vdots #; %h(%x_%n)  #] \\
              & = & #[ %f(%x_1) + %g(%x_1) #; \vdots #; %f(%x_%n) + %g(%x_%n)  #] \\
              & = & #[ %f(%x_1) #; \vdots #; %f(%x_%n) #] + #[ %g(%x_1) #; \vdots #; %g(%x_%n) #] \\
              & = &  \phi(%f) + \phi(%g)
\end{eqnarray}
Suppose that %h(%x) = %s \cdot %f(%x). Then we have:
\begin{eqnarray}
 \phi(%s \cdot %f) & = & \phi(%h) \\
              & = & #[ %h(%x_1) #; \vdots #; %h(%x_%n)  #] \\
              & = & #[ %s \cdot %f(%x_1) #; \vdots #; %s \cdot %f(%x_%n) #] \\
              & = & %s \cdot #[ %f(%x_1) #; \vdots #; %f(%x_%n) #] \\
              & = & %s \cdot \phi(%f)
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any two linear transformations %f: %V \to %W, %g: %U \to %V, the composition %f \circ %g: %U \to %W is also a linear transformation.
To show this, we must demonstrate that if %h(%x) = %f(%g(%x)) then %h respects vector addition and scalar multiplication. For any %u,%u' \in %U, and
%s \in \R, we have:
\begin{eqnarray}
 %h(%u + %u') & = & %f(%g(%u + %u')) \\
              & = & %f(%g(%u) + %g(%u')) \\
              & = & %f(%g(%u)) + %f(%g(%u')) \\
              & = & %h(%u) + %h(%u')
\end{eqnarray}
and
\begin{eqnarray}
 %h(%s \cdot %u) & = &  %f(%g(%s \cdot %u)) \\
              & = &  %f(%s \cdot %g(%u)) \\
              & = & %s \cdot  %f(%g(%u)) \\
              & = & %s \cdot %h(%u)
\end{eqnarray}
Because linear transformations are homomorphisms (and, thus, maps), we can ask whether they have properties of maps (i.e., whether they are injective,
surjective, bijective, and so on). We will do so further below.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Let the map \phi : \R^3 \to {%f | %f(%x) = %a %x^2 + %b %x + %c} be defined as follows:
\begin{eqnarray}
 \phi( #[%a #; %b #; %c#] ) & = & %h \~ where %h(%x) = %a %x^2 + %b %x + %c
\end{eqnarray}
It is the case that \phi is a linear transformation and a bijection. Thus, \phi is an isomorphism between \R^3 and
{%f | %f(%x) = %a %x^2 + %b %x + %c}.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Differentiation (i.e., finding the derivative) of polynomials is a linear transformation.

As an example, consider the space of polynomials of the form %f(%x) = %a %x^2 + %b %x + %c. If each polynomial is represented as a vector
of its coefficients, the differentiation operator for this vector space of functions can be represented as a matrix:
\begin{eqnarray}
  #[ 0 #, 0 #, 0 #; 2 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot #[ %a #; %b #; %c #] & = &  #[ 0 #; 2 %a #; %b #]
\end{eqnarray}
Because there exists an isomorphism \phi between {%f | %f(%x) = %a %x^2 + %b %x + %c} and \R^3, we can compose \phi and \phi^{-1} with
the following linear transformation \psi : \R^3 \to \R^3
\begin{eqnarray}
  \psi(%v) & = & #[ 0 #, 0 #, 0 #; 2 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot %v
\end{eqnarray}
Then, the following differential operator d/d%x : {%f | %f(%x) = %a %x^2 + %b %x + %c} \to {%f | %f(%x) = %a %x^2 + %b %x + %c} is a linear transformation because composition of linear transformations are linear transformations:
\begin{eqnarray}
  d/d%x & = & \phi \circ \psi \circ \phi^{-1}
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Orthogonal projection onto a vector space is a linear transformation.

We know that any vector space has an orthonormal basis %B. Thus, we can compute the projection of any vector %v using the formula %M_B \cdot %M_B^\top \cdot %v.
Thus, orthogonal projection onto \span %B can be defined as a linear transformation %f where:
\begin{eqnarray}
  %f(%v) & = & (%M_B \cdot %M_B^\top) \cdot %v
\end{eqnarray}
Since (%M_B \cdot %M_B^\top) is a matrix, %f must be a linear transformation.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any linear transformation %f:%V \to %W and appropriate constants \0 \in %V and \0' \in %W,

  $$ %f(\0) = \0' $$

We can show this in the following way: given any %v \in %V,
\begin{eqnarray}
  \0 & = & 0 \cdot %v \\
 %f(\0) & = & %f(0 \cdot %v) \\
        & = & 0 \cdot %f(%v) \\
        & = & \0'
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any linear transformation %f:%V \to %W and the corresponding additive inversion operations of
%V and %W, it is the case that %f respects additive inversion:

  $$ %f(-%v) = -%f(%v) $$

We can derive this fact in two ways. One approach is to use the previous fact that %f(\0) = \0' to show that %f(-%v) is indeed the inverse of %f(%v):

\begin{eqnarray}
  %f(%v) + %f(-%v) & = & %f(%v + (-%v)) \\
                   & = & %f(\0) \\
                   & = & \0'
\end{eqnarray}

The other approach is to use the fact that in both vector spaces,
%v = -1 \cdot %v:

\begin{eqnarray}
  %v & = & 1 \cdot %v \\
 %v + (-1 \cdot %v) & = & (1 \cdot %v) + ((-1) \cdot %v) \\
            & = & (1 + (-1)) \cdot %v \\
            & = & 0 \cdot %v \\
            & = & \0
\end{eqnarray}

Then, because %f respects scalar multiplication, we have that: 

\begin{eqnarray}
  %f(-1 \cdot %v) & = & -1 \cdot %f(%v)
\end{eqnarray}

Thus, the argument is:

\begin{eqnarray}
 %f(%v) + %f(-%v) & = & %f(%v) + %f(-1 \cdot %v) \\
                  & = & %f(%v) + ((-1) \cdot %f(%v)) \\
                  & = & %f(%v) + (-%f(%v)) \\
                  & = & \0
\end{eqnarray}
</div>

<a name="lecture20"></a>

<a name="5.4"></a>
<h3><span class="secn">5.4.</span> Orthogonal projections as linear transformations</h3>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For %v \in \R^n, an orthogonal projection onto a one-dimensional vector space \span{%w} is a linear transformation. First,
note that the dot product of two vectors can be rewritten as matrix multiplication of two matrices:

  $$ %w \cdot %v = %w^\top \cdot %v.$$

Also, notice that

\begin{eqnarray}
   ||%w|| & = & \sqrt(%w \cdot %w) \\
   ||%w||^2 & = & %w \cdot %w
\end{eqnarray}

Finally, notice that for nonzero %r \in \R where %r \neq 0, the 1 \times 1 matrix #[%r#] \in \R<sup>1 \times 1</sup> is invertible and has inverse #[1/%r#].

Consider the formula for the projection of %w \in \R^n onto \span(%v). We can rewrite the formula to use only multiplication of matrices.

\begin{eqnarray}
(%v \cdot %w/||%w||) \cdot %w/||%w|| & = & 1/||%w||^2 \cdot (%v \cdot %w) \cdot %w \\
                                     & = & %w \cdot 1/||%w||^2 \cdot (%v \cdot %w)\\
                                     & = & %w \cdot 1/(%w \cdot %w) \cdot (%w \cdot %v) \\
                                     & = & %w \cdot (%w^\top \cdot %w)^{-1} \cdot (%w^\top \cdot %v) \\
                                     & = & (%w \cdot (%w^\top \cdot %w)^{-1} \cdot %w^\top) \cdot %v
\end{eqnarray}

Thus, we have a product of the matrices %w \in \R^{n \times 1}, (%w^\top \cdot %w)^{-1} \in \R^{1 \times 1}, and %w^\top \in \R^{1 \times n}.
This product is a matrix in \R^{n \times n}. Call it %M<sub>\span{w}</sub>. Then we can define the linear transformation %f \in \R^n \to \R^n 
that performs the orthogonal projection onto \span{w} as:

  $$%f(%v) = %M<sub>\span{w}</sub> \cdot %v.$$

We used the above approach because we will see later that it can be generalized to orthogonal projections onto multidimensional vector spaces.
An alternative way to show the above is to notice that %w/||%w|| can be rewritten as a matrix %u \in \R^{n \times 1}. In that case, we have:

\begin{eqnarray}
  (%v \cdot %w/||%w||) \cdot %w/||%w|| & = & (%v \cdot %u) \cdot %u \\
                                       & = & %u \cdot (%u \cdot %v) \\
                                       & = & (%u \cdot %u^\top) \cdot %v \\
\end{eqnarray}

Here, %u \cdot %u^\top \in \R^{n \times n}.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we want to compute the projection of %v onto %w where:
\begin{eqnarray}
  %v & = & #[ 1 #; -2 #] \\
  %w & = & #[ 3 #; 4 #] 
\end{eqnarray}
We can compute the projection as follows:
\begin{eqnarray}
 || #[ 3 #; 4 #] || & = & 5 \\
(%v \cdot %w/||%w||) \cdot %w/||%w|| & = &  (#[ 1 #; -2 #] \cdot (#[ 3 #; 4 #] \cdot (1/5))) \cdot (#[ 3 #; 4 #] \cdot (1/5)) \\
                                     & = & #[ 3 #; 4 #] \cdot (1/25) \cdot (#[ 1 #; -2 #] \cdot #[ 3 #; 4 #])\\
                                     & = & #[ 3 #; 4 #] \cdot (#[ 3 #; 4 #] \cdot #[ 3 #; 4 #])^{-1} \cdot (#[ 3 #; 4 #] \cdot #[ 1 #; -2 #]) \\
                                     & = & (#[ 3 #; 4 #] \cdot (#[ 3 #, 4 #] \cdot #[ 3 #; 4 #])^{-1} \cdot #[ 3 #; 4 #]) \cdot #[ 1 #; -2 #] \\
                                     & = & (#[ 3 #; 4 #] \cdot #[ 25 #]^{-1} \cdot #[ 3 #; 4 #]) \cdot #[ 1 #; -2 #]\\
                                     & = & (#[ 3 #; 4 #] \cdot #[ 1/25 #] \cdot #[ 3 #; 4 #]) \cdot #[ 1 #; -2 #]\\
                                     & = & (#[ 3/25 #; 4/25 #] \cdot #[ 3 #; 4 #]) \cdot #[ 1 #; -2 #]\\
                                     & = & #[ 9/25 #, 12/25 #; 12/25 #, 16/25 #] \cdot #[ 1 #; -2 #]
\end{eqnarray}
Notice that the columns of the matrix above are in span {%w}:
\begin{eqnarray}
  #[ 9/25 #; 12/25 #] & = & (3/25) \cdot #[ 3 #; 4 #] \\
  #[ 9/25 #; 12/25 #] & \in & \span {#[ 3 #; 4 #]} \\
  #[ 12/25 #; 16/25 #] & = & (3/25) \cdot #[ 3 #; 4 #] \\
  #[ 12/25 #; 16/25 #] & \in & \span {#[ 3 #; 4 #]}
\end{eqnarray}
Thus, we have that:
\begin{eqnarray}
  \span {#[ 9/25 #; 12/25 #], #[ 12/25 #; 16/25 #]} = \span {#[ 3 #; 4 #]}
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose that for %M \in \R^{n \times m} and %f \in \R^m \to \R^n we have an overdetermined system:

  $$ %M \cdot %v = %w$$

If %M^\top \cdot %M is invertible (and, thus, (%M^\top \cdot %M)^{-1} is defined), we can say that:
\begin{eqnarray}
 %M \cdot %v & = & %w \\
 %M^\top \cdot (%M \cdot %v) & = & %M^\top \cdot %w \\
 (%M^\top \cdot %M) \cdot %v & = & %M^\top \cdot %w \\
 (%M^\top \cdot %M)^{-1} \cdot (%M^\top \cdot %M) \cdot %v & = & (%M^\top \cdot %M)^{-1} %M^\top \cdot %w \\
 %v & = & (%M^\top \cdot %M)^{-1} %M^\top \cdot %w\\
 %M \cdot %v & = & %M \cdot (%M^\top \cdot %M)^{-1} %M^\top \cdot %w.
\end{eqnarray}
Thus, even if %M \cdot %v = %w is overdetermined, %M^\top \cdot (%M \cdot %v) = %M^\top \cdot %w has a solution %v^\ast such that %M \cdot %v^\ast corresponds
to the orthogonal projection of %v onto the span of the column vectors of %M.

Thus, for any matrix %M where %M^\top \cdot %M is invertible, we can define the linear transformation that represents orthogonal projection onto
the span of the columns of %M:
\begin{eqnarray}
 %f(%v) & = & %M \cdot (%M^\top \cdot %M)^{-1} %M^\top \cdot %v.
\end{eqnarray}
</div>


<!--assignment5-->

<br/><hr/>
<a name="5.5"></a>
<a name="assignment5"></a>
<h3><span class="secn">5.5.</span>
  <b>Assignment #5: Approximations and Linear Transformations</b> <!--span class="btn_assignment">(<a href="materials.php?hw=5">show only this assignment</a>)</span-->
</h3>

<only142>
  <p>In this assignment you will solve problems involving overdetermined systems and linear transformations.</p>

  <p><b style="color:firebrick;">You are only required to choose and solve any four (4) of the problems below. 
  Solutions to any additional problems will count as extra credit.</b></p>

  <b>Please submit a single file <code>a5.*</code> containing your solutions. The file extension may be anything you choose;
  please ask before submitting a file in an exotic or obscure file format (plain text is preferred).</b>
</only142>
<only132>
  <p>In this assignment you will solve problems involving overdetermined systems and linear transformations.</p>

  <b>Please submit a single file <code>a5.*</code> containing your solutions. The file extension may be anything you choose;
  please ask before submitting a file in an exotic or obscure file format (plain text is preferred).</b>
</only132>

<ol>
  <li> For each of the linear transformations %f defined below, determine the following.
    <ul>
      <li>What are the domain and codomain?</li>
      <li>Is it injective? If it is, provide a step-by-step argument. If not, provide a counterexample.</li>
      <li>Is it surjective? If it is, provide a step-by-step argument. If not, provide a counterexample.</li>
      <li>Is it bijective? Explain why or why not.</li>
      <li>What is \dim(\im(%f))? Your answer must be an integer.</li>
    </ul>

    <ol style="list-style-type:lower-alpha;">
      <li> The linear transformation %f : \R^3 \to \R^2 where:
\begin{eqnarray}
 %f(%v) & = & #[ 1 #, 2 #, 0 #; 2 #, 4 #, 1 #] \cdot %v
\end{eqnarray}
      </li>

      <li> The linear transformation %f : \R^2 \to \R^2 where:
\begin{eqnarray}
 %f(%v) & = & #[ 1 #, 0 #; 0 #, 0 #] \cdot %v
\end{eqnarray}
      </li>

      <li> The linear transformation %f : \R^5 \to \R<sup>1</sup> where:
\begin{eqnarray}
 %f(%v) & = & #[ 0 #, 0 #, 0 #, 1 #, 2 #] \cdot %v
\end{eqnarray}
      </li>

      <li> The linear transformation %f : \R^2 \to \R^2 where:
\begin{eqnarray}
 %f(%v) & = & #[ 0 #, %a #; %a #, 0 #] \cdot %v \~ where %a \neq 0
\end{eqnarray}
      </li>
    </ol>
  </li>
  <li> In this problem, you will practice finding the least-squares approximate solutions to overdetermined systems.

    <ol style="list-style-type:lower-alpha;">
      <li> Find the least-squarse approximate solution to the following equation, and compute the error of that solution:
\begin{eqnarray}
 #[ 1 #, -2 #; -2 #, 4 #] \cdot #[ %x #; %y #] & = & #[ 1 #; 5 #]
\end{eqnarray}
      </li>

      <li> Suppose you are given the following polynomial:
\begin{eqnarray}
 %f(%x) & = & %x^2 - 2 %x + 6
\end{eqnarray}
           Find the best-fit curve in the space {%f | %f(%x) = %a %x + %b, %a,%b \in \R} for the above polynomial using the
           %x values {-1, 0, 2}.
      </li>

      <li> Suppose you are given the following data points:
\begin{eqnarray}
 #[ 1 #; 2 #], #[ 0 #; 1 #] , #[ -1 #; 0 #], #[ -2 #; 3 #]
\end{eqnarray}
           Find the best-fit curve in the space {%f | %f(%x) = %a %x^2 + %b %x + %c, %a,%b,%c \in \R} for these data points.
      </li>
    </ol>
  </li>

  <li> Two engineers are trying to find the best-fit curve in the space { %f | %f(%x) = %a %x + %b, %a,%b \in \R } for a collection
       of 100 data points of the form (%x_%i, %y_%i) for %i \in {1,...,100}, but that collection is split across two databases of 50
       points each. Each engineer has access
       to only one of the databases. Symbolically, the overdetermined system they are trying to solve is represented using
       the following equation:
\begin{eqnarray}
 #[ %x_1 #, 1 #; \vdots #, \vdots #; %x<sub>100</sub> #, 1 #] \cdot #[ %a #; %b #] & = & #[ %y_1 #; \vdots #; %y<sub>100</sub> #]
\end{eqnarray}
       Suppose the two column vectors [%x_1; ...; %x<sub>100</sub>] and [1; ...; 1] of the matrix are orthogonal to one another (but not necessarily unit vectors). Then,
       the first thing they need to do is project the vector of [%y_1; ...; %y<sub>100</sub>] onto the two column vectors of the matrix. Each engineer
       computes the following values using the data available to them:
\begin{eqnarray}
 ||#[%x_1 #; \vdots #; %x<sub>50</sub> #] || & = & 3 & \~ & &nbsp;&nbsp;&nbsp;
 %x_1 \cdot %y_1 + ... + %x<sub>50</sub> \cdot %y<sub>50</sub> & = & 62 & \~ & &nbsp;&nbsp;&nbsp;
  %y_1 + ... + %y<sub>50</sub> & = & 120 \\
 || #[ %x<sub>51</sub> #; \vdots #; %x<sub>100</sub> #] || & = & 4 & \~ & &nbsp;&nbsp;&nbsp;
 %x<sub>51</sub> \cdot %y<sub>51</sub> + ... + %x<sub>100</sub> \cdot %y<sub>100</sub> & = & 38 & \~ & &nbsp;&nbsp;&nbsp;
  %y<sub>51</sub> + ... + %y<sub>100</sub> & = & 80 
\end{eqnarray}
    <ol style="list-style-type:lower-alpha;">
      <li> Once the engineers compute the projection of [%y_1; ...; %y<sub>100</sub>] onto span of the vector [%x_1; ...; %x<sub>100</sub>],
           they collectively obtain some vector of the form:
\begin{eqnarray}
 %s \cdot #[%x_1#; \vdots #; %x<sub>100</sub>#] & = & orthogonal projection of #[%y_1#; \vdots #; %y<sub>100</sub>#] onto #[%x_1#; \vdots #; %x<sub>100</sub>#]
\end{eqnarray}
           for some %s. What is %s? Your answer must be an integer.
      </li>

      <li> Once the engineers compute the projection of [%y_1; ...; %y<sub>100</sub>] onto span of the vector [1; ...; 1],
           they collectively obtain some vector of the form:
\begin{eqnarray}
 %t \cdot #[1#; \vdots #; 1#] & = & orthogonal projection of #[%y_1#; \vdots #; %y<sub>100</sub>#] onto #[1#; \vdots #; 1#]
\end{eqnarray}
           for some %t. What is %t? Your answer must be an integer.
      </li>

      <li> What are the coefficients %a and %b of the best-fit line %f(%x) = %a %x + %b for the data? Your answers should be integers.
      </li>
    </ol>
  </li>

  <only132>
  <li> In this course, we learned how to find a solution to an overdetermined system such that the solution has the
       smallest possible error. However, sometimes a system is <i>underdetermined</i>, and we want to find the solution
       with minimal <i>cost</i> for some definition of cost.
       In this problem, you will devise a method for doing so using some of the techniques we have used to solve overdetermined systems.
       
Consider the following underdetermined system:
\begin{eqnarray}
 #[ 1 #, 3 #; 2 #, 6 #] \cdot #[ %x #; %y #] & = & #[ 10 #; 20 #]
\end{eqnarray}
    <ol style="list-style-type:lower-alpha;">
      <li> Define the solution space to the above system; you must use set notation. We will call this space %S.
      </li>

      <li> What is <i>shortest</i> vector in %S? Recall that the length of a vector is its distance from the
           origin. How can you use orthogonal projection to determine which vector in %S is the shortest?
      </li>

      <li> Suppose that the cost of a solution vector %v to the above system is defined to be ||%v||. What is the solution
           to the above system that has the lowest cost?
      </li>
    </ol>
  </li>
  </only132>
  <only142>
  <li>
Consider the following underdetermined system:
\begin{eqnarray}
 #[ 1 #, 3 #; 2 #, 6 #] \cdot #[ %x #; %y #] & = & #[ 10 #; 20 #]
\end{eqnarray}
    <ol style="list-style-type:lower-alpha;">
      <li> Define the solution space to the above system; you must use set notation. We will call this space %W.
      </li>
      <li> Define an isomorphism between %W and a vector space %V. You must provide a definition of both the vector space and the isomorphism.
      </li>
      <li> Suppose that the cost of a solution vector %v to the above system is defined to be ||%p --- %v|| where
\begin{eqnarray}
 %p & = & #[ 2 #; 3 #]
\end{eqnarray}
           What is the solution to the above system that has the lowest cost?
      </li>
    </ol>
  </li>
  </only142>

    <li> Suppose you are assembling a processing plant that generates electricity and <b>produces</b> biofuel as a byproduct. 
       You can purchase any number of each of the
       following two types of generators:
       <ul>
         <li>generators of type A produce 4 units of electricity for every 1 unit of biofuel produced;
         <li>generators of type B produce 8 units of electricity for every 2 units of biofuel produced.
       </ul>
       Furthermore, in order for the generators to work, each generator must be connected to every other generator
       of the same type as well as a central controller, and connections cost $4. 
       As a result, the connection cost for each kind of generator is computed as follows:
       <ul>
         <li>when you buy %x generators of type A, each generator of type A costs 4 \cdot %x dollars;
         <li>when you buy %y generators of type B, each generator of type B costs 4 \cdot %y dollars.
       </ul>
       If you must generate exactly 40 units of electricity and 10 units of biofuel while paying the least connection cost possible,
       how many of each kind of generator should you purchase?
  </li>

  <only142>
  <li> Suppose that vectors in \R^2 represent population quantities in two locations, the linear transformation %f:\R^2 \rightarrow \R^2 
   represents a change in the quantities over the course of one year if the economy is doing well, %g:\R^2 \rightarrow \R^2 represents
  a change in the quantities over the course of one year if the economy is not doing well, and
  %v_0 is the initial state:
\begin{eqnarray}
 %f(%v) & = & #[ 2 #, 1 #; 4 #, 2 #] \cdot %v\\
 %g(%v) & = & #[ 0.1 #, 0.2 #; 0.4 #, 0.3 #] \cdot %v\\
 %v_0   & = & #[ 100 #; 200 #]
\end{eqnarray}
  The population state is initially %v_0; after 302 years have passed, the total number of years with a good economy was 100
  and the total number of years with a poor economy was 202. What is the state of the population at this point?
  </li>
  </only142>

</ol>
<hr/><br/>

<!--/assignment5-->


<a name="5.6"></a>
<h3><span class="secn">5.6.</span> Matrices as linear transformations</h3>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose we have the following matrix %M \in \R^{m \times n} and the corresponding linear transformation %f : \R^{%n} \to \R^{%m}:
\begin{eqnarray}
  %M & = & #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #] \\
  %f(%v) & = & %M \cdot %v
\end{eqnarray}
Then the following is true:
\begin{eqnarray}
  \span {%c_1, ..., %c_%n} & = & \im(%f)
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose we have an invertible matrix %M \in \R^{n \times n} and the corresponding linear transformation %f : \R^{%n} \to \R^{%n}:
\begin{eqnarray}
  %f(%v) & = & %M \cdot %v
\end{eqnarray}
Then %f is a bijection. Because %f is also a linear transformation, %f is an isomorphism.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following linear transformation %f: \R \to \R^2:
\begin{eqnarray}
  %f(%x) & = & #[ %x #; %x #]
\end{eqnarray}
The linear transformation %f is injective but not surjective.

We can show %f is injective:
\begin{eqnarray}
  %x & \neq & %x' \\
  #[ %x #; %x #] & \neq & #[ %x' #; %x' #] \\
  %f(%x) & \neq & %f(%x')
\end{eqnarray}
To show it is not surjective, we choose the following vector %w \in \R^2 (i.e., in the codomain). The equation is then not solvable,
so %f is not surjective.
\begin{eqnarray}
  %w & = & #[ 1 #; 0 #] \\
  %f(%x) & = & #[ 1 #; 0 #] \\
  #[ %x #; %x #] & = &  #[ 1 #; 0 #] \\
  %x & = & 1 \\
  %x & = & 0 \\
  1 & = & 0
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following linear transformation %f: \R^2 \to \span{ [3;4] }:
\begin{eqnarray}
  %f(#[ %x #; %y #]) & = & #[ 3 #, 6 #; 4 #, 8 #] \cdot #[ %x #; %y #]
\end{eqnarray}
The linear transformation %f is surjective but not injective.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following linear transformation %f: \R^2 \to \span{ [3;4] }:
\begin{eqnarray}
  %f(#[ %x #; %y #; %z #]) & = & #[ 0 #, 1 #, 1 #; 1 #, 0 #, 1 #] \cdot #[ %x #; %y #; %z #]
\end{eqnarray}
The linear transformation %f is surjective but not injective.
</div>

<a name="lecture21"></a>

<a name="5.7"></a>
<h3><span class="secn">5.7.</span> Application: communication</h3>

We want to transmit information, but we have constraints (e.g., security or high cost of transmission).
Suppose we have two linear transformations %f : %V \rightarrow %W and %f^{-1} : %W \rightarrow %V
such that %f^{-1} \circ %f is a bijection (and, thus, an isomorphism). If \im(%f) has some desirable property that the domain of %f does not
possess (e.g., it is obscured, or it has a smaller representation size), we can use %f as an encoding function for the information
we want to transmit, and %f^{-1} as a decoding function.

<div class="mathenv example_to_know">
<b>Example:</b> Suppose Alice wants to transmit the following vectors to Bob:
\begin{eqnarray}
 { #[ -3 #; 6 #], #[ 2 #; -4 #], #[ 1 #; -2 #], #[ 0 #; 0 #], #[ -5 #; 10 #] }
\end{eqnarray}
If she simply transmitted the individual real numbers to Bob one-by-one, she would need to transmit 10 real numbers.
Can Alice and Bob agree on some protocol that would allow Alice to send fewer real numbers but transmit the same amount of information in
this case?

All the vectors in the set are in \R^2. However, Alice and Bob can take advantage of the fact that the vectors are in a subspace of \R^2. In
particular, all five vectors are in:
\begin{eqnarray}
 \span { #[ 1 #; -2 #] } & \subset & \R^2
\end{eqnarray}

Suppose Alice and Bob define the following encoding and decoding linear transformations:
\begin{eqnarray}
 %f : \span { #[ 1 #; -2 #] } \rightarrow \R \\
 %f^{-1} : \R \rightarrow \span { #[ 1 #; -2 #] }
\end{eqnarray}
\begin{eqnarray}
 %f( #[ x #; y #] ) & = & %x \\
 %f^{-1}(%s) & = & %s \cdot #[ 1 #; -2 #]
\end{eqnarray}
Then %f^{-1} \circ %f is invertible, and Alice can use %f to encode vectors that contain two real numbers within a single real number. This
means it is sufficient for Alice to send just five real numbers in order to send Bob the five vectors:
\begin{eqnarray}
 { %f(#[ -3 #; 6 #]), %f(#[ 2 #; -4 #]), %f(#[ 1 #; -2 #]), %f(#[ 0 #; 0 #]), %f(#[ -5 #; 10 #]) } & = & {-3, 2, 1, 0, -5}
\end{eqnarray}
</div>

<a name="5.8"></a>
<h3><span class="secn">5.8.</span> Affine spaces and affine transformations</h3>

We are interested in studying solution spaces of systems of the form %M \cdot %v = %w. However, these are not vector spaces because they do not always contain \0 (i.e., the origin).
Recall that if a solution space of a system does contain \0, the solution space is a vector space and %M \cdot %v = %w must be a homogenous system where %w = \0.

<div class="mathenv proposition_to_know">
<b>Definition:</b> Suppose %W is a set of vectors such that \0 \not\in %W, and suppose %w_0 is the orthogonal projection of \0 onto %W. Then we can define
%W to be a vector space with the following vector addition and scalar multiplication operations:

<ul>
 <li>addition (\oplus) can be defined as follows. It is an operation on elements of %W under which %W is closed, and which satisfies the vector space axioms:

  $$%v \oplus %v' = %u \~ where \~ %u = (%v - %w_0) + (%v' - %w_0) + %w_0 = %v + %v' - 2 \cdot %w_0 + %w_0 = %v + %v' - %w_0.$$

 </li>
 <li>scalar multiplication (\otimes) can be defined as follows. It is an operation on elements of %A under which %A is closed, 
     and which satisfies the vector space axioms:

  $$%s \otimes %v = %u \~ where \~ %u = (%s \cdot (%v - %w_0)) + %w_0 = %s %v - %s %w_0  + %w_0 = %s %v + (1-%s) \cdot %w_0.$$

 </li>
 <li>there is a unique additive identity in %W; it is the vector %w_0:

  $$%v \oplus %w_0 = %v + %w_0 - %w_0 = %v.$$

 </li>
</ul>
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If %V is a vector space with addition operation + and scalar multiplication operation \cdot,
the vector space %W = {%v + %w_0 | %v \in %V} with addition operation \oplus and scalar multiplication operation \otimes is
isomorphic to %V, with isomorphism %f:%V \rightarrow %W defined as:
\begin{eqnarray}
 %f(%v) & = & %v + %w_0
\end{eqnarray}
We can see that %f is a linear transformation because it has the properties of a linear transformation:
\begin{eqnarray}
 %f(%v + %v') & = & %v + %v' + %w_0 \\
              & = & %v + %w_0 + %v'\\
              & = & %v + %w_0 + %v' + %w_0 - %w_0 \\
              & = & (%v + %w_0) + (%v' + %w_0) - %w_0 \\
              & = & %f(%v) \oplus %f(%v') \\
 %f(s \cdot %v) & = & %s \cdot %v + %w_0 \\
              & = & %s \cdot %v + %w_0 + (%s \cdot %w_0 - %s \cdot %w_0) \\
              & = & %s \cdot %v + %s \cdot %w_0 + %w_0 - %s \cdot %w_0 \\
              & = & %s \cdot (%v + %w_0) + (1-%s) \cdot %w_0 \\
              & = & %s \otimes %f(%v)
\end{eqnarray}
We can see it is injective:
\begin{eqnarray}
 %v & \neq & %v'\\
 %v+%w_0 & \neq & %v'+%w_0\\
 %f(%v) & \neq & %f(%v')
\end{eqnarray}
We can see it is surjective because for any %w' \in %W:
\begin{eqnarray}
 %f(%v) & = & %w'\\
 %v+%w_0 & = & %w'\\
 %v & = & %w' - %w_0
\end{eqnarray}
Since %f is injective, surjective, and a linear transformation, it is an isomorphism. Thus, %V and %W are isomorphic. This means we can work with %W as if it is a vector space
by simply mapping back to %V using the isomorphism, doing any necessary work, and then mapping back to %W by using the inverse of that isomorphism, %f^{-1}.
</div>

<a name="lecture22a"></a>

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following affine space:
\begin{eqnarray}
 %W & = & {#[%x #; %y#] | %y = 2 %x + 3}
\end{eqnarray}
Suppose we want to find the orthogonal projection of the following point %p \in \R onto the affine space %W:
\begin{eqnarray}
 p & = & #[4#;3#]
\end{eqnarray}
To do so, we first need to find the isomorphism between a vector space %V and the space %W. We will choose the vector space %V that is parallel to %W but that goes through
the origin; it is defined as:
\begin{eqnarray}
 %V & = & {#[%x #; %y#] | %y = 2 %x }
\end{eqnarray}
Notice that %V is a line with the same slope as %W. Next, we must the additive identity %w_0 of %W by finding the intersection of %V^\bot and %W, where %V^\bot is the orthogonal
complement of %V. We first compute %V^\bot:
\begin{eqnarray}
 %V & = & \span { #[ 1 #; 2 #] } \\
 %V^\top & = & { %u | %u \cdot %v = 0 for all %v \in %V} \\
         & = & { %u | %u \cdot %v = 0 for all %v \in basis %V} \\
         & = & { %u | %u \cdot %v = 0 for all %v \in {#[1 #; 2#]}} \\
         & = & { %u | %u \cdot #[1 #; 2#] = 0 } \\
         & = & { #[%x #; %y #] | #[%x #; %y #] \cdot #[1 #; 2#] = 0 } \\
         & = & { #[%x #; %y #] | %x + 2 \cdot %y = 0 } \\
         & = & { #[%x #; %y #] | %y = -(1/2) %x }
\end{eqnarray}
We can now compute the intersection and, thus, %w_0:
\begin{eqnarray}
 {%w_0} & = & %W \cap %V^\bot \\
      & = & { #[%x #; %y#] | %y = 2 %x + 3} \cap { #[%x #; %y #] | %y = -(1/2) %x } \\
      & = & { #[%x #; %y#] | %y = 2 %x + 3 and %y = -(1/2) %x} \\
      & = & { #[---6/5 #; 3/5 #] } \\
   %w_0 & = &  #[---6/5 #; 3/5 #] 
\end{eqnarray}
We can now define an isomorphism %f:%V \rightarrow %W:
\begin{eqnarray}
 %f(%v) & = & %v + %w_0 \\
        & = & %v + #[---6/5 #; 3/5 #]
\end{eqnarray}
Next, to project the point %p onto %W, it is now sufficient to first project the point onto %V, then apply %f:
\begin{eqnarray}
 %V & = & \span { #[1 #; 2#] } \\
 ||#[1 #; 2#]|| & = & \sqrt(5)
\end{eqnarray}
\begin{eqnarray}
 orthogonal projection of #[4#;3#] onto \span { #[1 #; 2#] } & = & (#[4#;3#] \cdot (1/\sqrt(5)) #[1 #; 2#]) \cdot (1/\sqrt(5)) #[1 #; 2#] \\
                                                             & = & (10/\sqrt(5)) \cdot (1/\sqrt(5)) #[1 #; 2#] \\
                                                             & = & 2 \cdot  #[1 #; 2#] \\
                                                             & = &  #[2 #; 4#]
\end{eqnarray}
Finally, the orthogonal projection of %p onto %W can be computed via %f:
\begin{eqnarray}
 orthogonal projection of #[4#;3#] onto %W & = & %f(#[2 #; 4#]) \\
                                           & = & #[2 #; 4#] + #[---6/5 #; 3/5 #] \\
                                           & = & #[4/5 #; 23/5#]
\end{eqnarray}
</div>

How do we find an isomorphism between an affine space and a parallel vector space?

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Suppose that for a matrix %M \in \R^{n \times n}, %M \cdot %u = %w 
is a non-homogenous system with at least one solution. Let %U be the solution space of %M \cdot %u = %w:
\begin{eqnarray}
 %U & = & {%u | %M \cdot %u = %w }
\end{eqnarray}
How can we find an isomorphism between %U and some vector space %V? We let %V be the corresponding homogenous system:
\begin{eqnarray}
 %V & = & {%v | %M \cdot %v = #[0 #; \vdots #; 0#] }
\end{eqnarray}
We must now find %V^\bot; we can do so by taking the transpose of %M and setting up a new homogenous system:
\begin{eqnarray}
 %V^\bot & = & {%M^\top \cdot %v | %v \in \R^{%n} }
\end{eqnarray}
To find %u_0, it is now sufficient to find the intersection of %U and %V^\bot:
\begin{eqnarray}
 {%u_0} & = & %U \cap %V^\bot
\end{eqnarray}
The isomorphism %f:%V \rightarrow %U is then:
\begin{eqnarray}
 %f(%v) & = & %v + %u_0
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we have a non-homogenous system %M \cdot %v = %w where %M \in \R^{n \times n} and we want to compute the projection of some %p \in \R^n onto the affine
space {%v | %M \cdot %v = %w}. How can we do so?

Suppose that %M is of the following form:
\begin{eqnarray}
  %M & = & #[ \uparrow #, #, \uparrow #; %c_1 #, \ldots #, %c_%n #; \downarrow #, #, \downarrow #] 
     & = & #[ \leftarrow #, %r_1 #, \rightarrow #; #, \vdots #,  #; \leftarrow #, %r_%n #, \rightarrow #]
\end{eqnarray}
Then we can say that we want to project %p onto {%v | %M \cdot %v = \0} and find an isomorphism %f: {%v | %M \cdot %v = \0} \rightarrow {%v | %M \cdot %v = %w}; the
orthogonal projection is then %f(%p).

Thus, we find an orthonormal basis of {%v | %M \cdot %v = \0} and compute the orthogonal projection of %p onto {%v | %M \cdot %v = \0}; call this %p^\ast:
\begin{eqnarray}
  %p^\ast & = & orthogonal projection of %p onto {%v | %M \cdot %v = \0}
\end{eqnarray}
We then find %f by solving the following system of equations for %u_0:
\begin{eqnarray}
  %M \cdot %u_0 & = & %w \\
  %M^\top \cdot %v & = & %u_0
\end{eqnarray}
We solve the above by first solving for %v and then computing %u_0:
\begin{eqnarray}
  %M \cdot (%M^\top \cdot %v) & = & %w \\
  %u_0 & = & %M^\top \cdot %v
\end{eqnarray}
We now have the isomorphism %f:
\begin{eqnarray}
 %f(%v) & = & %v + %u_0
\end{eqnarray}
Then, we can compute the orthogonal projection:
\begin{eqnarray}
  orthogonal projection of %p onto {%v | %M \cdot %v = %w} & = & %f(%p^\ast) & = & %p^\ast + %u_0
\end{eqnarray}
</div>


<div class="mathenv example_to_know">
<b>Example:</b> We can find a basis of the following set of solutions of a homogenous system (thus, a vector space):
\begin{eqnarray}
 %V & = & { %v | #[ 3 #, 6 #; 1 #, 2 #] \cdot %v = \0 }
\end{eqnarray}
We can do so by starting with a basis that spans the entire space that contains %V (which in this case is \R^2),
and then "removing" the contribution of the orthogonal complement %V^\bot from these basis vectors.

Thus, we begin with:
\begin{eqnarray}
 \R^2 & = & \span {#[ 1 #; 0 #], #[ 0 #; 1 #]}
\end{eqnarray}
We know that:
\begin{eqnarray}
 %V^\bot & = & \span {#[ 3 #; 6 #], #[ 1 #; 2 #]} & = & \span { #[ 1 #; 2 #] }
\end{eqnarray}
Thus, we project each basis vector for \R^2 onto %V^\bot, then subtract that from the basis vector:
\begin{eqnarray}
 #[ 1 #; 0 #] - (#[ 1 #; 0 #] \cdot (1/\sqrt(5)) \cdot #[ 1 #; 2 #]) \cdot (1/\sqrt(5)) \cdot #[ 1 #; 2 #] & = & #[ 4/5 #; ---2/5 #] \\
 #[ 0 #; 1 #] - (#[ 0 #; 1 #] \cdot (1/\sqrt(5)) \cdot #[ 1 #; 2 #]) \cdot (1/\sqrt(5)) \cdot #[ 1 #; 2 #] & = & #[ ---2/5 #; 1/5 #]
\end{eqnarray}
The remaining vectors above span %V:
\begin{eqnarray}
 %V & = & { %v | #[ 3 #, 6 #; 1 #, 2 #] \cdot %v = \0 } & = & \span {#[ 4/5 #; ---2/5 #], #[ ---2/5 #; 1/5 #]} & = & \span { #[ ---2/5 #; 1/5 #] }
\end{eqnarray}
</div>




<a name="lecture22b"></a>

<a name="5.9"></a>
<h3><span class="secn">5.9.</span> Fixed points, eigenvectors, and eigenvalues</h3>

We introduce several properties of linear transformations; these properties it possible to work with linear transformations in new and convenient ways.

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Given a linear transformation %f: %V \to %V, %v \in %V is a <i>fixed point</i> of %f if:
\begin{eqnarray}
  %f(%v) & = & %v
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following linear transformation %f: \R^2 \to \R^2:
\begin{eqnarray}
  %f(%v) & = & #[ 2 #, 1 #; 1 #, 2 #] \cdot %v
\end{eqnarray}
The following vector %v is a fixed point of %f:
\begin{eqnarray}
  %v & = & #[ ---1 #; 1 #]\\
  %f( #[ ---1 #; 1 #] ) & = & #[ 2 #, 1 #; 1 #, 2 #] \cdot #[ ---1 #; 1 #] \\
                      & = & #[ ---1 #; 1 #]
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any linear transformation %f, \0 is a fixed point of %f (see the definition of a linear transformation).
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> If %v is a fixed point of %f and %v \neq \0, then %f has an infinite number of fixed points. This is because for any %s \in \R and
any fixed point %v, we have that:
\begin{eqnarray}
           %f(%v) & = & %v \\
  %f(%s \cdot %v) & = & %s \cdot %f(%v) \\
                  & = & %s \cdot %v
\end{eqnarray}
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any linear transformation %f where %f(%v) = %M \cdot %v, let %S be the set of fixed points of %f:

  $$ %S = {%v | %f(%v) = %v } $$

Then %S is a vector space. There are several ways to show this. Let %v be any fixed point of %f. Then we have:

\begin{eqnarray}
  %f(%v) & = & %v \\
  %M \cdot %v & = & %v \\
  (%M \cdot %v)  --- %v & = & \0 \\
  (%M \cdot %v)  --- \I \cdot %v & = & \0 \\
  (%M  --- \I) \cdot %v & = & \0
\end{eqnarray}

Thus, the fixed points of %f represented by %M are exactly the elements of the solution space of the homogenous system (%M  - \I) \cdot %v = \0.
Alternatively, we could show that %S contains \0, %S is closed under scalar addition, and %S is closed under scalar multiplication.
</div>

We can generalize the notion of a fixed point by observing that a fixed point is just a vector on which the linear transformation acts
as a scalar multiplier (in the case of a fixed point, the multiplier is 1). If %v is a fixed point of %f, then we have that:

  $$ %f(%v) = 1 \cdot %v$$

What if we created another notion that was not restricted to 1?

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> Given a linear transformation %f: %V \to %V, %v \in %V is an <i>eigenvector</i> of %f with <i>eigenvalue</i>
\lambda if:

  $$ %f(%v) = \lambda \cdot %v $$
</div>
  
<div class="mathenv fact proposition_to_know">
<b>Fact:</b> For any linear transformation %f, if %v is a fixed point of %f, then it is an eigenvector of %f with eigenvalue 1.
</div>

<div class="mathenv fact proposition_to_know">
<b>Definition:</b> The set of eigenvectors of a linear transformation %f that have eigenvalue \lambda is its <i>eigenspace</i>:
\begin{eqnarray}
  eigenspace of %f with eigenvalue \lambda & = & { %v | %f(%v) = \lambda \cdot %v }
\end{eqnarray}
Any eigenspace is a vector space.
</div>

<div class="mathenv fact proposition_to_know">
<b>Fact:</b> Given a linear transformation %f \in \R^n \to \R^n represented by %M \in \R^{n \times n} and an eigenvector %v, consider the following:
\begin{eqnarray}
  %f(%v) & = & \lambda %v \\
  %M \cdot %v & = & \lambda %v  \\
  (%M \cdot %v)  --- \lambda %v & = & \0 \\
  (%M \cdot %v)  --- \lambda \I \cdot %v & = & \0 \\
  (%M  --- \lambda \I) \cdot %v & = & \0
\end{eqnarray}
The above equation has only zero solutions if (%M  - \lambda \I) is invertible:
\begin{eqnarray}
  (%M  - \lambda \I) \cdot %v & = & \0 \\
   (%M  - \lambda \I)^{-1} \cdot (%M  - \lambda \I) \cdot %v & = &  (%M  --- \lambda \I)^{-1} \cdot \0 \\
    %v & = &  (%M  --- \lambda \I)^{-1} \cdot \0 \\
    %v & = &  \0
\end{eqnarray}
Thus, nonzero eigenvectors %v exist only if (%M  - \lambda \I) is not invertible. However, if it is not invertible, this means that \det (%M  --- \lambda \I) = 0. Thus, if it
is the case that \det (%M  - \lambda \I) = 0, then there must exist at least one \lambda that solves this equation. In fact, the eigenvalues
of %T are exactly the solutions to the equation:
\begin{eqnarray}
   \det (%M  --- \lambda \I) & = & 0
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose that we are modelling a system with two dimensions that are each modelled using \R:
<ul>
 <li>population in the city;</li>
 <li>population in the suburbs.</li>
</ul>

The following matrix (call it %T) represents the movement of the two populations between the two locations (with entries in percentages) over
one year:

<table cellpadding="0" cellspacing="0" style="font-size:12px;">
 <tr>
  <td></td>
  <td></td>
  <td align="center">from city</td>
  <td align="center">from suburbs</td>
  <td></td>
 </tr>
 <tr>
  <td style="text-align:right;">to city \~ </td>
  <td><table style="border-left:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0.95 <i style="color:gray;">stay in the city</i> \~ </td>
  <td>0.03 <i style="color:gray;">move from suburbs to city</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td style="text-align:right;">to suburbs \~ </td>
  <td><table style="border-left:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0.05 <i style="color:gray;">move from city to suburbs</i> \~ </td>
  <td>0.97 <i style="color:gray;">stay in the suburbs</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
</table>

For example, suppose the population in 1999 is represented by %v below. Then the population in 2000 is represented by %T \cdot %v:

\begin{eqnarray}
  %T \cdot %v & = & #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ 600,000 #; 400,000 #] \\
              & = & #[ 582,000 #; 418,000 #]
\end{eqnarray}

Let %f(%v) = %T \cdot %v be the linear transformation represented by %T.
<ol type="a">
  <li>What does a fixed point of %f represent?

  A fixed point of %f represents a stable population distribution that will not change from one year to the next. For example:

\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ 375,000 #; 625,000 #] & = & #[ 375,000 #; 625,000 #]
\end{eqnarray}

  Notice that any scalar multiple of this vector, including a vector that is normalized so that the two components add up to 1,
  is also a fixed point of %f:

\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ 375,000 #; 625,000 #] & = & #[ 0.375 #; 0.625 #]
\end{eqnarray}

 Thus, for this transformation, for any distribution of the population in which 37.5% live in the city, the distribution is stable.
  </li>

  <li>What does an eigenvector of %f represent?

  An eigenvector of %f represents a population distribution that may grow or shrink, but whose relative distribution between the city and
  the suburbs remains the same from one year to the next.

  </li>
  
  <li>
  Does %f have any nonzero eigenvectors other than the fixed points?

  No, because the sum of the components of any vector in \im %f is always the same as the sum of the components of the input vector that produced
  that image.
  
\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ %x #; %y #] & = & #[ 0.95 %x + 0.03 %y #; 0.05 %x + 0.97 %y #] \\
   %x + %y & = & (0.95 %x + 0.05 %x) + (0.03 %y + 0.97 %y) \\ 
           & = & (0.95 %x + 0.03 %y) + (0.05 %x + 0.97 %y)
\end{eqnarray}

  This is because the sums of the components of the column vectors of %T are 1. This makes %T a <i>stochastic</i> matrix, and it makes
  the fixed points the <i>steady-state</i> or <i>equilibrium</i> vectors of %T.

  </li>
  
  <li>
  Find the vector space of fixed points of %f.

  Note that either the fixed points of %f are {\0}, or there are infinitely many. Thus, if we can find a matrix equation whose solutions
  are the fixed points, we will obtain either a system whose only solution is \0, or an underdetermined system.
  
  We know that for our particular %f and %T, the space of fixed points is the set of solutions to the equation:
\begin{eqnarray}
   %S & = & { %v | (#[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] - #[ 1 #, 0 #; 0 #, 1 #]) \cdot %v & = & #[ 0 #; 0 #] }
\end{eqnarray}
  We then find the basis that spans %S:
\begin{eqnarray}
  %S & = & {%v | %v \cdot #[ -0.05 #; 0.03 #] = 0} \\
          & = & \span {#[ 0.03 #; 0.05 #]} \\
          & = & \span {#[ 3 #; 5 #]}
\end{eqnarray}
  </li>
  <li>
  Suppose we want to find a closed formula for the population %k years after the initial state %v (i.e., after applying %T to an initial
  vector %v eight times, or %T^k \cdot %v) where we have:

\begin{eqnarray}
  %v & = &  #[ 0.6 #; 0.4 #]
\end{eqnarray}

  The formula should be in terms of %k and should not require matrix multiplication. In other words,
  we should be able to obtain closed formulas for the city population and the suburb population in terms of %k.


  We can approach this problem by finding the eigenvectors of %f. Then, we can express the result of %T^k \cdot %v
  as a linear combination of eigenvectors.
  
\begin{eqnarray}
  \det (#[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] - \lambda \cdot #[ 1 #, 0 #; 0 #, 1 #]) & = & 0 \\
  \det #[ 0.95 - \lambda #, 0.03 #; 0.05 #, 0.97 - \lambda #] & = & 0 \\
  (0.95 - \lambda) \cdot (0.97 - \lambda) - (0.03 \cdot 0.05) & = & 0 \\
  \lambda^2 - 1.92 \lambda + 0.9215 - 0.0015 & = & 0 \\
  \lambda^2 - 1.92 \lambda + 0.92 & = & 0 \\
  \lambda & = & (1.92 &pm; &radic;(1.92^2 - 4(0.92)))/2 \\
  \lambda & = & 1.92/2 &pm; 0.08/2 \\
  \lambda & = & 0.96 &pm; 0.04 \\
  \lambda & = & 1 \~ <b>or</b> \~ 0.92
\end{eqnarray}

We already know the eigenvector for eigenvalue 1 (it is the fixed point). To find the eigenvector for the other eigenvalue, we solve:

\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[%x #; %y#] & = & 0.92 \cdot #[%x #; %y#] \\
  0.95 %x + 0.03 %y & = & 0.92 %x \\
  0.05 %x + 0.97 %y & = & 0.92 %y \\
  0.03 %x + 0.03 %y & = & 0 \\
  0.05 %x + 0.05 %y & = & 0 \\
  %x & = & 1 \\
  %y & = & -1
\end{eqnarray}

Thus, our eigenvectors are:

\begin{eqnarray}
  %e_1 & = & #[ 3 #; 5 #]\\
  %e_2 & = & #[ 1 #; -1 #]
\end{eqnarray}

Notice that we had a space of solutions because the system was underdetermined; we chose a particular eigenvector. Finally, notice that:

\begin{eqnarray}
  %T^k \cdot (%a \cdot %e_1 + %b \cdot %e_2) & = & (%T^k \cdot %a \cdot %e_1) + (%T^k \cdot %b \cdot %e_2) \\
                                             & = & %a \cdot (%T^k \cdot %e_1) + %b \cdot (%T^k \cdot %e_2) \\
                                             & = & %a \cdot %e_1 + %b \cdot 0.92^k \cdot %e_2
\end{eqnarray}

Thus, if the initial input vector can be rewritten in terms of the two eigenvectors, we can find the closed formula. In fact, it can be
because the two eigenvectors are linearly independent:

\begin{eqnarray}
  %a \cdot #[ 3 #; 5 #] + %b \cdot #[ 1 #; -1 #] & = & #[ %x #; %y #] \\
  #[ 3 #, 1 #; 5 #, -1 #] \cdot #[%a #; %b #] & = & #[ %x #; %y #] \\
  #[%a #; %b #] & = & #[ 3 #, 1 #; 5 #, -1 #]^{-1} #[ %x #; %y #] \\
  #[%a #; %b #] & = & #[ 3 #, 1 #; 5 #, -1 #]^{-1} #[ 0.6 #; 0.4 #] \\
  #[%a #; %b #] & = & #[ 0.125 #; 0.225 #] \\
  0.125 \cdot #[ 3 #; 5 #] + 0.225 \cdot #[ 1 #; -1 #] & = & #[ 0.6 #; 0.4 #] \\
\end{eqnarray}

The closed formula is:

\begin{eqnarray}
  %T^k \cdot #[ 0.6 #; 0.4 #] & = & 0.125 \cdot #[ 3 #; 5 #] +  0.225 \cdot 0.92^k \cdot #[ 1 #; -1 #]
\end{eqnarray}

  </li>
</ol>
</div>

Eigenspaces have many other applications. In particular, they make it possible to provide "natural" interpretations of general notions of
concepts such as differentiation in the context of vector spaces. 

<div class="mathenv example_to_know">
<b>Example:</b> Differentiation is a linear transformation from the vector space of differentiable functions (or a subset, e.g., the polynomials):

\begin{eqnarray}
  \ddx (%a %f + %b %g) \~ & = & \~ %a \ddx %f + %b \ddx %g
\end{eqnarray}

As an example, consider the space of polynomials of the form %f(%x) = %a %x^2 + %b %x + %c. If each polynomial is represented as a vector
of its coefficients, the differentiation operator for this vector space of functions can be represented as a matrix:

\begin{eqnarray}
  #[ 0 #, 0 #, 0 #; 2 #, 1 #, 0 #; 0 #, 1 #, 0 #] \cdot #[ %a #; %b #; %c #] & = &  #[ 0 #; 2 %a #; %b #]
\end{eqnarray}

Thus, an eigenvector in this vector space is any differentiable function %f such that:

\begin{eqnarray}
  \ddx %f & = & \lambda %f
\end{eqnarray}

Notice that the above is a differential equation. If \lambda = 0, then we have for any constant %c \in \R the solution:

\begin{eqnarray}
  %f(%x) = %c
\end{eqnarray}

If \lambda \neq 0 and we do not restrict ourselves to polynomials but allow all infinitely differentiable functions, then we have the solution:

\begin{eqnarray}
  %f(%x) = %c %e<sup>\lambda%x</sup>
\end{eqnarray}
</div>



















<a name="R.2"></a>
<hr style="margin-bottom:120px;"/>
<h2><span class="secn">Review 2.</span> Vector and Matrix Algebra, Vector Spaces, and Linear Transformations</h2>

The following is a breakdown of what you should be able to do at the end of the course 
(and of what you may be tested on in an exam).
Notice that many of the tasks below can be composed.
This also means that many problems can be solved in more than one way.

<ul>
  <li>vectors
    <ul>
      <li>definitions and algebraic properties of scalar and vector operations (addition, multiplication, etc.)</li>
      <li>vector properties and relationships between vectors
        <ul>
          <li>dot product of two vectors</li>
          <li>norm of a vector</li>
          <li>unit vectors</li>
          <li>orthogonal projection of a vector onto another vector</li>
          <li>orthogonal vectors</li>
          <li>linear dependence of two vectors</li>
          <li>linear independence of two vectors</li>
          <li>linear combinations of vectors</li>
          <li>linear independence of three vectors</li>
        </ul>
      </li>
      <li>lines and planes
        <ul>
          <li>line defined by a vector and the origin ([0; 0])</li>
          <li>line defined by two vectors</li>
          <li>line in \R^2 defined by a vector orthogonal to that line</li>
          <li>plane in \R^3 defined by a vector orthogonal to that plane</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>matrices
    <ul>
      <li>algebraic properties of scalar and matrix multiplication and matrix addition</li>
      <li>collections of matrices and their properties (e.g., invertibility, closure)
        <ul>
          <li>identity matrix</li>
          <li>elementary matrices</li>
          <li>scalar matrices</li>
          <li>diagonal matrices</li>
          <li>upper and lower triangular matrices</li>
          <li>matrices in reduced row echcelon form</li>
          <li>determinant of a matrix in \R^{2 \times 2}</li>
          <li>inverse of a matrix and invertible matrices</li>
        </ul>
      </li>
      <li>other matrix operations and properties
        <ul>
          <li>determine whether a matrix is invertible
            <ul>
              <li>using the determinant for matrices in \R^{2 \times 2}</li>
              <li>using facts about rref for matrices in \R^{n \times n}</li>
            </ul>
          </li>
          <li>algebraic properties of matrix inverses with respect to matrix multiplication</li>
          <li>transpose of a matrix
            <ul>
              <li>algebraic properties of transposed matrices with respect to matrix addition, multiplication, and inversion</li>
            </ul>
          </li>
          <li>matrix rank</li>
        </ul>
      </li>
      <li>matrices in applications
        <ul>
          <li>solve an equation of the form %L%U = %w</li> 
          <li>matrices and systems of states
            <ul>
              <li>interpret partial observations of system states as vectors</li>
              <li>interpret relationships betweem dimensions in a system of states as a matrix</li>
              <li>given a partial description of a system state and a matrix of relationships, find the full description of the system state</li>
              <li>interpret system state transitions/transformations over time as matrices
                <ul>
                  <li>population growth/distributions over time
                    <ul>
                      <li>compute the system state after a specifieds amount of time</li>
                      <li>find the fixed point of a transition matrix</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>vector spaces
    <ul>
      <li>vector spaces and their properties
        <ul>
          <li>given a set of vectors or other objects, show that it is a vector space</li>
          <li>express a vector space as a span of a finite set of vectors</li>
          <li>given two vector spaces defined using span notation, show one is a subspace of the other</li>
          <li>given two vector spaces defined using span notation, show they are equal</li>
          <li>find the basis of a vector space</li>
          <li>find an orthonormal basis of a vector space</li>
          <li>find the dimension of a vector space</li>
          <li>find the orthogonal complement of a vector space</li>
        </ul>
      </li>
      <li>particular vector spaces
        <ul>
          <li>the set of polynomials {%f | %f(%x) = %a_%k %x^{%k} + ... + %a_1 %x + %a_0, %a_1,...,%a_%k \in \R}</li>
          <li>the solution set of an equation { %v | %M \cdot %v = %w }</li>
        </ul>
      </li>
      <li>affine spaces
      </li>
      <li>find the least-squares approximation of an overdetermined linear system</li>
  </ul>
  </li>
  <li>linear transformations
    <ul>
      <li>determine if a relation is a map</li>
      <li>determine if a map is a linear transformation</li>
      <li>given a linear transformation (and/or its matrix representation)...
        <ul>
          <li>show it is injective</li>
          <li>show it is surjective</li>
          <li>show it is bijective</li>
          <li>find its image (a vector space)</li>
          <li>find its space of fixed points</li>
          <li>find its eigenvalues</li>
          <li>find its eigenvector for a given eigenvalue</li>
        </ul>
      </li>
      <li>compositions of linear transformations and their properties</li>
      <li>compute orthogonal projections...
        <ul>
          <li>onto the span of a single vector in \R^{%n}</li>
          <li>onto a subspace of \R^{%n}...
            <ul>
            <li>by first computing an orthonormal basis and then using it to find the projection</li>
            <li>by using the formula %M \cdot %M^\top if %M has orthonormal columns</li>
            <li>by using the formula %M \cdot (%M^\top \cdot %M)^{-1} \cdot %M^\top</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>applications and solving systems of equations
    <ul>
      <li>curve fitting
        <ul>
          <li>find a polynomial curve that exactly fits a given set of points in \R^2</li>
          <li>find a least-squares approximate polynomial curve that best fits a set of points in \R^2</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

Below is a comprehensive collection of review problems going over the material covered in this course. These problems are an
accurate representation of the kinds of problems you may see on an exam.

<div class="mathenv example_to_know">
<b>Example:</b> Consider the following vector space:
\begin{eqnarray}
 %V & = & \span{ #[2#;1#;4#], #[0#;1#;-1#]}
\end{eqnarray}

<ul type="a">
  <li>Find the orthogonal projection of the following vector onto %V:
\begin{eqnarray}
 %u & = & #[1#;2#;0#]
\end{eqnarray}
If the two spanning vectors were orthogonal, one approach would be to project %u onto a normalized
form of each vector, and to add the results. If the two spanning vectors were orthonormal, it would be
sufficient to simply project onto each vector, and add the results. Because the two vectors are neither,
we can use the formula %M (%M^\top %M)^{-1} %M^\top %u for the projection where
\begin{eqnarray}
%M & = & #[ 2#,0 #; 1#,1 #; 4#,-1 #] 
\end{eqnarray}
  </li>
  <li>Find a basis of %V^\bot.

It is sufficient to set up an underdetermined
system, solve for two of the variables in terms of the third, and set the third to an arbitrary constant:

\begin{eqnarray}
 #[2#;1#;4#] \cdot #[%x#;%y#;%z#] & = & 0 \\
 #[0#;1#;-1#] \cdot #[%x#;%y#;%z#] & = & 0
\end{eqnarray}
\begin{eqnarray}
 2%x + %y + 4%z & = & 0 \\
 %y - %z & = & 0 \\
 %y & = & %z \\
 %x & = & -%y/2 - 4%z/2 = -%z/2 - 4%z/2 = -5/2 \cdot %z 
\end{eqnarray}
Setting %z = 2, we get:
\begin{eqnarray}
  %V^\bot & = & \span { #[ -5 #; 2 #; 2 #] }
\end{eqnarray}
  </li>
  <li>Find any matrix %M such that for %f(%v) = %M \cdot %v, %f(%v) = \0 for all %v \in %V.

It is sufficient to find a matrix that maps both spanning vectors to \0.
From above, we already have a vector that spans %V^\bot, so such a matrix can be computed using the formula:
\begin{eqnarray}
  #[ -5 #, 2 #, 0 #; 2 #, 1 #, 1 #; 2 #, 4 #, -1 #] \cdot  #[ 1 #, 0 #, 0 #; 0 #, 0 #, 0 #; 0 #, 0 #, 0 #] \cdot  #[ -5 #, 2 #, 0 #; 2 #, 1 #, 1 #; 2 #, 4 #, -1 #]^{-1}
\end{eqnarray}
  </li>  
</ul>
</div>




<div class="mathenv example_to_know">
<b>Example:</b> Consider the vector space of polynomials of degree at most 2:

  $$ %F = { %f | %f(%x) = %a %x^2 + %b %x + %c, %a,%b,%c \in \R} $$

The map %d:%F \to %F represent differentiation. For example:

\begin{eqnarray}
 %f(%x) & = & 5 %x^2 - 2 %x + 3 \\
 %g(%x) & = & 10 %x - 2 \\
 %d(%f) & = & %g
\end{eqnarray}

<ul type="a">
  <li>Determine whether %d:%F \to %F is injective.  

It is not injective because we can find two unequal inputs that produce the same output. Consider the following polynomials:
\begin{eqnarray}
%f(%x) & = & 1 \\
%g(%x) & = & 2 \\
%h(%x) & = & 0
\end{eqnarray}
Then we have that:
\begin{eqnarray}
%d(%f) & = & %h \\
%d(%g) & = & %h
\end{eqnarray}
Thus, the map %d is not injective.
  </li>
  <li>Recall that a polynomial can be represented as a vector. For example, %f(%x) = 5 %x^2 - 2 %x + 3 can be
  represented as:
\begin{eqnarray}
  #[ 5 #; -2 #; 3 #]
\end{eqnarray}
  Show that %d is a linear transformation by finding a matrix representation for %d.

The matrix representation is:
\begin{eqnarray}
#[ 0 #, 0 #, 0 #; 2 #, 1 #, 0 #; 0 #, 1 #, 0 #]
\end{eqnarray}
Notice that:
\begin{eqnarray}
  #[ 0 #, 0 #, 0 #; 2 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot #[ %a #; %b #; %c #] & = &  #[ 0 #; 2 %a #; %b #]
\end{eqnarray}
  </li>
  <li>Show that %d:%F \to %F is not surjective.

Intuitively, we see that there are only two linearly independent columns in the matrix representing %d, 
so there must be vectors in the codomain of %d that are not in the image of %d, so %d is not surjective.
To prove this, we find some vector %w in the codomain for which there is no solution to the following equation:
\begin{eqnarray}
  %w & = & #[ 1 #; 0 #; 0 #] \\
  #[ 0 #, 0 #, 0 #; 2 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot #[ %a #; %b #; %c #] & = &  %w \\
  #[ 0 #, 0 #, 0 #; 2 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot #[ %a #; %b #; %c #] & = &  #[ 1 #; 0 #; 0 #] \\
                                                        #[ 0 #; 2%a #; %b #] & = &  #[ 1 #; 0 #; 0 #] \\
                                                        0 & = &  1
\end{eqnarray}
Since we derive a contradiction, the above equation must have no solution.
  </li>
</ul>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Alice wants to send vectors in \R^2 to Bob. For any vector %v \in \R^2 that
she wants to send, she generates a random scalar %r \in \R and sends %w
to Bob as defined below:
\begin{eqnarray}
 %v & = & #[ %x #; %y #] \\
 %w & = & #[ 1 #, 2 #, 0 #; 0 #, 3 #, 0 #; 0 #, 0 #, 2 #] #[ %x #; %y #; %r #]
\end{eqnarray}

<ul type="a">
  <li>Find a matrix %D \in \R^{3 \times 3} that Bob can use to decode Alice's messages.

Alice is sending vectors that are a linear combination of two vectors that carry the two scalars in a message vector %v and a noise vector:
\begin{eqnarray}
#[ 1 #; 0 #; 0 #] %x + #[ 2 #; 3 #; 0 #] %y + #[ 0 #; 0 #; 2 #] %z
\end{eqnarray}
Bob must first cancel out the noise using a matrix in \R^3. Then he must take the result of that and recover
the scalars %x and %y.
To find an appropriate matrix to cancel out the noise, Bob can use the following formula:
\begin{eqnarray}
  %C & = & #[ 1 #, 0 #, 0 #; 0 #, 1 #, 0 #; 0 #, 0 #, 0 #]
\end{eqnarray}
Once Bob applies %C to the vector he receives from Alice, he has:
\begin{eqnarray}
#[ 1 #; 0 #; 0 #] %x + #[ 2 #; 3 #; 0 #] %y + #[0#;0#;2#] \cdot 0 & = & #[ 1 #, 2 #, 0 #; 0 #, 3 #, 0 #; 0 #, 0 #, 0 #] #[ %x #; %y #; 0 #]
\end{eqnarray}
Notice that we can find a matrix that inverts this operation by replacing the top-left portion of the above encoding matrix with its inverse:
\begin{eqnarray}
#[ 1 #, 2 #; 0 #, 3 #]^{-1} & = & (1/3) \cdot #[ 3 #, -2 #; 0 #, 1 #] & = & #[ 1 #, -2/3 #; 0 #, 1/3 #]
\end{eqnarray}
\begin{eqnarray}
%D & = & #[ 1 #, -2/3 #, 0 #; 0 #, 1/3 #, 0 #; 0 #, 0 #, 0 #] \\
#[ 1 #, -2/3 #, 0 #; 0 #, 1/3 #, 0 #; 0 #, 0 #, 0 #] #[ 1 #, 2 #, 0 #; 0 #, 3 #, 0 #; 0 #, 0 #, 0 #] #[ %x #; %y #; 0 #] & = &  #[ %x #; %y #; 0 #]
\end{eqnarray}
Thus, Bob can use the following matrix to decode messages:
\begin{eqnarray}
  %D \cdot %C
\end{eqnarray}
If Bob receives a message %w that is the encoded version of %v, he can retrieve it by computing:
\begin{eqnarray}
  %D \cdot %C \cdot %w
\end{eqnarray}
  </li>
  <li>Find a matrix %D' \in \R^{2 \times 3} that Bob can use to retrieve %v \in \R^2 given a transmitted %w \in \R^3.

Bob simply needs to drop the third component of the result of %D \cdot %C \cdot %w. This can be accomplished by computing:
\begin{eqnarray}
#[1 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot %D \cdot %C \cdot %w
\end{eqnarray}
Thus, an appropriate matrix in \R^{2 \times 3} would be:
\begin{eqnarray}
#[1 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot %D \cdot %C
\end{eqnarray}

  </li>
</ul>
</div>


<div class="mathenv example_to_know">
<b>Example:</b> Suppose there are two locations across which a population is distributed. Over the course of each year,
the population moves between the two locations according to one of two population distribution transformations depending
on how well the economy is doing (%A if the economy is doing well, %B otherwise):
\begin{eqnarray}
 %A & = & #[ 0.55 #, 0.9  #; 0.45 #, 0.1 #] \\
 %B & = & #[ 0.75 #, 0.3  #; 0.25 #, 0.7 #] \\
\end{eqnarray}

<ul type="a">
  <li>Find an initial state %v \in \R^2 such that if the economy is always doing well,
      the population distribution will remain the same.

We need to find a fixed point of %f(%v) = %A \cdot %v. Since \dim \R^2 = 2, we know that there are three possibilities:
<ul>
 <li>\0 is the only fixed point;</li>
 <li>the space of fixed points has dimension 1, so there are infinitely many fixed points that all fall on a single line;</li>
 <li>all points in \R^2 are fixed points.</li>
</ul>

We solve the following system:
\begin{eqnarray}
 #[ 0.55 #, 0.9  #; 0.45 #, 0.1 #] \cdot #[%x #; %y#] & = & #[%x #; %y#] \\
 0.55 %x + 0.9 %y & = & %x \\
 0.9 %y & = & 0.45 %x \\
 0.45 %x + 0.1 %y & = & %y \\
 0.45 %x & = & 0.9 %y \\
 %x & = & 2 %y
\end{eqnarray}
Since one equation can be derived from the other, the above system is underdetermined, but %x can be expressed in terms of %y. Thus, the space
of fixed points is one-dimensional. Setting %y = 10, it can be expessed as a span of the following fixed point vector:
\begin{eqnarray}
 #[ 20 #; 10 #]
\end{eqnarray}
  </li>
  <li>Suppose that over the course of several years, the economy has both done well and not well.
      Has the total population (the sum of the populations in the two locations) changed over this
      duration?

Notice that neither %A nor %B change the total population in the two locations as represented by a state vector in \R^2. Thus, the total
population has not changed.
  </li>
</ul>
</div>





















<div class="mathenv example_to_know">
<b>Example:</b> Given %u \in \R^n, what is the orthogonal projection of %v \in \R^n onto \span{%u}?

It is sufficient to compute the orthogonal projection of %v onto %u. Given a unit vector %e parallel to %u, the projection of %v onto %e would
be:

  $$ (%e \cdot %v) \cdot %e.$$

However, we cannot assume %u is a unit vector. Thus, we scale %u to obtain a unit vector %e = %u/||%u||. Then, the solution is:

  $$((%u/||%u||) \cdot %v) \cdot (%u/||%u||).$$

</div>

<div class="mathenv example_to_know">
<b>Example:</b> Let %M \in \R^{17 \times 9} and %f(%v) = %M \cdot %v. What is the largest possible value of \dim(\im(%f))?


We know that we can only compute %M %v if %v has as many rows as %M has columns. Thus, if %f(%v) = %M \cdot %v, then %v \in \R^9. We
also know that %M \cdot %v will be a vector with 17 rows because %M has 17 rows, so %f(%v) \in \R^{17}. Thus, %f \in \R^9 \to \R^{17}.

This means that \dim(\im(%f)) cannot be greater than \dim(\R^{17}), so it cannot exceed 17. However, we also need to note that %M
has 9 columns, and that any value in the image of %f is thus a linear combination of at most 9 vectors. Thus, any basis of \im(%f) has
at most 9 distinct vectors in it. Since \dim(\im(%f)) is the size of the basis of \im(%f), \dim(\im(%f)) can be at most 9.

</div>

<div class="mathenv example_to_know">
<b>Example:</b> Find an orthonormal basis for the following vector space:

\begin{eqnarray}
 \span { #[ 1 #; 0 #; 2 #; 0 #] , #[ 0 #; 1 #; 2 #; 3 #] , #[ 0 #; 0 #; 0 #; 1 #] }.
\end{eqnarray}

We can use the <a href="#algorithmgramschmidt">algorithm</a> for computing the vectors in an orthonormal basis. We can work with the vectors in any order, so suppose we have:

\begin{eqnarray}
 %v_1 = #[ 0 #; 0 #; 0 #; 1 #] , \~ %v_2 = #[ 0 #; 1 #; 2 #; 3 #] , and \~ %v_3 = #[ 1 #; 0 #; 2 #; 0 #] .
\end{eqnarray}

According to the algorithm, we then let %u_1 = %v_1 and %e_1 = %u_1 / ||%u_1||. In this case, we still have %e_1 = %v_1. Next, we compute:

\begin{eqnarray}
 %u_2 & = & %v_2 - ((%v_2 \cdot %e_1) \cdot %e_1) & = & #[ 0 #; 1 #; 2 #; 3 #] - #[ 0 #; 0 #; 0 #; 3 #] & = & #[ 0 #; 1 #; 2 #; 0 #]
 & \~ & %e_2 & = & #[ 0 #; 1/\sqrt(5) #; 2/\sqrt(5) #; 0 #]
\end{eqnarray}

\begin{eqnarray}
 %u_3 & = & %v_3 - ((%v_3 \cdot %e_1) \cdot %e_1) - ((%v_3 \cdot %e_2) \cdot %e_2)
      & = & #[ 1 #; 0 #; 2 #; 0 #] - #[ 0 #; 0 #; 0 #; 0 #] - #[ 0 #; 4/5 #; 8/5 #; 0 #]  & = & #[ 1 #; -4/5 #; 2/5 #; 0 #]
 & \~ & %e_3 & = & (3\sqrt(5))/5 \cdot #[ 1 #; -4/5 #; 2/5 #; 0 #]
\end{eqnarray}

Thus, {%e_1, %e_2, %e_3} is an orthonormal basis for \span{%v_1, %v_2, %v_3}.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we are given a linear transformation %f:\R^2 \to \R^2 where:

\begin{eqnarray}
 %f(%v) & = & #[ 2 #, 7 #; 3 #, 1 #] \cdot %v.
\end{eqnarray}

<ol type="a">
<li>Find an orthonormal basis for \im(%f).

We know that the column vectors of the matrix are linearly independent. Thus, \dim(\im(%f)) as at least 2, and since
the codomain of %f is \R^2, \im(%f) = \R^2. Thus, any orthonormal basis of \R^2 is appropriate, such as:

\begin{eqnarray}
 { #[ 1 #; 0#] , #[ 0 #; 1#] }.
\end{eqnarray}
</li>

<li>Find the set of all %v such that %f(%v) = \0.

Let the matrix in the definition of %f be %M.
We know from lecture that %M is invertible iff %M \cdot %v = \0 has exactly one solution \0. Since \det %M \neq 0, %M is invertible, so
there is exactly one solution in the solution space {\0}.

Alternatively, it is sufficient to find the set of solutions to the equation %f(%v) = \0, which is the set of solutions (expanding the
definition of %f) of:
\begin{eqnarray}
 #[ 2 #, 7 #; 3 #, 1 #] \cdot %v & = & #[ 0 #; 0 #].
\end{eqnarray}
Since %M is invertible, we can multiply both sides by %M^{-1} to obtain:
\begin{eqnarray}
  %v & = & 1/(3-21) \cdot #[ 1 #, -7 #; -3 #, 2 #] \cdot #[ 0 #; 0 #] \\
  %v & = & #[ 0 #; 0 #].
\end{eqnarray}
Thus, there is only one solution in the solution set: {\0}.
</li>

<li>Show that %f is surjective.

To show that %f is surjective, it is sufficient to show that for every %v in the codomain, there exists %x such that:
\begin{eqnarray}
  %f(%x) & = & %v
\end{eqnarray}
In other words, we want a formula for %x in terms of %v that is defined for any %v. Let the matrix in the definition of %f be %M. Since %M
is invertible, we can define:
\begin{eqnarray}
  %x & = & 1/(3-21) \cdot #[ 1 #, -7 #; -3 #, 2 #] \cdot %v
\end{eqnarray}
This formula for %x is always defined, so %f is surjective.
</li>
</ol>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we are given a linear transformation %f:\R^2 \to \R^2 where:

\begin{eqnarray}
 %f(%v) & = & #[ 2 #, 1 #; 8 #, 4 #] \cdot %v.
\end{eqnarray}

<ol type="a">
<li>Find \dim(\im(%f)).

Recall that the dimension of a space is the size of any basis of that space (all bases of a space have the same size). Let %M be the
matrix in the definition of %f. The space \im(%f) is equivalent to the span of the column vectors of the matrix:

\begin{eqnarray}
  \im(%f) & = & \span{ #[ 2 #; 8 #] , #[ 1 #; 4 #] }
\end{eqnarray}

To find a basis of a space spanned by a collection of vectors, we create a matrix whose rows are the vectors in that collection, find its
reduced row echelon form, and keep only the nonzero rows in the basis:

\begin{eqnarray}
 #[ 2 #, 8 #; 1 #, 4 #] \to #[ 1 #, 4 #; 1 #, 4  #] \to #[ 1 #, 4 #; 0 #, 0 #]
\end{eqnarray}
\begin{eqnarray}
  \im(%f) & = & \span{ #[ 1 #; 4 #] }
\end{eqnarray}

Thus, the dimension of \im(%f) is 1.
</li>
<li>Show that %f is not injective.

To show that a map is not injective, it is sufficient to find %v and %v' such that %v \neq %v' but %f(%v) = %f(%v').

One approach to finding such %v and %v' is to expand %f(%v) = %f(%v') until the constraints are simple enough that
it is straightforward to try some inputs and easily check that they satisfy the constraints.

\begin{eqnarray}
  %f(%x) & = & %f(%x') \\
   #[ 2 #, 1 #; 8 #, 4 #] \cdot #[ %x #; %y #] & = & #[ 2 #, 1 #; 8 #, 4 #] \cdot #[ %x' #; %y' #]
\end{eqnarray}

Since the top row of the matrix is a multiple of the bottom row, we get one equation:

\begin{eqnarray}
  %x & \neq & %x' \\
  %y & \neq & %y' \\
  2%x + %y & = & 2%x' + %y'
\end{eqnarray}

One possible pair of vectors in the domain that satisfies the above is:

\begin{eqnarray}
  #[%x #; %y #] & = & #[ 0 #; 2 #] \\
  #[%x' #; %y' #] & = & #[ 1 #; 0 #]
\end{eqnarray}

Thus, %f is not injective.
</li>
<li>Define the solution space %f(%v) = \0 as a span and find its basis.

We write down the definition of the solution space of the homogenous system above:
\begin{eqnarray}
  { #[ %x #; %y #] | %f( #[ %x #; %y #] ) =  #[ 0 #; 0 #] }
              & = & { #[ %x #; %y #] | #[ 2 #, 1 #; 8 #, 4 #] \cdot #[ %x #; %y #] = #[ 0 #; 0 #] }
\end{eqnarray}
The constraints imposed above can be summarized as:
\begin{eqnarray}
   { #[ %x #; %y #] | 2%x + %y = 0} & = & { #[ %x #; %y #] | %y = -2%x}
\end{eqnarray}
Thus, the solution space is the line in \R^2 defined by the equation %y = -2%x. We can choose any vector on this line and take the span of the set
containing only that vector to provide an explicit definition for the set of solutions:
\begin{eqnarray}
  \ker(%f) & = & \span { #[ -1 #; 2 #] } .
\end{eqnarray}
</li>
</ol>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Find a matrix %M such that for %f(%v) = %M \cdot %v,
\begin{eqnarray}
  \im(%f) & = & (\span{ #[ 2 #; 0 #; 0 #; 0 #] , #[ 0 #; 0 #; 2 #; 0 #] })^\bot
\end{eqnarray}

One way to approach this problem is to expand the definition on the right-hand side using the
definition of the orthogonal complement operation:

\begin{eqnarray}
  \im(%f) & = & (\span{ #[ 2 #; 0 #; 0 #; 0 #] , #[ 0 #; 0 #; 2 #; 0 #] })^\bot
          & = & { #[ x #; y #; z #; t #] | #[ 2 #; 0 #; 0 #; 0 #] \cdot #[ x #; y #; z #; t #] = 0, \~ #[ 0 #; 0 #; 2 #; 0 #] \cdot #[ x #; y #; z #; t #] = 0 } \\
\end{eqnarray}
\begin{eqnarray}
          & = & { #[ x #; y #; z #; t #] | 2%x  = 0, \~ 2%z = 0, \~ %y,%t \in \R } \\
          & = & { #[ x #; y #; z #; t #] | %x  = 0, \~ %z = 0, \~ %y,%t \in \R } \\
          & = & { #[ 0 #; y #; 0 #; t #] | %y,%t \in \R }.
\end{eqnarray}
Thus,
\begin{eqnarray}
  \im(%f) & = & \span{ #[ 0 #; 1 #; 0 #; 0 #] , #[ 0 #; 0 #; 0 #; 1 #] }.
\end{eqnarray}
Recall that the image of %f is the span of the columns of %M. Thus, one possible solution is:
\begin{eqnarray}
  %M & = & #[ 0 #, 0 #; 1 #, 0 #; 0 #, 0 #; 0 #, 1 #] .
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose that %a^2 + %b^2 = 1. What is the orthogonal projection of %v onto \im(%f) if %f(%v) = %M \cdot %v, and:

\begin{eqnarray}
 %v & = & #[ 1  #; 2 #; 3 #] \\
  %M & = & #[ %a #, 0 #, 0 #; %b #, 0 #, 0 #; 0 #, %c #, 0 #]
\end{eqnarray}
We have that:
\begin{eqnarray}
  \im(%f) & = & \span{ #[ %a #; %b #; 0 #] , #[ 0 #; 0 #; %c #] }.
\end{eqnarray}
Because %a^2 + %b^2 = 1, the first vector is already a unit vector and orthogonal to the second vector. By rescaling the second vector to be
a unit vector, we can obtain an orthonormal basis:
\begin{eqnarray}
  \im(%f) & = & \span{ #[ %a #; %b #; 0 #] , #[ 0 #; 0 #; 1 #] }.
\end{eqnarray}
We can now find the orthogonal projection of %v onto each vector in the orthonormal basis, and add these to find the orthogonal projection of the
vector onto \im(%f):
\begin{eqnarray}
  ( #[ 1 #; 2 #; 3 #] \cdot #[ %a #; %b #; 0 #] ) \cdot #[ %a #; %b #; 0 #] +
    ( #[ 1  #; 2 #; 3 #] \cdot #[ 0 #; 0 #; 1 #] ) \cdot #[ 0 #; 0 #; 1 #]
  & = &  #[ %a^2 + 2%b%a #; %a%b + 2%b^2 #; 3 #].
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Suppose we are given the following points:

\begin{eqnarray}
 #[ 0 #; 6 #] , #[ 2 #; 6 #] , #[ 2 #; 12 #].
\end{eqnarray}

<ol type="a">
<li>Find a function of the form %f(%x) = %a%x^2 + %b%x + %c that is the best least-squares fit for these points.

We begin by writing down the equations in terms of %f for each point:

\begin{eqnarray}
 %f(0) & = & 6 \\
 %f(2) & = & 6 \\
 %f(2) & = & 12
\end{eqnarray}

We construct the matrix equation that represents the above system:

\begin{eqnarray}
 #[ (0)^2 #, (0) #, 1 #; (2)^2 #, (2) #, 1  #; (2)^2 #, (2) #, 1 #] \cdot  #[ %a #; %b #; %c #] & = & #[ 6 #; 6 #; 12 #]
\end{eqnarray}
There is no solution to the above equation. However, we are looking for the least-squares best fit approximation. Thus, we first find an
orthonormal basis of the image of the matrix. If the matrix is %M and %f(%v) = %M \cdot %v, we have that:

\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 4 #; 4 #] , #[ 0 #; 2 #; 2 #] , #[ 1 #; 1 #; 1 #] }.
\end{eqnarray}

Using the <a href="#algorithmgramschmidt">algorithm</a> for finding an orthonormal basis, we obtain:
\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 1/\sqrt{2} #; 1/\sqrt{2} #] , #[ 1 #; 0 #; 0 #] }.
\end{eqnarray}

\begin{tabular}
We now find the orthogonal projection of #[ 6 #; 6 #; 12 #] onto \im(%f) and use it to rewrite the equation so that it is not overdetermined:
\end{tabular}

\begin{eqnarray}
 #[ (0)^2 #, (0) #, 1 #; (2)^2 #, (2) #, 1  #; (2)^2 #, (2) #, 1 #] \cdot  #[ %a #; %b #; %c #] & = & #[ 6 #; 9 #; 9 #]
\end{eqnarray}

This system is underdetermined. The space of solutions implied by the above equation is:
\begin{eqnarray}
 { %f \~ | \~ %f(%x) = %a%x^2 + %b%x + %c, \~ %c = 6, \~ 4%a + 2%b + %c = 9}
\end{eqnarray}
This means that any solution to the system is a least-squares best fit approximation. One possible best fit approximation is:

  $$ %f(%x) = %x^2 + (-1/2)%x + 6 $$

</li>

<li>Find a function of the form %f(%x) = %b%x + %c that is the best least-squares fit for these points.

We follow the same process as in part (a) above. The matrix equation is:

\begin{eqnarray}
 #[ (0) #, 1 #; (2) #, 1  #; (2) #, 1 #] \cdot  #[ %b #; %c #] & = & #[ 6 #; 6 #; 12 #]
\end{eqnarray}

There is no solution to the above equation. However, we are looking for the least-squares best fit approximation. Thus, we first find an
orthonormal basis of the image of the matrix. If the matrix is %M and %f(%v) = %M \cdot %v, we have that:

\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 2 #; 2 #] , #[ 1 #; 1 #; 1 #] }.
\end{eqnarray}

Using the <a href="#algorithmgramschmidt">algorithm</a> for finding an orthonormal basis, we obtain:

\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 1/\sqrt{2} #; 1/\sqrt{2} #] , #[ 1 #; 0 #; 0 #] }.
\end{eqnarray}

\begin{tabular}
We again find the orthogonal projection of #[ 6 #; 6 #; 12 #] onto \im(%f) and use it to rewrite the equation so that it is not overdetermined:
\end{tabular}

\begin{eqnarray}
 #[ (0) #, 1 #; (2) #, 1  #; (2) #, 1 #] \cdot  #[ %b #; %c #] & = & #[ 6 #; 9 #; 9 #]
\end{eqnarray}

This yields %c = 6 and %b = 3/2. Thus, the least-squares best fit approximation is:

  $$ %f(%x) = (3/2)%x + 6 $$

</li>
</ol>
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Show that for %M \in \R^{n \times n}, the space of solutions to %M %x = \0 is a vector space (you do not need to show that
all the axioms hold; it is sufficient to show the appropriate closure properties).

Let %S be the space of solutions.

We show that \0 \in %S:
\begin{eqnarray}
 %M \cdot \0 = \0.
\end{eqnarray}

We show that if %v,%v' \in %S, then %v + %v' \in %S:
\begin{eqnarray}
 %M \cdot %v & = & \0 \\
 %M \cdot %v' & = & \0 \\
 %M \cdot (%v + %v') & = & %M \cdot %v + %M \cdot %v' \\
                     & = & \0 + \0 \\
                     & = & \0.
\end{eqnarray}

We show that if %v \in %S, then for any scalar %s \in \R, %s %v \in %S:
\begin{eqnarray}
 %M \cdot %v & = & \0 \\
 %M \cdot (%s \cdot %v) & = & %s \cdot (%M \cdot %v) \\
                        & = & %s \cdot \0 \\
                        & = & \0.
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Compute the orthogonal projection of %v onto \im(%f) where %f(%v) = %M \cdot %v,

\begin{eqnarray}
 %v & = & #[ 1  #; -2 #; 3 #] , and %M & = & #[ 1 #, 1 #; 2 #, 8 #; 1 #, 5 #].
\end{eqnarray}

Normally, the way to approach such problems is to find an orthonormal basis for \im(%f) where %f(%v) = %M \cdot %v, then use
that orthonormal basis to project %v onto \im(%f). However, it's a good idea to check for special cases before delving into a large
computation. In this particular example, we have that:

\begin{eqnarray}
 #[ 1  #; -2 #; 3 #] \cdot #[ 1  #; 2 #; 1 #] & = & 0 \\
 #[ 1  #; -2 #; 3 #] \cdot #[ 1  #; 8 #; 5 #] & = & 0.
\end{eqnarray}

Thus, %v is orthogonal to both vectors, so the orthogonal projection of %v onto \im(%f) is \0 \in \R^3.

It is worth noting that %v is in the orthogonal complement of \im(%f). In general, given a vector subspace %W \subset %V,
for any %w \in %W^\bot, the orthogonal projection of %w onto %W is \0.
</div>

<div class="mathenv example_to_know">
<b>Example:</b> Is the system below overdetermined or underdetermined? Does it have a solution?

\begin{eqnarray}
 #[ 1 #, 2 #, 0 #; 0 #, 1 #, -4 #; 0 #, 0 #, 1 #] \cdot %v & = &  #[ 8 #; 3 #; 0 #]
\end{eqnarray}

The system is neither overdetermined nor underdetermined. It has a solution. Since the matrix is upper triangular, we can use the
algorithm for solving a system with an upper triangular matrix to obtain %v:
\begin{eqnarray}
 %v & = & #[ 2 #; 3 #; 0 #].
\end{eqnarray}
</div>

<div class="mathenv example_to_know">
<b>Example:</b>  Determine whether the system below has a solution for all possible %v \in \R^3. If it does not, describe exactly
the set of vectors %v for which the system below has a solution and determine whether this set a vector space.

\begin{eqnarray}
 #[ 1 #, 2 #, 2 #; 0 #, 2 #, -1 #; 1 #, 4 #, 1 #] \cdot %x & = & %v .
\end{eqnarray}


Notice that determining whether the system has a solution for any %v is equivalent to determining whether the linear
transformation represented by the matrix is surjective. It is also equivalent to determining whether the matrix is invertible.

To determine whether the matrix is invertible, we could compute its reduced row echelon form by finding an appropriate series of row operations.

\begin{eqnarray}
 #[ 1 #, 2 #, 2 #; 0 #, 2 #, -1 #; 1 #, 4 #, 1 #] \to  #[ 1 #, 2 #, 2 #; 0 #, 1 #, -1/2 #; 1 #, 4 #, 1 #]
                                                     \to #[ 1 #, 0 #, 3 #; 0 #, 1 #, -1/2 #; 1 #, 4 #, 1 #]
                                                     \to #[ 1 #, 0 #, 3 #; 0 #, 1 #, -1/2 #; 0 #, 4 #, -2 #]
                                                     \to #[ 1 #, 0 #, 3 #; 0 #, 1 #, -1/2 #; 0 #, 0 #, 0 #]
\end{eqnarray}

Given the above, we see that the matrix is not invertible.

Let the matrix be %M. The set of vectors for which there is a solution to the above equation is \im(%f) where %f(%x) = %M \cdot %x. Thus,
we want to find \im(%f) explicitly. We could say:
\begin{eqnarray}
 \im(%f) = \span { #[ 1 #; 0 #; 1 #] , #[ 2 #; 2 #; 4 #] , #[ 2 #; -1 #; 1 #] }.
\end{eqnarray}
If we want to be more precise, we could compute \rref(%M^\top) to find a basis for \im(%f).

Since the set of possible vectors %v for which the equation has a solution is \im(%f) and %f is a linear transformation, the set of such %v is a vector
space.
</div>










<!--assignment6-->

<br/><hr/>
<a name="6.1"></a>
<a name="assignment6"></a>
<h3><span class="secn"></span>
  <b>Extra Problems</b> <!--span class="btn_assignment">(<a href="materials.php?hw=6">show only this assignment</a>)</span-->
</h3>

<only132>

       <p>In this assignment you will solve problems involving underdetermined systems and eigenvectors. All problems count
       as extra credit; there is no penalty (in terms of your overall course grade) for not completing this assignment.</p>

       <b>Please submit a single file <code>a6.*</code> containing your solutions. The file extension may be anything you choose;
       please ask before submitting a file in an exotic or obscure file format (plain text is preferred).</b>
       
<ol>
  <li>
  Consider the following vector subspaces of \R^3:
\begin{eqnarray}
  %V & = & \span { #[ 1 #; 0 #; -2 #] } \\
  %W & = & \span { #[ 4 #; 1 #; 2 #] }
\end{eqnarray}
You are in a spaceship positioned at:
\begin{eqnarray}
  %p & = & #[ 20 #; 15 #; -30 #] 
\end{eqnarray}
    <ol style="list-style-type:lower-alpha;">
      <li> Suppose a transmitter's signal beam only travels in
two opposite directions along %V. What is the shortest distance
your spaceship must travel to intercept the signal beam?
      </li>
      <li> Suppose the transmitter is also rotating around the
axis collinear with %W. What is the shortest distance your
spaceship must travel to reach a position at which it can
hear the signal?
      </li>
    </ol>
  </li>

  <li>
Consider the following underdetermined system:
\begin{eqnarray}
 #[ -2 #, 4 #; 4 #, -8 #] \cdot #[ %x #; %y #] & = & #[ -8 #; 16 #]
\end{eqnarray}
    <ol style="list-style-type:lower-alpha;">
      <li> Define an isomorphism between the solution space %W of the above equation and a parallel vector space %V.
      You must provide a definition of both the vector space and the isomorphism.
      </li>
      <li> Suppose that the cost of a solution vector %v to the above system is defined to be ||%p --- %v|| where
\begin{eqnarray}
 %p & = & #[ 1 #; 5 #]
\end{eqnarray}
           What is the solution to the above system that has the lowest cost?
      </li>
    </ol>
  </li>

  <li> Suppose that vectors in \R^2 represent population quantities in two locations
  (e.g., %x is the number of people in the city and %y is the
number of people in the suburbs), the linear transformation %f:\R^2 \rightarrow \R^2 
   represents a change in the quantities over the course of one year if the economy is doing well, %g:\R^2 \rightarrow \R^2 represents
  a change in the quantities over the course of one year if the economy is not doing well, and
  %v_0 is the initial state:
\begin{eqnarray}
 %f(%v) & = & #[ 2 #, 1 #; 4 #, 2 #] \cdot %v\\
 %g(%v) & = & #[ 0.1 #, 0.2 #; 0.4 #, 0.3 #] \cdot %v\\
 %v_0   & = & #[ 100 #; 200 #]
\end{eqnarray}
  The population state is initially %v_0; after 302 years have passed, the total number of years with a good economy was 100
  and the total number of years with a poor economy was 202. What is the state of the population distribution at this point?
  </li>

 <li> As above, let vectors in \R^2 be system state
descriptions that represent population quantities in two locations, and let %f represent a change in
the quantities over the course of one year if the economy is doing
well, and let %g represents a change in the quantities over the
course of one year if the economy is not doing well:
\begin{eqnarray}
 %f(%v) & = & #[ 60 #, 120 #; 20 #, 40 #] \cdot %v\\
 %g(%v) & = & #[ 0.06 #, 0.12 #; 0.02 #, 0.04 #] \cdot %v\\
 %v_0   & = & #[ 3 #; 1 #] \\
 %v<sub>100</sub>   & = & #[ 300000 #; 100000 #]
\end{eqnarray}

If the initial state is %v_0 and after 100 years
the state is %v<sub>100</sub>, during how many years
of this period was the economy doing well?
  </li>
</ol>
<hr/><br/>

</only132>

<!--/assignment6-->


<a name="A"></a>
<hr style="margin-bottom:80px;"/>
<h2><span class="secn">Appendix A.</span> Notes on tools</h2>

This section contains some useful information and comments about some of the tools you may employ in this course.

<a name="A.1"></a>
<h3> Aartifact </h3>

Aartifact is an interactive logic and algebra verifier developed within the Boston University Computer Science Department that runs inside a web browser.

<b>Syntactic layout of an argument:</b> Each formula in an argument starts on an un-indented line (a line with no spaces
at the beginning). Thus, it is possible to process multiple formulas simultaneously. See example below.

@
2 + 3 = 5

\forall %x,%y \in \R,
  %[
  %x + %y = %y + x
  %]
\forall %x \in \R,
  %[
  0 + %x = %x
  %]
/@

Each formula consists of one or more lines; the first line is unindented, while all subsequent lines in the formula are indented.
Each new line after a quantifier represents a subformula; each new line is assumed to begin with an <b>and</b> logical operator that
connects that subformula to the subformulas on the lines above it.

An <b>implies</b> logical operator should appear on its own line. All lines above <b>implies</b> are <i>premises</i>. All lines below it are the <i>conclusions</i>.

@
\forall %w,%x,%y,%z \in \R,
  %[
    %w & = & %x <comment> this is a premise </comment> \\
    %x & = & %y <comment> this is a premise </comment> \\
    %y & = & %z <comment> this is a premise </comment>
  %]
  \implies
  %[
        %w & = & %z <comment> this line is connected to the next </comment> \\
    %w + 1 & = & %z + 1 <comment> ... by an "and" operation  </comment>
  %]

/@

<b>Output:</b> The results of a verification process are color-coded:
<ul>
  <li style="margin-bottom:9px;"><span style="background-color:lightgreen; padding:4px;">assumption (assumed to be true)</span>;</li>
  <li style="margin-bottom:9px;"><span style="background-color:LightSkyBlue; padding:4px;">true (due to logical and algebraic properties)</span>;</li>
  <li style="margin-bottom:9px;"><span style="background-color:LemonChiffon; padding:4px;">unknown (may be true or false)</span>;</li>
  <li style="margin-bottom:9px;"><span style="background-color:pink; padding:4px;">false</span>;</li>
  <li style="margin-bottom:9px;"><span style="background-color:firebrick; padding:2px;"><span style="background-color:LightSkyBlue; margin:2px;">contradiction (both true and false)</span></span>.</li>
</ul>
  See the example below.

@
\forall %x,%y \in \R,
  %[
    %x = 0 <comment> assumption (assumed to be true) </comment>
  %]
  \implies
  %[
    %x & = & %y <comment> unknown (may be true or false) </comment> \\
    1 & = & 1 <comment> true (due to logical and algebraic properties)</comment> \\
    1 & = & 2 <comment> false </comment> \\
    2 & = & 1 <comment> contradiction (both true and false) </comment>
  %]

/@


<only132>
<a name="A.2"></a>
<h3> Python </h3>

Python is a programming language; an interpreter for Python can be downloaded here:
<a href="http://www.python.org/download/">http://www.python.org/download/</a>.
In this section we present some examples of Python functions.

Below is a function that returns the <code>j</code>th column of a matrix as a vector. You may
find this function useful when completing <a href="#assignment3">Assignment #3</a>.
<pre class="snippet">def matrix_column(m, j):
    # Users will specify j as a matrix column
    # in the range {1,...,n}. Since Python uses
    # 0-indexing, we adjust.
    j = j - 1
    
    # v will be the column vector.
    v = [0 for i in range(0,len(m))]
     
    for i in range(0,len(m)): # for each row
        v[i] = m[i][j]
 
    return tuple(v)
 
matrix_column( ((1,2,3),(4,5,6),(7,8,9)), 2) # Returns (2, 5, 8).</pre>

Below is a more concise definition of the above function.
<pre class="snippet">def matrix_column(m, j):
    return tuple([m[i][j-1] for i in range(0,len(m))])</pre>


Below is one way to implement vector addition.
<pre class="snippet">def plus(u, v):
    if len(u) != len(v):
        return None
 
    w = [0 for x in u]
    for i in range(0,len(u)):
        w[i] = u[i] + v[i]
 
    return tuple(w)
 
# Example.
plus ((1,2), (3,4)) # Returns (4,6).
</pre>

Below is a more concise implementation of both vector addition and vector subtraction.
<pre class="snippet">def plus(u, v):
    if len(u) != len(v):
        return None
 
    return tuple([u[i] + v[i] for i in range(0,len(u))])
 
def minus(u, v):
    if len(u) != len(v):
        return None
 
    return tuple([u[i] - v[i] for i in range(0,len(u))])</pre>

We can use the vector addition function to implement matrix addition.
<pre class="snippet">def plus_matrix(a, b):
    if len(a) != len(b): # Number of rows does not match.
        return None
 
    # We can add the rows using vector addition.
    # Notice we do not need to check that lengths
    # match because it is checked by plus().
    return tuple([plus(a[row], b[row]) for row in range(0,len(a))])</pre>

Below is a more complex function that checks if two vectors with any number of
components are linearly dependent.
<pre class="snippet"># This helper function returns True only if the
# vector consists of all 0 components (i.e., it
# is the origin).
def is_zero(v):
    for i in range(0,len(v)):
        if v[i] != 0:
            return False
    return True
 
# The following function checks if u and v are linearly dependent.
def lin_dep(u, v):
    # Make sure vectors have same number of components,
    # and at least two components.
    if len(u) != len(v) or len(u) < 2 or len(v) < 2:
        return None
 
    # We want to solve v*s = u for s, if possible.
    # We break v*s = u into a series of equations
    # containing s:
    #
    #   u[0] * s = v[0]
    #   u[1] * s = v[1]
    #     ...
    #   u[n] * s = v[n]
    #
    # The same s should work for all of the equations
    # above.
 
    # Until we determine what s should be, we set it
    # to None.
 
    s = None
 
    # If either vector is the (0,...,0) vector,
    # then u*s = v or v*s = u can be solved with s = 0,
    # so they are trivially linearly dependent.
 
    if is_zero(u) or is_zero(v):
        return True
 
    # At this point, s != 0, since the above case
    # handles the s = 0 cases.
 
    # We want to solve v*s = u and s != 0.
    # Look for an equation that determines s, or
    # if s is known, make sure it is a solution for
    # each equation.
    for i in range(0,len(u)):
        if (u[i] == 0 or v[i] == 0) and u[i] != v[i]:
            # One is zero, the other is not;
            # since s != 0, there is no way to
            # solve for s.
            return False
        else:
            if s == None:
                # We haven't found an s yet
                # (only zeroes so far).
                s = u[i] / v[i]
            elif s != u[i] / v[i]:
                return False
 
    return True</pre>

The function below checks if the first vector <code>u</code> is on the line defined
by <code>v</code> and <code>w</code>. This occurs only if the vector <code>minus(v,w)</code>
is linearly dependent with <code>minus(u,w)</code> (or with <code>minus(u,v)</code>).
<pre class="snippet">def vector_on_line_between(u,v,w):
    return lin_dep(minus(v,w), minus(u,w))</pre>



    
</only132>

<!-- <b>Syntax for terms and formulas:</b> 




<a name="A.2"></a>
<h3><span class="secn">A.2.</span> Mathematica/Wolfram Alpha </h3>






<a name="A.3"></a>
<h3><span class="secn">A.3.</span> MATLAB </h3>

-->



<!--eof-->
