
<br/>
<ul>
 <li> <a href="#1">Introduction</a></li>
 <li> <a href="#2">Vectors</a>
  <ul>
   <li><a href="#2.1">Defining operations on vectors</a></li>
   <li><a href="#2.2">Vector spaces</a></li>
   <li><a href="#2.3"><b>Assignment #1: Vector Algebra</b></a></li>
   <li><a href="#2.4">Common vector properties, relationships, and operators</a></li>
   <li><a href="#2.5">Review of Notation: Logical formulas with quantifiers, sets, and set comprehensions.</a></li>
   <li><a href="#2.6">Review: Problems with Vector Algebra</a></li>
   <li><a href="#2.7">Applying Vectors and Linear Combinations to Problems</a></li>
  </ul>
 </li>
 <li> <a href="#3">Matrices</a>
  <ul>
   <li><a href="#3.1">Matrices and multiplication of a vector by a matrix</a></li>
   <li><a href="#3.2.a">Interpreting matrices as tables of relationships and transformations of system states</a></li>
   <li><a href="#3.2.b">Interpreting multiplication of matrices as composition of system state transformations</a></li>  
   <li><a href="#3.3"><b>Assignment #2: Matrix Algebra</b></a></li>
   <li><a href="#3.4">Matrix operations and their interpretations</a></li>
   <li><a href="#3.5">Matrix properties</a></li>
   <li><a href="#3.6">Solving the equation %M %v = %w for %M with various properties</a></li>
   <li><a href="#3.7">Row echelon form and reduced row echelon form</a></li>
   <li><a href="#3.8">Matrix transpose</a></li>
   <li><a href="#3.9">Orthogonal matrices</a></li>
   <li><a href="#3.10">Matrix rank</a></li>
   <li><a href="#3.11">Matrix similarity</a></li>
  </ul>
 </li>
 <li> <a href="#4">Review #1</a> </li>
 <li> <a href="#5">Vector Spaces</a> 
  <ul>
   <li><a href="#5.1">Sets of vectors and their notation</a></li>
   <li><a href="#5.2">Equality of sets of vectors and vector spaces</a></li>
   <li><a href="#5.3">Vector spaces as abstract structures</a></li>
   <li><a href="#5.4">Subspace relation on vector spaces</a></li>
   <li><a href="#5.5">Finding a basis and an orthonormal basis of a vector space</a></li>
   <li><a href="#5.5.b"><b>Assignment #3: Vector Spaces</b></a></li>
   <li><a href="#5.6.a">Homogenous, non-homogenous, overdetermined, and underdetermined systems</a></li>
   <li><a href="#5.6.b">Application: approximating overdetermined systems</a></li>
   <li><a href="#5.6.c">Application: approximating a model of system state relationships</a></li>
   <li><a href="#5.7">The dimension of a vector space</a></li>
   <li><a href="#5.8">Orthogonal complements of vector spaces</a></li>
   <li><a href="#5.9">Algebra of vector spaces</a></li>
  </ul>
 </li>
 <li> <a href="#6">Linear Transformations</a>
  <ul>
   <li><a href="#6.1">Maps (a.k.a., functions) and their properties</a></li>
   <li><a href="#6.2">Homomorphisms and isomorphisms</a></li>
   <li><a href="#6.3">Linear transformations</a></li>
   <li><a href="#6.4">Data modelling as a linear transformation</a></li>
   <li><a href="#6.5">Kernels of linear transformations</a></li>
   <li><a href="#6.6">Matrices as symbolic representations of linear transformations</a></li>
   <li><a href="#6.6.b"><b>Assignment #4: Algebra of Linear Transformations</b></a></a></li>
   <li><a href="#6.7">Orthogonal Projections are Linear Transformations</a></li>
  </ul>
 </li>
 <li> <a href="#7">Review #2</a> </li>
 <li> <a href="#8">Applications to Linear Systems</a>
  <ul>
   <li><a href="#8.1">Interpreting and Using Vector Spaces and Linear Transformations in Applications</a></li>
   <li><a href="#8.2">Communications: Applications of Properties of Linear Transformations and Curve Fitting</a></li>
   <li><a href="#8.3"><b>Assignment #5: Applications: Communication</b></a></a></li>
   <li><a href="#8.4">Modelling Systems of Populations: Applications of Linear Transformations and Eigenvalues</a></li>
  </ul>
 </li> 
 <li> <a href="#9">Review #3</a>
  <ul>
   <li><a href="#9.1">Comprehensive List of Course Topics</a></li>
   <li><a href="#9.2">Practice Problems</a></li>
  </ul>
 </li>
 <li> <a href="#A">Appendix</a>
  <ul>
   <li><a href="#A.1">Logical formulas and quantifiers</a></li>
   <li><a href="#A.2">Set comprehensions</a></li>
   <li><a href="#A.3">Sets and operators</a></li>
   <li><a href="#A.4">Relations, maps, and functions</a></li>
  </ul>
 </li>
</ul>

<a name="lecture1"></a>
<a name="section1"></a>
<hr style="margin-bottom:120px;"/>
<h2>Introduction</h2>

When real-world problems are addressed mathematically, the details of those problems are abstracted away until they
can be represented directly as idealized mathematical objects (e.g., numbers, geometric structures, and so on). In this course, we will
study one collection of such idealized mathematical objects: points, lines, planes, spaces, and the relationships and transformations
between them that preserve linearity, (i.e. the straight characteristic of lines). This particular collection of "linear" objects and relationships can be used to represent many common real-world problems
(and their possible solutions).</p>

In order to study these objects and their relationships, we will define a symbolic language for naming them. We will define
rules for this symbolic language (i.e., a linear <i>algebra</i>) and we will show that these rules correspond to the actual
geometric properties of the objects that the language allows us to describe. We will then practice using this language to solve
problems.

In this course, we will focus on problems that involve some number of degrees of freedom, each of which can be represented as a
linear dimension (i.e., the real numbers). Each of these problems can typically be represented as a system of linear equations. The simplest
examples of such problems should be familiar to most college students. Below is an example.

<b>Example:</b> Suppose we have $12,000 and two investment opportunities: one has an annual return of 10%, and the other has an annual return
of 20%. How much should we invest in each opportunity to get $1800 over one year?

The above problem can be represented as a system of equations with two variables (each of which ranges over the real numbers):

\begin{eqnarray}
 %x + %y & = & 12000 \\
 0.1 %x + 0.2 %y & = & 1800
\end{eqnarray}

The possible solutions to this system are the collection of all pairs of real numbers that can be assigned to (%x,%y). Notice that these can
be interpreted directly as points on a plane.

<a name="2"></a>
<hr style="margin-bottom:120px;"/>
<h2>Vectors</h2>

We will call points in geometric spaces <i>vectors</i>. We first define a notation for names of vectors. Vectors will be written using any
of the the following equivalent notations (there are distinctions between these that will become relevant later).

  $$ (2,3) \ [2;3] $$

<a name="2.1"></a>
<h3> Defining operations on vectors </h3>

We will also use symbols to name some geometric manipulations of vectors. One such operation is addition. Suppose we treat vectors as paths
from (0,0), so that (2,3) is the path from (0,0) to (2,3) and (1,2) is the path from (0,0) to (1,2). Then vector addition would represent
the final destination if someone walks first along the length and direction specified by one vector, and then from that destination along
the length and direction specified by the other. The following definition for the operation + and vectors of two components corresponds
to this intuition.

@
\forall %x, %y, %x', %y' \in \R,
  %[
  [%x; %y] + [ %x'; %y'] & = & [%x + %x'; %y + %y']
  %]
/@

Notice that we have defined an entirely new operation; we are merely reusing (or "overloading") the + symbol to represent this operation.
We cannot assume that this operation has any of the properties we normally associate with addition of real numbers. However, we do know
that according to our interpretion of vector addition (walking along one vector, then along the other from that destination), this operation
should be commutative. Does our symbolic definition conform to this interpretation? If it does, then we should be able to show that
[%x;%y] + [%x';%y'] and [%x+%x';%y+%y'] are names for the <i>same</i> vector. Using the commutativity of the real numbers and our definition
above, we can indeed write the proof of this property.

@
\forall %x, %y, %x', %y' \in \R,
  %[
  [%x; %y] + [%x'; %y'] & = & [%x + %x'; %y + %y'] \and \\
                     `` & = & [%x' + %x; %y' + %y] \and \\
                     `` & = & [%x'; %y'] + [%x; %y]  
  %]
/@

Recall that we can view multiplication by a number as repeated addition. Thus, we could use this intuition and our new notion of vector addition
defined above to define multiplication by a real number; in this context, it will be called a <i>scalar</i>, and the operation is known as
<i>scalar multiplication</i>.

@
\forall %s,%x,%y \in \R,
  %[
  s [%x; %y] = [%s (%x); %s (%y)]
  %]
/@

<a name="lecture2"></a>

Scalar multiplication has some of the intuitive algebraic properties that are familiar to us from our experience with \R. Below, we provide
proofs of a few.

@
\forall %s, %t, %x, %y \in \R,
  %[
  %s (%t [%x; %y]) & = & %s ([%t %x; %t %y]) \and \\
           ``  & = & [ %s (%t %x); %s (%t %y) ] \and \\
           ``  & = & [ (%s %t) %x; (%s %t) %y ] \and \\
           ``  & = & [ (%t %s) %x; (%t %s) %y ] \and \\
           ``  & = & [ %t (%s %x); %t (%s %y) ] \and \\
           ``  & = & %t [ %s %x; %s %y ] \and \\
           ``  & = & %t (%s ( [%x; %y])) \and
  %]

  # alternatively, we could also derive...
                                 
  %[
 [ %s (%t %x); %s (%t %y) ] & = & [ (%s %t) %x; (%s %t) %y ] \and \\
                       ``   & = & (%s %t) [%x;%y]
  %]
/@

<a name="2.2"></a>
<h3> Vector spaces </h3>

In this course, we will study collections of vectors that have particular properties. When a collection of vectors satisfies these properties,
we will call it a <i>vector space</i>. We list these eight properties (in the form of equations) below. These must hold for any %u, %v, and %w
in the collection, and \0 must also be a vector in the collection.

\begin{eqnarray}
  %u + (%v + %w) & = & (%u + %v) + %w \\
       %u + %v  & = & %v + %u \\
      \0 + %v  & = & %v \\
       %v + (-%v) & = & \0 \\
        1 * %v & = & %v \\     
 %s * (%u + %v) & = & (%s * %u) + (%s * %v)\\
 (%u + %v) * %s & = & (%u * %s) + (%v * %s)\\
 %s * (%t * %u) & = & (%s * %t) * %u
\end{eqnarray}

Note that these are not unique; we could have specified an alternative equation for the additive identity.

  $$ %v + \0 = %v $$

Each equation can be derived from the other using the commutativity 

  $$ %v + \0 = \0 + %v $$

These eight equations can be called <i>axioms</i> (i.e., assumptions) when we are studying vector spaces without thinking about the internal
representation of vectors. For example, the above derivation of the alternative additive identity equation is based entirely on the axioms
and will work for any vector space. However, to show that some <i>other</i> structure, such as \R^2, is a vector space we must prove that these
properties are satisfied. Thus, to define a new vector space, we usually must:
<ol>
<li>define a way to construct vectors;</li>
<li>define a vector addition operator + (i.e., how to add vectors that we have constructed);</li>
<li>specify the vector that is the additive identity (call it <b>0</b>);</li>
<li>define vector inversion: a process for constructing the inverse of a vector;</li>
<li>prove that the eight properties of a vector space are satisfied by the vectors, +,0, and *.
</ol>

<!--assignment1-->
<br/><hr/>
<a name="2.3"></a>
<a name="assignment1"></a>
<b>Assignment #1: Vector Algebra</b> <!--span class="btn_assignment">(<a href="materials.php?hw=1">show only this assignment</a>)</span-->

       <p>In this assignment you will perform step-by-step algebraic manipulations involving vectors and vector operations. In doing
       so, you will assemble machine-verifiable proofs of algebraic facts. Please submit your solutions by email in a single text file.
       <p> Working with machine verification can be frustrating; minute operations that are normally implied must often be explicit.
       However, the process should familiarize you with common practices of rigorous formal reasoning in mathematics: finding, applying,
       or expanding definitions to reach a particular formula that represents a desired argument or solution.</p>

<ol>
  <li>
    <ol style="list-style-type:lower-alpha;">
      <li> Finish the proof below by solving for <i>a</i> using a sequence of permitted algebraic manipulations.

@
\forall %a, %b \in \R,
    %[
    [%a; 4] = [%b + 3; %b]
    %]
  \implies

    # put proof steps here
    %[
    %a = \undefined  # should be an integer
    %]
/@

      </li>
      
      <li> Finish the proof below by solving for <i>b</i> in terms of <i>a</i>.

@
\forall %a,%b \in \R,
  %[
  [11; 24] = [3; 7] %a  + [1; 2] %b
  %]
  \implies
  # ???
  %[
  %b = \undefined # should be an expression with "a"
  %]
/@

      </li>
      <li> Finish the proof below to show that [1;2] and [-2;1] are linearly independent.

@
%[
[1; 2] * [-2; 1] = 1 * -2 + 2 * 1 \and
%]
  # ???
%[
  `([1; 2]) and ([-2; 1]) are orthogonal`
%]
/@

      </li>
      <li> Solve for <i>x</i> (the solution is an integer).

@
\forall %x \in \R,
    %[
    `([%x; 4]) and ([1; 1]) are orthogonal`
    %]
  \implies
    # ???
    %[
    %x = \undefined
    %]
/@

      </li>
      <li> Show that no [%x; %y] exists satisfying both of the below properties by deriving a contradiction (e.g., <code>1 = 0</code>).

@
\forall %x,%y \in \R,
    %[
    `([%x; %y]) is a unit vector` \and \\
    `([%x*%x; %y*%y]) and ([1; 1]) are  orthogonal`
    %]
  \implies
    # ???
    \undefined # this should be a contradiction
/@

      </li>
    </ol>
  </li>
  <li> We have shown in lecture that \R^2, together with vector addition and scalar multiplication, satisfies some of the vector space axioms.
       In this problem, you will show that the remaining axioms are satisfied.
    <ol style="list-style-type:lower-alpha;">
      <li> Finish the proof below showing that [0; 0] is a <i>left</i> identity for addition of vectors in \R^2.

@
\forall %x,%y \in \R,
  # ???
  %[
  [0; 0] + [%x; %y] = [%x; %y]
  %]
/@

      </li>
      <li> Finish the proof below showing that [0; 0] is a <i>right</i> identity for addition of vectors in \R^2.

@
\forall %x,%y \in \R,
  # ???
  %[
  [%x; %y] + [0; 0] = [%x; %y]
  %]
/@

      </li>
      <li> Finish the proof below showing that 1 is the identity for scalar multiplication of vectors in \R^2.

@
\forall %x,%y \in \R,
  # ???
  %[
  1 * [%x; %y] = [%x; %y]
  %]
/@

      </li>
      <li> Finish the proof below showing that the component-wise definition of inversion is consistent with the inversion axiom for vector spaces.

@
\forall %x,%y \in \R,
  # ???
  %[
  [%x; %y] + (-[%x; %y]) = [0; 0]
  %]
/@

      </li>
      <li> Finish the proof below showing that the addition of vectors in \R^2 is associative.

@
\forall %a,%b,%c,%d,%e,%f \in \R,
  # ???
  %[
  [%a; %b] + ([%c; %d] + [%e; %f]) = ([%a; %b] + [%c; %d]) + [%e; %f]
  %]
/@

      </li>
      <li> Finish the proof below showing that the distributive property applies to vector addition and scalar multiplication for \R^2.

@
\forall %s, %x, %y, %x', %y' \in \R,
  # ???
  %[
  %s ([%x; %y] + [%x'; %y']) = (%s [%x; %y]) + (%s [%x'; %y'])
  %]
/@

      </li>
    </ol>
  </li>
  <li> <p>Any point %p on the line between vectors %u and %v can be expressed as %a(%u-%v)+%u for some scalar %a. In fact, it can also be
       expressed as %b(%u-%v)+%v for some other scalar %b. In this problem, you will prove this fact for \R^3.</p>
       
       <p>Given some %p = %a(%u-%v)+%u, find a formula for %b in terms of %a so that %p = %b(%u-%v)+%v. Add this formula to the beginning
       of the proof (after <code>a = </code>) and then complete the proof.</p>

@
\forall %a,%b \in \R, \forall %u,%v,%p \in \R^3,
    %[
    %p & = & %a(%u-%v) + %u \and \\
    %a & = & \undefined # define a in terms of b
    %]
  \implies
    # ???
    %[
    %p = %b (%u-%v) + %v
    %]
/@

       <p>The verifier's library contains
       a variety of derived algebraic properties for \R and \R^3 in addition to the vector space axioms for \R^3. Look over them
       to see which might be useful.</p>
  </li>
</ol>
<hr/><br/>
<!--/assignment1-->

<a name="lecture3"></a>
<a name="2.4"></a>
<h3> Common vector properties, relationships, and operators </h3>

We introduce two new operations on vectors (e.g. %u and %v): the norm (|| v ||) and the dot product (%u \cdot %v). When interpreting some other structure,
such as \R^2, as a vector space, we must provide definitions for these operators in order to use them.

\begin{eqnarray}
 [%x; %y] \cdot [%x'; %y'] & = & %x*%x' + %y*%y' \\
 || [%x;%y] || & = & &radic;(%x*%x + %y*%y)
\end{eqnarray}

Notice that the dot product is a new, distinct form of multiplication (distinct from multiplication of real numbers, and distinct from scalar multiplication). Also
notice that the two operations are related:

  $$||[%x; %y] || = &radic;(%x*%x + %y*%y) = &radic;([%x; %y] \cdot [%x; %y])$$

These operations can be shown to have various algebraic properties. For example, the dot product is commutative.

@
\forall %a,%b,%c,%d \in \R,
  %[
  [%a; %b] * [%c; %d] & = & %a*%c + %b*%d \and \\
               `` & = & %c*%a + %d*%b \and \\
               `` & = & [%c; %d] * [%a; %b]
  %]
/@

We also introduce several vector properties. Some deal with a single vector; some deal with two vectors; and some deal with three vectors.

The table below summarizes the vector properties and relationships and how they are related in some cases to vector operators and associated algebraic properties.

<table class="fig_table">
 <tr>
  <td><b>property</b></td>
  <td><b>definition</b></td>
  <td><b>algebraic properties for \R^2 <br/>%u = [%x;%y], %v = [%x',%y'], %w = [%x'',%y'']</b></td>
 </tr>
 <tr> 
  <td>%v has length %s</td>
  <td>||v|| = %s or<br/> &radic;(%v \cdot %v) = %s</td>
  <td>||v|| = &radic;(%x*%x + %x*%y) = &radic;([%x,%y] \cdot [%x,%y])</td>
 </tr>
 <tr>
  <td>%v is a unit vector</td>
  <td>||%v|| = 1 or <br/> %v \cdot %v = 1</td>
  <td>1 = ||v|| = &radic;(%x*%x + %x*%y) = &radic;([%x,%y] \cdot [%x,%y]) <br/> 1 = ||v|| = %x*%x + %x*%y = [%x,%y] \cdot [%x,%y]</td>
 </tr>
 <tr>
  <td>%u and %v are linearly dependent <br/> %u and %v are collinear</td>
  <td>&exist; %a \in \R, %a %u = %v</td>
  <td>%y/%x = %y'/%x'</td>
 </tr>
 <tr>
  <td>%u and %v are linearly independent</td>
  <td>&forall; %a \in \R, %a %u &ne; %v</td>
  <td>%y/%x &ne; %y'/%x'</td>
 </tr>
 <tr>
  <td>%u and %v are orthogonal</td>
  <td>%u \cdot %v = 0</td>
  <td>%y/%x = -%x'/%y'</td>
 </tr>
 <tr>
  <td>%w is a projection of %u onto %v</td>
  <td>%w = %v \cdot ((%u \cdot %v)/||%v||)</td>
  <td></td>
 </tr>
 <tr>
  <td>%w is a linear combination of %u and %v</td>
  <td>&exist; %a,%b \in \R, %w = %a%u + %b%v</td>
  <td></td>
 </tr>
</table>

In \R^2, we can derive the linear independence of [%x;%y] and [%x';%y'] from the orthogonality of [%x;%y] and [%x';%y'] using a proof
by contradiction. Consider the following.

\begin{eqnarray}
 %y/%x & = & -%x'/%y' \\
 %y/%x * %y'/%x' & = & -1
\end{eqnarray}

Now, suppose that [%x;%y] and [%x';%y'] are <i>not</i> linearly independent. Then they are linearly dependent, so %y/%x = %y'/%x'.
But this means that:

\begin{eqnarray}
 (%y/%x * %y/%x) & = & -1 \\
 (%y/%x)<sup>2</sup> & = & -1 \\
 %y/%x & = & &radic;(-1)
\end{eqnarray}

No real numbers %y and %x satisfy the above equation, so we must have introduced a contradiction by supposing that [%x;%y] and [%x';%y'] 
are <i>not</i> linearly independent. Thus, they must be linearly independent.

<a name="2.4"></a>
<h3>Some properties and operators of sets of vectors</h3>

Properties that can hold between two vectors (e.g. orthogonality) can often be generalized to
<i>sets</i> of vectors. One way to do so is to require that for every pair of vectors in a set,
the property holds.

<table class="fig_table">
 <tr>
  <td><b>property</b></td>
  <td><b>definition</b></td>
 </tr>
 <tr>
  <td>{%w_1, ..., %w_n} are pairwise linearly dependent</td>
  <td>&forall; %u,%v \in {%w_1, ..., %w_n},<br/>&nbsp;&nbsp; %u and %v are linearly dependent</td>
 </tr>
 <tr>
  <td>{%w_1, ..., %w_n} are pairwise linearly independent</td>
  <td>&forall; %u,%v \in {%w_1, ..., %w_n},<br/>&nbsp;&nbsp; %u and %v are linearly independent</td>
 </tr>
 <tr>
  <td>{%w_1, ..., %w_n} are pairwise orthogonal</td>
  <td>&forall; %u,%v \in {%w_1, ..., %w_n},<br/>&nbsp;&nbsp; %u and %v are orthogonal</td>
 </tr>
 <tr>
  <td>{%w_1, ..., %w_n, ...} are linear combinations of %u and %v <br/>
      {%w_1, ..., %w_n, ...} are linear combinations of %u and %v<br/>
      {%w_1, ..., %w_n, ...} is the span of %u and %v<br/>
      <b>span</b>{%u,%v} = {%w_1, ..., %w_n, ...}
  </td>
  <td>{ %w | &exist; %a,%b \in \R, %w = %a%u + %b%v }</td>
 </tr>
</table>

Notice that the <b>span</b> operator can be used in equations.

<b>Example:</b> Find a pair of vectors in \R^2 (not necessarily distinct) that solve the following equation:

  $$<b>span</b>{%v,%w) = %v$$

The only solution is %v = [0; 0], %w = [0; 0].

We can also define setwise versions of the above properties. We reproduce them below, but we do not yet need to
understand them. We will come back to them when we study vector spaces.

<table class="fig_table">
 <tr>
  <td><b>property</b></td>
  <td><b>definition</b></td>
 </tr>
 <tr>
  <td>{w_1, ..., %w_n, ...} are linear combinations of %V<br/>
      {%w_1, ..., %w_n, ...} is the span of %V<br/>
      <b>span</b> %V = {%w_1, ..., %w_n, ...}
  </td>
  <td>{ %w | &exist; %a_1,...,%a_n \in \R, %v_1,...,%v_n \in %V, %w = %a_1 %v_1 + ... + %a_n %v_n }</td>
 </tr>
 <tr>
  <td>{%w_1, ..., %w_n} are (setwise) linearly dependent</td>
  <td>\forall %w \in {%w_1, ..., %w_n}, %w \in \span({%w_1, ..., %w_n} - %w)</td>
 </tr>
 <tr>
  <td>{%w_1, ..., %w_n} are (setwise) linearly independent</td>
  <td>\forall %w \in {%w_1, ..., %w_n}, %w \not\in \span({%w_1, ..., %w_n} - %w)</td>
 </tr>
 <tr>
  <td>{%w_1, ..., %w_n} are (setwise) orthogonal</td>
  <td>{%w_1, ..., %w_n} are pairwise orthogonal</td>
 </tr>
</table>

One thing we might choose to note: given what we know so far, how could we approach determining whether a given set
of vectors is linearly independent? Is this approach practical or efficient?

<a name="lecture4"></a>
<a name="2.5"></a>
<h3>Review of Notation: Logical formulas with quantifiers, sets, and set comprehensions.</h3>

Consult <a href="#A.1">Section A.1</a> for a review of logical formulas and quantifiers, and <a href="#A.1">Section A.2</a> for a review
of set comprehensions.

<a name="2.6"></a>
<h3>Review: Problems with Vector Algebra</h3>

We review the operations and properties of vectors introduced in this section by considering several example problems.

<b>Example:</b> Given the points %u = [%x_1,%y_1] and %v = [%x_2,%y_2], find the equation of the line between these two points in the
form %y = %m%x + %b.

We recall the definition for a line defined by two points:

  $${ %p | \exists %a \in \R, %p = %a (%u - %v) + %u }.$$

Thus, if [%x; %y] is on the line, we have:

  $$ [%x; %y] = %a ([%x_1,%y_1] - [%x_2,%y_2]) + [%x_1,%y_1] $$

This implies the following system of equations (one from the %x components in the above, and one from the %y components):

\begin{eqnarray}
 %x & = & %a (%x_1 - %x_2) + %x_1 \\
 %y & = & %a (%y_1 - %y_2) + %y_1
\end{eqnarray}

If we solve for %a in terms of %x, we can recover a single equation for the line:

\begin{eqnarray}
 %a & = & (%x - %x_1)/(%x_1 - %x_2) \\
 %y & = & ((%x - %x_1)/(%x_1 - %x_2)) (%y_1 - %y_2) + %y_1 \\
 %y & = & ((%y_1 - %y_2)/(%x_1 - %x_2)) (%x - %x_1)  + %y_1
\end{eqnarray}

Notice that we can set %m = (%y_1 - %y_2)/(%x_1 - %x_2) because that is exactly the slope of the line between [%x_1,%y_1] and [%x_2,%y_2].

\begin{eqnarray}
 %y & = & %m (%x - %x_1)  + %y_1 \\
 %y & = & %m%x - %m %x_1 + %y_1
\end{eqnarray}

We see that we can set %b = - (%m %x_1 + %y_1).

  $$ %y = %m%x + %b $$

<b>Example:</b> Define the line that is orthogonal to the vector [%x; %y] but also crosses [%x; %y] (i.e, [%x; %y] falls on the line).

We know that the line must be parallel to the vector that is orthogonal to [%x; %y]. The line crossing [0; 0] that is orthogonal
to [%x; %y] is defined as:

  $${ %p | [%x; %y] \cdot %p = 0 }.$$

However, we need the line to also cross the point [%x; %y]. This is easily accomplished by adding the vector [%x; %y] to all the points
on the orthogonal line going through [0; 0] (as defined above). Thus, we have:

  $${ %p + [%x; %y] | [%x; %y] \cdot %p = 0 }.$$



<!-- If we wanted to write down the equations for %x and %y that define this line -->



<b>Example:</b> Is [7; -1] on the line defined by the points %u = [19; 7] and %v = [1; -5]?

To solve this problem, we recall the definition for a line defined by two points:

  $${ %p | \exists %a \in \R, %p = %a (%u - %v) + %u }.$$

Thus, we want to know if [7; -1] is in the set defined as above. This can only occur if there exists
an %a such that [7; -1] = %a (%u - %v) + %u.

We solve for %a; if no solution exists, then the point [7; -1] is not on the line %L. If an %a exists, then it is. In this case,
%a = 1/3 is a solution to both equations, so [7; -1] is on the line.

\begin{eqnarray}
 [7; -1] & = & %a([19; 7] - [1; -5]) + [1; -5]\\
         & = & ([19%a - 1%a; 7%a + 5%a]) + [1; -5]\\
         & = & ([18%a+1; 12%a-5])\\
       7 & = & 18%a+1\\
      -1 & = & 12%a-5\\
      %a & = & 1/3
\end{eqnarray}

<b>Example:</b> Is [8; -6] a linear combination of the vectors [19; 7] and [1; -5]?

We recall the definition of a linear combination and instantiate it for this example:

  $$\exists %a,%b \in \R, [8; -6] = %a [19; 7] + %b [1; -5].$$

Thus, if we can solve for %a and %b, then [8; -6] is indeed a linear combination.

\begin{eqnarray}
 [8; -6] & = & %a [19; 7] + %b [1; -5]\\
         & = & [19%a ; 7%a ] +  [1%b; -5%b]\\
       8 & = & 19%a + %b \\
       %b & = & -19%a + 8 \\
       -6 & = & 7%a - 5%b \\
       -6 & = & 7%a - 5`(-19%a + 8) \\
       -6 & = & 7%a + 95%a - 40 \\
       34 & = & 102%a \\
       %a & = & 1/3 \\
       %b & = & -19/3 + 8 = 5/3
\end{eqnarray}

<b>Example:</b> Given %v = [15; 20], list the vectors that are orthogonal to %v, but of the same length as %v.

The two constraints on the vectors [%x;%y] we seek are:

\begin{eqnarray}
 [15; 20] \cdot [%x; %y] & = & 0\\
 ||[15; 20]||  & = & ||[%x; %y]||
\end{eqnarray}

We take the first constraint and solve for %x in terms of %y.

\begin{eqnarray}
 [15; 20] \cdot [%x; %y] & = & 0\\
 15%x + 20%y & = & 0\\
 %x +  & = & (-20/15)%y
\end{eqnarray}

We now plug this into the second equation.

\begin{eqnarray}
 ||[15;20]||  & = & ||[(-20/15)%y;%y]|| \\
 \sqrt(15^2 + 20^2) & = & \sqrt((400/225)%y^2 + y^2) \\
 \sqrt(625) & = & \sqrt((625/225)%y^2) \\
 625 & = & (625/225)%y^2 \\
 225 & = & %y^2 \\
 \pm 15 & = & %y
\end{eqnarray}

Thus, the vectors are (-20, 15) and (20,-15).


<b>Example:</b> Are the vectors [2; 1] and [3; 2] pairwise linearly independent?

There are at least two ways we can proceed in checking pairwise linear independence. Both involve checking if the vectors
are linearly dependent. If they are linearly dependent, then they cannot be linearly independent. If they are not linearly
dependent, they must be linearly independent.

We can compare the slopes; we see they are different, so they must be linearly independent.

  $$ 1/2 \neq 2/3 $$

We can also use the definition of linear dependence. If they are linearly dependent, then we know that

  $$ \exists %a \in \R, %a [2; 1] = [3; 2]. $$

Does such an %a exist? We try to solve for it:

\begin{eqnarray}
 %a[2; 1] & = & [3; 2] \\
 2%a & = & 3 \\
 %a & = & 2 \\
  4 & = & 3
\end{eqnarray}

Since we derive a contradiction, there is no such %a, so the two vectors are not linearly dependent, which means they
are linearly independent.

<b>Example:</b> Are the vectors %V = {[2; 0; 4; 0], [6; 0; 4; 3], [1; 7; 4; 3]} pairwise linearly independent?

The definition for pairwise linear independence requires that all pairs of vectors being considered are
linearly independent with each other. Thus, we must check all pairs. Because linear indepence of two
vectors is a symmetric relation, we only need to consider 3!/2! = 3 combinations. 

For each combination,
we can check whether the pair is linearly dependent. If it is linearly dependent, we can stop and say that the three vectors are not
linearly independent. If it is not, we must continue checking all the pairs. If all the pairs are not
linearly independent, then the three vectors are pairwise linearly independent.

  $$ not (\forall %u,%v \in %V, %u and %v are linearly independent) &nbsp;&nbsp;iff&nbsp;&nbsp; (\exists %u,%v \in %V, %u and %v are linearly dependent)$$

Notice that this an example of a general logical rule:

  $$ not (\forall %x \in %S, %p) &nbsp;&nbsp;iff&nbsp;&nbsp; (\exists %x \in %S, not %p)$$

<b>Note: This is the procedure for <i>pairwise</i> linear independence, which is distinct from <i>setwise</i>
linear independence. In that case, we would need to apply the definition of setwise linear independence, which
would mean that for each vector, we would need to determine whether it is a linear combination of the other
two vectors.</b> 

We find that for all three pairs, assuming linear dependence leads to a contradiction.

\begin{eqnarray}
 %a [2; 0; 4; 0] & = & [6; 0; 4; 3] \\
 2 %a & = & 6 \\
 0 %a & = & 3
\end{eqnarray}

\begin{eqnarray}
 %a [2; 0; 4; 0] & = & [1; 7; 4; 3] \\
 2 %a & = & 1 \\
 0 %a & = & 3
\end{eqnarray}

\begin{eqnarray}
 %a [6; 0; 4; 3] & = & [1; 7; 4; 3] \\
 6 %a & = & 1 \\
 0 %a & = & 7
\end{eqnarray}

Thus, %V is pairwise linearly independent.

<b>Example:</b> Given constants %a,%b,%c \in \R, find a vector orthogonal to the plane defined by {(%x,%y,%z) | %a(%x+%y+%z) + %b(%y+%z) + %c%z = 0}.

We only need to rewrite the equation defining the plane in a more familiar form.

\begin{eqnarray}
 %a(%x+%y+%z) + %b(%y+%z) + %c%z & = & 0\\
 %a%x + (%a+%b)%y + (%a+%b+%c)%z & = & 0\\
 (%a,%b,%c) \cdot (%x,%y,%z) & = & 0
\end{eqnarray}

In order to be orthogonal to a plane, a vector must be orthogonal to all vectors (%x,%y,%z) on that plane.
Since all points on the plane are orthogonal to (%a,%b,%c) by definition, (%a,%b,%c) is such a point.

<a name="lecture5"></a>
<a name="2.7"></a>
<h3>Applying Vectors and Linear Combinations to Problems</h3>

In the introduction we noted that in this course, we would define a symbolic language for working with a certain
collection of idealized mathematical objects that can be used to model aspects of real-world problems in an abstract
way. Because we are considering a particular collection of objects (vectors, planes, spaces, and their relationships),
it is natural to ask what kinds of problems are well-suited for such a representation (and also what problems are
not well-suited).

What situations and associated problems can be modelled using vectors and related operators and properties?
Problems involving concrete objects that have a position, velocity, direction, geometric shape, and relationships between
these (particularly in two or three dimensions) are natural candidates. For example, we have seen that it is possible
to compute the projection of one vector onto another. However, these are just a particular example of a more general
family of problems that can be studied using vectors and their associated operations and properties.

A vector of real numbers can be used to represent an object or collection of objects with some fixed number of characteristics (each
corresponding to a dimension or component of the vector) where each characteristic has a range of possible values.
This range could be a set of magnitudes (e.g., position, cost, mass), a discrete collection of states (e.g.,
absence or presence of an edge in a graph), or even a set of relationships (e.g., for every cow, there are four
cow legs; for every $1 invested, there is a return of $0.02). Thus, vectors are well-suited for representing 
problems involving many instances of objects where all the objects have the same set of possible characteristics
along the same set of linear dimensions. In these instances, many vector operations also have natural interpretations.
For example, addition and scalar multiplication (i.e., linear combinations) typically correspond to the aggregation
of a property across multiple instances or copies of objects with various properties (e.g., the total mass of a
collection of objects).

In order to illustrate how vectors and linear combinations of vectors might be used in applications, we introduce the
notion of a <i>system</i>. A <i>system</i> is any physical or abstract phenomenon, or observations of a phenomenon, that
we characterize as a collection of real values along one or more <i>dimensions</i>. A <i>state</i> of a system is a particular collection
of real values. For example, if a system is represented by \R^4, states of that system are represented by individual vectors
in \R^4 (note that not all vectors need to correspond to valid or possible states; see the examples below).

<b>Example:</b> Consider the following system: a barn with cows and chickens inside it. There are several dimensions along
which an observer might be able to measure this system (we assume that the observer has such poor eyesight that chickens and
cows are indistinguishable from above):
<ul>
 <li>number of chickens inside</li>
 <li>number of cows inside</li>
 <li>number of legs that can be seen by peeking under the door</li>
 <li>number of heads that can be seen by looking inside from a high window</li>
</ul>
Notice that we could represent a particular <i>state</i> of this system using a vector in \R^4. However, notice also that
many vectors in \R^4 will <i>not</i> correspond to any system that one would expect to observe. Usually, the number of
legs and heads in the entire system will be a <i>linear combination</i> of two vectors: the number of legs per cow,
and the number of legs per chicken:

\begin{eqnarray}
 #[ 1 <i style="color:gray;">head</i> #; 2 <i style="color:gray;">legs</i> #] \cdot 
   %x <i style="color:gray;">chickens</i> + 
      #[1 <i style="color:gray;">head</i> #; 4 <i style="color:gray;">legs</i> #] \cdot %y <i style="color:gray;">cows</i> 
      & = &  #[ %x+%y <i style="color:gray;">heads</i>#; 2%x+4%y <i style="color:gray;">legs</i> #]
\end{eqnarray}
 
<br/>Given this relationship, it may be possible to derive some characteristics of the system given only partial information.
Consider the following problem: how many chickens and cows are in a barn if 8 heads and 26 legs were observed?<br/><br/>

\begin{eqnarray}
 #[ 1 <i style="color:gray;">head</i> #; 2 <i style="color:gray;">legs</i> #] \cdot 
   %x <i style="color:gray;">chickens</i> +
      #[1 <i style="color:gray;">head</i> #; 4 <i style="color:gray;">legs</i> #] \cdot %y <i style="color:gray;">cows</i> 
      & = & #[ 8 <i style="color:gray;">heads</i>#; 26 <i style="color:gray;">legs</i> #]
\end{eqnarray}

Notice that a linear combination of vectors can be viewed as a translation from a vector describing one set of
dimensions to a vector describing another set of dimensions. Many problems might exist in which the values are
known along one set of dimensions and unknown along another set.

<b>Example:</b> We can restate the example from the introduction using linear combinations. Suppose we have $12,000 and two
investment opportunities: A has an annual return of 10%, and B has an annual return of 20%. How much should we invest
in each opportunity to get $1800 over one year?

The two investment opportunities are two-dimensional vectors representing the rate of return on a dollar:

\begin{eqnarray}
     #[ 1 <i style="color:gray;">dollar</i> #; 0.1 <i style="color:gray;">interest</i> #] and #[ 1 <i style="color:gray;">dollar</i> #; 0.2 <i style="color:gray;">interest</i> #]
\end{eqnarray}

The problem is to find what combination of the two opportunities would yield the desired observation of the entire system:

\begin{eqnarray}
    #[ 1 <i style="color:gray;">dollar</i> #; 0.1 <i style="color:gray;">dollars of interest</i> #] 
    \cdot x <i style="color:gray;">dollars in opportunity A</i> 
    + #[ 1 <i style="color:gray;">dollar</i> #; 0.2 <i style="color:gray;">dollars of interest</i> #] 
     \cdot y <i style="color:gray;">dollars in opportunity B</i> 
     & = & #[ 12,000 <i style="color:gray;">dollars</i> #; 1800 <i style="color:gray;">dollars of interest</i> #]
\end{eqnarray}

Next, we consider a problem with discrete dimensions.

<b>Example:</b> Suppose there is a network of streets and intersections and the city wants to set up cameras
at some of the intersections. Cameras can only see as far as the next intersection. Suppose there are five
streets (#1, #2, #3, #4, #5) and four intersections (A, B, C, and D) at which cameras can be placed, and the city
wants to make sure a camera can see every street while not using any cameras redundantly (i.e., two cameras
should not film the same street).

Vectors in \R^5 can represent which streets are covered by a camera. A fixed collection of vectors, one for each
intersection, can represent what streets a camera can see from each intersection. Thus, the system's dimensions
are:
<ul>
 <li>is street #1 covered by a camera?</li>
 <li>is street #2 covered by a camera?</li>
 <li>is street #3 covered by a camera?</li>
 <li>is street #4 covered by a camera?</li>
 <li>is street #5 covered by a camera?</li>
 <li>is there a camera at intersection A? (represented by the variable %a below)</li>
 <li>is there a camera at intersection B? (represented by the variable %b below)</li>
 <li>is there a camera at intersection C? (represented by the variable %c below)</li>
 <li>is there a camera at intersection D? (represented by the variable %d below)</li>
</ul>
Four fixed vectors will be used to represent which streets are adjacent to which intersections:

\begin{eqnarray}
    #[0 #; 1 #; 0 #; 0 #; 1 #] , 
    #[1 #; 0 #; 1 #; 1 #; 0 #] ,
    #[1 #; 1 #; 0 #; 0 #; 0 #] ,
    #[1 #; 0 #; 0 #; 1 #; 1 #]
\end{eqnarray}

Placing the cameras in the way required is possible if there is integer solution to the following equation
involving a linear combination of the above vectors:

\begin{eqnarray}
    #[0 #; 1 #; 0 #; 0 #; 1 #] %a +  #[1 #; 0 #; 1 #; 1 #; 0 #] %b + #[1 #; 1 #; 0 #; 0 #; 0 #] %c + #[1 #; 0 #; 0 #; 1 #; 1 #] %d 
      & = & #[1 #; 1 #; 1 #; 1 #; 1 #]
\end{eqnarray}

<b>Example:</b> Suppose a chemist wants to model a chemical reaction. The dimensions of the system might be:
<ul>
 <li>how many molecules of C_3H_8 are present?</li>
 <li>how many molecules of O_2 are present?</li>
 <li>how many molecules of CO_2 are present?</li>
 <li>how many molecules of H_2O are present?</li>
 <li>how many atoms of carbon are present?</li>
 <li>how many atoms of hydrogen are present?</li>
 <li>how many atoms of oxygen are present?</li>
</ul>

Individual vectors in \R^3 can be used to represent how many atoms of each element are in each type of
molecule being considered:

\begin{eqnarray}
  C_3H_8: #[ 3 #; 8 #; 0 #] ,\~ O_2:  #[ 0 #; 0 #; 2 #] ,\~ CO_2: #[ 1 #; 0 #; 2 #] ,\~ H_2O: #[ 0 #; 2 #; 1 #]
\end{eqnarray}

Suppose we know that the number of atoms in a system may never change during a reaction, and that some quantity
of C_3H_8 and O_2 can react to yield only CO_2 and H_2O. How many molecules of each compound will be involved in the reaction? That is the solution
to the following linear combination.

\begin{eqnarray}
  #[ 3 #; 8 #; 0 #] %x_1 + #[ 0 #; 0 #; 2 #] %x_2  & = & #[ 1 #; 0 #; 2 #] %x_3 + #[ 0 #; 2 #; 1 #] %x_4
\end{eqnarray}

The notion of a linear combination of vectors is common and can be used to mathematically model a wide variety of problems.
Thus, a more concise notation for linear combinations of vectors would be valuable. 

<a name="3"></a>
<hr style="margin-bottom:120px;"/>
<h2>Matrices</h2>

<a name="3.1"></a>
<h3>Matrices and multiplication of a vector by a matrix</h3>

Matrices are a concise way to represent and reason about linear combinations and linear independence of vectors (e.g., setwise linear
independence might be difficult to check using an exhaustive approach), reinterpretations of systems using different dimensions,
and so on. One way to interpret a matrix is as a collection of vectors. Multiplying a matrix by a vector corresponds to computing a linear
combination of that collection of vectors.

As an example, we consider the case for linear combinations of two vectors. The two scalars in the linear combination
can be interpreted as a 2-component vector. We can then put the two vectors together into a single object in our
notation, which we call a <i>matrix</i>.

\begin{eqnarray}
  #[%a#,%b#;%c#,%d#] \cdot #[%x#;%y#] & = & #[%a#;%c#] \cdot %x + #[%b#;%d#] \cdot %y
\end{eqnarray}

Notice that the columns of the matrix are the vectors used in our linear combination. Notice also that we can now
reinterpret the result of multiplying a vector by a matrix as taking the dot product of each of the matrix rows with the
vector.

\begin{eqnarray}
  #[%a#,%b#;%c#,%d#] \cdot #[%x#;%y#] & = & #[%a%x + %b%y#; %c%x + %d%y#] \\
                                      & = & #[(%a,%b) \cdot (%x,%y)#; (%c,%d) \cdot (%x,%y)#]
\end{eqnarray}

Because a matrix is just two column vectors, we can naturally extend this definition of multiplication 
to cases in which we have multiplication of a <i>matrix</i> by a matrix: we simply multiply each column of the
second matrix by the first matrix and write down each of the resulting columns in the result matrix.

\begin{eqnarray}
  #[%a#,%b#;%c#,%d#] \cdot#[%x #, %s #;%y #, %t#] & = & #[(%a,%b) \cdot (%x,%y) #, (%a,%b) \cdot(%s,%t) #; (%c,%d) \cdot (%x,%y) #, (%c,%d) \cdot (%s,%t)#]
\end{eqnarray}

These definitions can be extended naturally to vectors and matrices with more than two components. If we denote using %M_{ij} the entry in a matrix %M found
in the %ith row and %jth column, then we can define the result of matrix multiplication of two matrices %A and %B as a matrix %M such that

  $$ %M_{ij} &nbsp;&nbsp;=&nbsp;&nbsp; %ith row of %A \cdot %jth row of %B.$$

<a name="3.2.a"></a>
<h3>Interpreting matrices as tables of relationships and transformations of system states</h3>

We saw how vectors can be used to represent system states. We can extend this interpretation to matrices and use
matrices to represent relationships between the dimensions of system states. This allows us to interpret matrices as transformations
between system states (or partial observations of system states).

If we again consider the example system involving a barn of cows and chickens, we can reinterpret the matrix as a table of relationships between
dimensions. Each entry in the table has a unit indicating the relationship it represents.

<table>
 <tr>
  <td></td>
  <td align="center">chickens</td>
  <td align="center">cows</td>
  <td></td>
 </tr>
 <tr>
  <td>heads</td>
  <td>1 <i style="color:gray;">head/chicken</i></td>
  <td>1 <i style="color:gray;">head/cow</i></td>
  <td></td>
 </tr>
 <tr>
  <td>legs</td>
  <td>2 <i style="color:gray;">legs/chicken</i></td>
  <td>4 <i style="color:gray;">legs/cow</i></td>
  <td></td>
 </tr>
</table>

Notice that the <i>column labels</i> in this table represent the dimensions of an "input" vector that could be multiplied by this matrix, and
the <i>row labels</i> specify the dimensions of the "output" vector that is obtained as a result. That is,
if we multiply using the above matrix a vector that specifies the number of chickens and the number of cows in a system state, we will get a
vector that specifies the number of heads and legs we can observe in that system.

\begin{eqnarray}
 #[ 1 <i style="color:gray;">head/chicken</i> #,  1 <i style="color:gray;">head/cow</i> #; 
   2 <i style="color:gray;">legs/chicken</i> #, 4 <i style="color:gray;">legs/cow</i> #] 
   \cdot 
   #[x <i style="color:gray;">chickens</i> #; y <i style="color:gray;">cows</i> #]
      & = &  #[ %x+%y <i style="color:gray;">heads</i>#; 2%x+4%y <i style="color:gray;">legs</i> #]
\end{eqnarray}

Thus, we can interpret multiplication by this matrix as a function that takes system states that only specify the number of chickens and
cows, and converts them to system states that only specify the number of heads and legs:

  $$ (<i style="color:gray;"># chickens</i> \times <i style="color:gray;"># cows</i>) \to (<i style="color:gray;"># heads</i> \times <i style="color:gray;"># legs</i>) $$

<a name="3.2.b"></a>
<h3>Interpreting multiplication of matrices as composition of system state transformations</h3>

<b>Example:</b> Suppose that we have a system with the following dimensions.
<ul>
 <li>number of wind farms</li>
 <li>number of coal power plants</li>
 <li>units of power</li>
 <li>units of cost (e.g., pollution)</li>
 <li>number of single family homes (s.f.h.'s)</li>
 <li>number of businesses</li>
</ul>

Two different matrices might specify the relationships between some combinations of dimensions in this system.

\begin{eqnarray}
 %M_1 = #[ 100 <i style="color:gray;">power/wind farm</i> #, 250 <i style="color:gray;">power/coal plant</i> #; 
    50 <i style="color:gray;">cost/wind farm</i> #,  400 <i style="color:gray;">cost/coal plant</i> #]
   , \~
 %M_2 = #[ 4 <i style="color:gray;">s.f.h./unit power</i> #,  -2 <i style="color:gray;">s.f.h./unit cost</i> #; 
    1 <i style="color:gray;">businesses/unit power</i> #,  0 <i style="color:gray;">businesses/unit cost</i> #]
\end{eqnarray}

Notice that these two matrices both represent transformations between partial system state descriptions.

  $$ %T_1: (<i style="color:gray;"># wind farms</i> \times <i style="color:gray;"># coal plants</i>) \to (<i style="color:gray;">units of power</i> \times <i style="color:gray;">units of cost</i>) $$
  $$ %T_2: (<i style="color:gray;">units of power</i> \times <i style="color:gray;">units of cost</i>) \to (<i style="color:gray;"># s.f.h.</i> \times <i style="color:gray;"># businesses</i>) $$

Notice that because the interpretion of a result obtained using the first transformation matches the interpretation of an input to the second, we
can compose these transformations to obtain a third transformation.

  $$ %T_2 \circ %T_1: (<i style="color:gray;"># wind farms</i> \times <i style="color:gray;"># coal plants</i>) \to (<i style="color:gray;"># s.f.h.</i> \times <i style="color:gray;"># businesses</i>) $$

This corresponds to multiplying the two matrices to obtain a third matrix. Notice that the units of the resulting matrix can be computed using a
process that should be familiar to you from earlier coursework.

\begin{eqnarray}
 #[ 4 <i style="color:gray;">s.f.h./unit power</i> #,  -2 <i style="color:gray;">s.f.h./unit cost</i> #; 
    1 <i style="color:gray;">businesses/unit power</i> #,  0 <i style="color:gray;">businesses/unit cost</i> #]
   \cdot 
 #[ 100 <i style="color:gray;">power/wind farm</i> #, 250 <i style="color:gray;">power/coal plant</i> #; 
    50 <i style="color:gray;">cost/wind farm</i> #,  400 <i style="color:gray;">cost/coal plant</i> #]
   & = & 
 #[ 300 <i style="color:gray;">s.f.h./wind farm</i> #,  200 <i style="color:gray;">s.f.h./coal plant</i> #; 
    100 <i style="color:gray;">business/wind farm</i> #,  250 <i style="color:gray;">businesses/coal plant</i> #]
\end{eqnarray}

Thus, given some vector describing the number of wind farms and coal plants in the system, we can multiply that vector by (%M_2 \cdot %M_1) 
to compute the number of single family homes and business we expect to find in that system.

<!--assignment2-->
<br/><hr/>
<a name="3.3"></a>
<a name="assignment2"></a>
<b>Assignment #2: Matrix Algebra</b> <!--span class="btn_assignment">(<a href="materials.php?hw=2">show only this assignment</a>)</span-->

       <p>In this assignment you will perform step-by-step algebraic manipulations involving vector and matrix properties and operations.
       For the word problems, you must solve them by setting up a system with dimensions, introducing a matrix characterizing the relationships
       between the dimensions in that system, and then solving for the system states that contain the information being sought in the 
       problem statements.</p>

<ol>
  <li>
    <ol style="list-style-type:lower-alpha;">
      <li> Finish the argument below that shows that matrix multiplication of vectors in \R^3 preserves lines.

@
\forall %M \in \R^(3 \times 3), \forall %u,%v,%w \in \R^3, \forall %s \in \R,
   %[
    %w = %s(%u-%v) + %v \and
   %]
   %[
    `(%w) is on the line defined by (%u) and (%v)`
   %]
  \implies

    # ...

    %[
    `(%M %w) is on the line defined by (%M %u) and (%M %v)`
    %]
/@

      </li>
      <li> Finish the argument below that shows that if multiplication by a matrix %M maps three vectors to [0; 0; 0], it
      maps any linear combination %v' of those vectors to [0; 0; 0].

@
\forall %M \in \R^(3 \times 3), \forall %u,%v,%w,%v' \in \R^3, \forall %a,%b,%c \in \R,
    %[
    %M %u & = & [0;0;0] \and \\
    %M %v & = & [0;0;0] \and \\
    %M %w & = & [0;0;0] \and \\
    %v' & = & %a %u + %b %v + %c %w
    %]
  \implies

    # ...

    %[
    %M %v' = [0;0;0]
    %]
/@

           <b>Extra credit:</b> if %u, %v, and %w are setwise linearly independent, what can you say about the matrix %M? Be as specific as
           possible.

      </li>
      <li> Show that if a matrix %M in \R^{2 \times 2} is invertible, there is a unique solution for %u given any equation %M %u = %v with %v \in \R^2.

@
\forall %M \in \R^(2 \times 2), \forall %u,%v \in \R^2,
    %[
    `(%M) is invertible` \and
    %]
    %[
     %M %u = %v
    %]
  \implies

    # ...

    # replace the right-hand side below 
    # with an expression in terms of M and v
    %[
    %u = \undefined 
    %]
/@

      </li>


      <li> Show that if the column vectors of a matrix in \R^{2 \times 2} are linearly dependent, then its determinant is 0.

@
\forall %a,%b,%c,%d \in \R,
    %[
    `([%a;%c]) and ([%b;%d]) are linearly dependent`
    %]
  \implies

    # ...
    %[
    \det [%a#,%b; %c#,%d] = 0
    %]
/@

           <b>Extra credit:</b> you may include the opposite direction of this proof for extra credit (build a new, separate argument in which 
           <code>\det [a,c; c,d] = 0</code> is found above the <code>\implies</code> and <code>`([a;c]) and ([b;d]) are linearly dependent`</code>
           is at the end of the argument below it).
      </li>
    </ol>
  </li>

  <li> In this problem, you will define explicit matrices that correspond to elementary row operations on matrices in \R^{2 \times 2}.
    <ol style="list-style-type:lower-alpha;">
      <li> Find appropriate matrices %A, %B, %C, and %D in \R^{2 \times 2} to finish the following argument.

@
\forall %A,%B,%C,%D \in \R^(2 \times 2), \forall %a,%b,%c,%d,%t \in \R,
    %[
    %A & = & \undefined \and \\
    %B & = & \undefined \and \\
    %C & = & \undefined \and \\
    %D & = & \undefined
    %]
  \implies

    # ...
    %[
    %A [%a#,%b;%c#,%d] & = & [%a+c#, %b+%d   ;   %c#, %d]    \and \\
    %B [%a#,%b;%c#,%d] & = & [%a#, %b       ; %c+%a#, %d+%b]  \and \\
    %C [%a#,%b;%c#,%d] & = & [%t*%a#, %t*%b   ;   %c#, %d]    \and \\
    %D [%a#,%b;%c#,%d] & = & [%a#, %b       ; %t*%c#, %t*%d]
    %]
/@

      </li>
      <li> Use the matrices %A, %B, %C, and %D from part (a) with matrix multiplication to construct a matrix %E that can be
           shown to satisfy the last line in the following argument.

@
\forall %A,%B,%C,%D,%E \in \R^(2 \times 2), \forall %a,%b,%c,%d,%t \in \R,
    %[
    %t & = & \undefined \and \\
    %A & = & \undefined \and \\
    %B & = & \undefined \and \\
    %C & = & \undefined \and \\
    %D & = & \undefined \and \\
    %E & = & \undefined 
    %]
  \implies

    # ...
    %[
    %E [%a#,%b; %c#,%d] = [%c#,%d; %a#,%b]
    %]
/@

      </li>
      <li> <b>Extra credit:</b> the row operations defined by %A, %B, %C, and %D are all invertible as long as %t \neq 0; 
      prove this for %t = -1 by showing that the matrices %A, %B, %C, and %D are invertible.

@
\forall %A,%B,%C,%D \in \R^(2 \times 2), \forall %t \in \R,
    %[
    %t & = & -1 \and \\
    %A & = & \undefined \and \\
    %B & = & \undefined \and \\
    %C & = & \undefined \and \\
    %D & = & \undefined
    %]
  \implies

    # ...

    %[
    `(%A) is invertible` \and \\
    `(%B) is invertible` \and \\
    `(%C) is invertible` \and \\
    `(%D) is invertible`
    %]
/@

      </li>
    </ol>
  </li>

  <li> <p>You decide to drive the 2800 miles from New York to Los Angeles in a hybrid vehicle. 
           A hybrid vehicle has two modes: using only the electric motor and battery, it can travel 1 mile on 3 units of battery power; 
           using only the internal combustion engine, it can travel 1 mile on 0.1 liters of gas (about 37 mpg) while also charging the 
           battery with 1 unit of battery power. At the end of your trip, you have 1400 fewer units of battery power than you did when 
           you began the trip. How much gasoline did you use (in liters)?</p>

           <p>You should define a system with the following dimensions:
            <ul>
             <li>net change in the total units of battery power;</li>
             <li>total liters of gasoline used;</li>
             <li>total number of miles travelled;</li>
             <li>number of miles travelled using the electric motor and battery;</li>
             <li>number of miles travelled using the engine.</li>
            </ul>
           </p>
           
           <p>You should define a matrix in \R^{3 \times 2} to characterize this system. Then, write down an equation containing
           that matrix (and three variables in \R), and solve it to obtain the quantity of gasoline.</p>

@
\forall %x,%y,%z \in \R,
    %[
    [ \undefined#, \undefined;
      \undefined#, \undefined;
      \undefined#, \undefined ] [%x; %y] = [\undefined; \undefined; \undefined]
    %]
  \implies

    # ...

    %[
    %z = \undefined
    %]
/@

  </li>
      
      
  <li> <p>Suppose we create a very simple system for modelling how predators and prey interact in a closed environment. 
       Our system has only two dimensions: the number of prey animals, and the number of predators. We want to model 
       how the state of the system changes from one generation to the next.</p>
       <p>If there are %x predators and %y prey animals in a given generation, in the next generation the following will be the case:
        <ul>
         <li>all predators already in the system will stay in the system;</li>
         <li>all prey animals already in the system will stay in the system;</li>
         <li>for every prey animal, two new prey animals are introduced into the system;</li>
         <li>for every predator, two prey animals are removed from the system;</li>
         <li>we ignore any other factors that might affect the state (e.g., natural death or starvation).</li>
        </ul>

       <ol style="list-style-type:lower-alpha;">
        <li> <p>Specify explicitly a matrix %T in \R^{2 \times 2} that takes a description of the system state
             in one generation and produces the state of the system during the next generation. <b>Note:</b> you may simply
             enter the matrix on its own line for this part of the problem, but you must also use it in the
             remaining three parts below.</p>
        </li>
        <li> <p>Show that the number of predators does not change from one generation to the next.</p>

@
\forall %T \in \R^(2 \times 2), \forall %x,%y,%y' \in \R,
    # replace \undefined with explicit matrix from part (a)
    %[
    %T = \undefined
    %]
  \implies

    # ...

    # In the conclusion below, you may replace y' 
    # with the correct expression; you may NOT replace
    # x.
    %[
    %T [%x;%y] = [%x;%y']
    %]
/@

        </li>
        <li> <p>Determine what initial state [%x; %y] is such that there is no change in the state from one generation to another.</p>    

@
\forall %T \in \R^(2 \times 2), \forall %x,%y \in \R,
    # replace \undefined with explicit matrix from part (a)
    %[
    %T & = & \undefined \and \\
    [%x;%y] & = & \undefined # find the explicit vector
    %]
  \implies

    # ...

    %[
    %T [%x;%y] = [%x;%y]
    %]
/@

        </li>
        <li> <p>Suppose that in the fourth generation of an instance of this system (that is, after the transformation has 
                 been applied three times), we have 2 predators and 56 prey animals. How many predators and prey animals were 
                 in the system in the first generation (before any transformations were applied)? Let [%x; %y] represent the
                 state of the system in the first generation. Set up a matrix equation that involves [%x; %y] and
                 matrix multiplication, and solve it to obtain the answer.</p>
        </li>
      </ol>
  </li>

</ol>
<hr/><br/>
<!--/assignment2-->


<a name="lecture7"></a>
<a name="3.4"></a>
<h3>Matrix operations and their interpretations</h3>

The following table summarizes the matrix operations that we are considering in this course.

<table class="fig_table">
 <tr>
  <td><b>operation</b></td>
  <td><b>definition</b></td>
  <td><b>restrictions</b></td>
  <td><b>general properties</b></td>
 </tr>
 <tr>
  <td>%M_1 + %M_2</td>
  <td>component-wise</td>
  <td>matrices must have<br/>the same number of rows<br/>and columns</td>
  <td>
    commutative,<br/> 
    associative,<br/> 
    has identity (matrix with all 0 components),<br/> 
    has inverse (multiply matrix by -1),<br/>
    scalar multiplication is distributive
  </td>
 </tr>
 <tr>
  <td>%M_1 \cdot %M_2</td>
  <td>row-column-wise <br/> dot products</td>
  <td>columns in %M_1 = rows in %M_2<br/>rows in %M_1 \cdot %M_2 = rows in %M_1<br/>columns in %M_1 \cdot %M_2 = columns in %M_2</td>
  <td>
    associative,<br/>
    has identity \I (1s in diagonal and 0s elsewhere),<br/>
    distributive over matrix addition,<br/>
    <b>not commutative</b> in general,<br/>
    <b>no inverse</b> in general<br/>
  </td>
 </tr>
 <tr>
  <td>%M^{-1}</td>
  <td></td>
  <td>
    columns in %M = rows in %M<br/>
    matrix is invertible
  </td>
  <td>
    %M^{-1} \cdot %M = %M \cdot %M^{-1} = \I
  </td>
 </tr>
</table>

The following tables list some high-level intuitions about how matrix operations can be understood.

<table class="fig_table">
 <tr>
  <td><b>level of<br/>abstraction</b></td>
  <td colspan="4"><b>interpretations of multiplication<br/>of a vector by a matrix</b></td>
 </tr>
 <tr>
  <td>applications</td>
  <td>transformation of<br/>system states</td>
  <td>extraction of information<br/>about system states</td>
  <td>computing properties of<br/>combinations or aggregations<br/>of objects (or system states)</td>
  <td>conversion of system <br/> state observations<br/> from one set of dimensions<br/>to another</td>
 </tr>
 <tr>
  <td>geometry</td>
  <td>"moving" vectors in <br/> a space (stretching,<br/>skewing, rotating,<br/> reflecting)</td>
  <td>projecting vectors</td>
  <td>taking a linear combination<br/> of two vectors</td>
  <td>reinterpreting vector notation<br/>as referring to a collection<br/>of non-canonical vectors</td>
 </tr>
</table>

<table class="fig_table">
 <tr>
  <td><b>level of<br/>abstraction</b></td>
  <td colspan="4"><b>interpretations of multiplication of two matrices</b></td>
 </tr>
 <tr>
  <td>applications</td>
  <td>composition of system state<br/>transformations or conversions</td>
 </tr>
 <tr>
  <td>geometry</td>
  <td>sequencing of motions of vectors within <br/> a space (stretching, skewing, rotating,<br/> reflecting)</td>
 </tr>
</table>

<table class="fig_table">
 <tr>
  <td><b>level of<br/>abstraction</b></td>
  <td colspan="2"><b>invertible matrix</b></td>
  <td colspan="2"><b>singular matrix</b></td>
 </tr>
 <tr>
  <td>applications</td>
  <td>reversible transformation<br/> of system states</td>
  <td>extraction of <i>complete</i> <br/>information uniquely determining<br/>a system state</td>
  <td>irreversible transformation<br/> of system states</td>
  <td>extraction of <i>incomplete</i> <br/>information about<br/>a system state</td>
 </tr>
 <tr>
  <td>geometry</td>
  <td colspan="2">reversible transformation or motion<br/>of vectors in a space</td>
  <td colspan="2">projection onto a <i>strict subset</i> of<br/> a set of vectors (space)</td>
 </tr>
 <tr>
  <td>symbolic</td>
  <td colspan="2">
    reversible transformation of <br/>information numerically encoded in matrix<br/>
    (example of such information: system of<br/>linear equations encoded as matrix)
  </td>
  <td colspan="2">irreversible/"lossy" transformation of<br/>information encoded in matrix</td>
 </tr>
</table>

Suppose we interpret multiplication of a vector by a matrix %M as a function from vectors to vectors:

  $$ %f(%v) = %M %v.$$

<b>Fact:</b> Notice that for any %M, if %f(%v) = %M %v then %f is always a function because %M %v has only one possible result (i.e.,
matrix multiplication is <i>deterministic</i>) for a given %M and %v.

<b>Fact:</b> If %f(%v) = %M %v, then %f is invertible if %M is an invertible matrix. The inverse of %f is then defined to be:

  $$ %f^{-1}(%v) = %M^{-1} %v.$$

Notice that f^{-1} is a function because %M^{-1} %v only has one possible result. Notice that %f^{-1} is the inverse of %f because

  $$ %f^{-1}(%f(%v)) = %M^{-1} %M %v = \I %v = %v.$$

<b>Fact:</b> If the columns of a matrix %M are linearly dependent and %f(%v) = %M %v, then %M cannot have an inverse. We consider the case
in which %M \in \R^{2 \times 2}. Suppose we have that

\begin{eqnarray}
 %M & = & #[%a #, %b #; %c #, %d #].
\end{eqnarray}

If the columns of %M are linearly dependent, then we know that there is some %s \in \R such that

\begin{eqnarray}
 %s #[%a #; %c#] & = & #[%b #; %d#].
\end{eqnarray}

This means that we can rewrite %M:

\begin{eqnarray}
 %M & = & #[%a #, %s%a #; c #, %s%c #].
\end{eqnarray}

Since matrix multiplication can be interpreted as taking a linear combination of the column vectors, this means that for %x,%y \in \R,

\begin{eqnarray}
 #[%a #, %s%a #; c #, %s%c #] #[%x #; %y#] & = & #[%a #; c#] %x + #[%s%a #; %s%c#] %y & = & (%x + %s %y) #[%a #; c#]
\end{eqnarray}

But this means that for any two vectors (%x,%y) and (%x',%y'), if %x + %s%y = %x' + %s%y' then multiplying by %M will lead to the same result.
Thus, %f is a function that takes two different vector arguments and maps them to the same result. If we interpret %f as a relation and take
its inverse %f^{-1}, %f^{-1} cannot be a function. Thus, %M cannot have an inverse (if it did, then %f^{-1} would be a function).

<a name="lecture8"></a>
<a name="3.5"></a>
<h3>Matrix properties</h3>

The following are subsets of \R^{n \times n} that are of interest in this course because they
correspond to transformations and systems that have desirable or useful properties. For some of these
sets of matrices, matrix multiplication and inversion have properties that they do not in general.

<table class="fig_table">
 <tr>
  <td><b>subset of \R^{n \times n}</b></td>
  <td><b>definition</b></td>
  <td><b>closed under<br/>matrix<br/>multiplication</b></td>
  <td><b>properties of<br/>matrix multiplication</b></td>
  <td><b>inversion</td>
 </tr>
 <tr>
  <td>identity matrix</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = 1 if %i=%j, 0 otherwise </td>
  <td>closed</td>
  <td><b>commutative</b>,<br/> associative,<br/> distributive with addition, <br/>has identity</td>
  <td>has inverse (itself);<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>elementary matrix</td>
  <td>
    can be obtained via an<br/>elementary row operation<br/>from \I:
    <ul style="margin-right:-40px; padding-right:-40px;">
     <li>add multiple of one row<br/>of the matrix to another<br/>row</li>
     <li>multiply a row by a<br/>scalar</li>
     <li>swap two rows of the<br/>matrix</li>
    </ul>
    Note: the third is a combination<br/>of the first two operations.
  </td>
  <td></td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>scalar matrices</td>
  <td>\exists %s \in \R, \forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = %s if %i=%j, 0 otherwise </td>
  <td>closed</td>
  <td><b>commutative</b>,<br/> associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>diagonal matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} \in \R if %i=%j, 0 otherwise </td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>matrices with<br/>constant diagonal</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ii} = %M_{jj}</td>
  <td></td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
 <tr>
  <td>symmetric matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = %M_{ji}</td>
  <td></td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
 <tr>
  <td>symmetric matrices<br/>with constant diagonal</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ii} = %M_{jj} and %M_{ij} = %M_{ji}</td>
  <td>closed</td>
  <td><b>commutative</b>, <br/>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
 <tr>
  <td>upper triangular matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = 0 if i > j</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td><b>not</b> invertible in general;<br/>closed under inversion<br/>when invertible</td>
 </tr>
 <tr>
  <td>lower triangular matrices</td>
  <td>\forall %i,%j <br/>&nbsp;&nbsp;&nbsp; %M_{ij} = 0 if i < j</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td><b>not</b> invertible in general;<br/>closed under inversion<br/>when invertible</td>
 </tr>
 <tr>
  <td>invertible matrices</td>
  <td>%M^{-1} %M = %M %M^{-1} = \I</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
 <tr>
  <td>square matrices</td>
  <td>all of \R^{n \times n}</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td></td>
 </tr>
</table>

Two facts presented in the above table are worth noting.

<b>Fact:</b> Suppose %A is invertible. Then the inverse of %A^{-1} is %A, because %A %A^{-1} = \I. Thus, (%A^{-1})^{-1} = %A.

<b>Fact:</b> If %A and %B are invertible, so is %A%B. That is, invertible matrices are closed under matrix multiplication. We can show
this by using the associativity of matrix multiplication. Since %A and %B are invertible, there exist %B^{-1} and %A^{-1} such that:

\begin{eqnarray}
 (%B^{-1} %A^{-1}) %A %B & = & (%B^{-1} %A^{-1}) %A %B\\
                        & = & %B^{-1} (%A^{-1} %A) %B\\
                        & = & %B^{-1} \I %B\\
                        & = & %B^{-1} %B\\
                        & = & \I.
\end{eqnarray}

Thus, since there exists a matrix (%B^{-1} %A^{-1}) such that (%B^{-1} %A^{-1}) %A %B = \I, (%B^{-1} %A^{-1}) is the inverse of %A%B.

Another fact not in the table is also worth noting.

<b>Fact:</b> Any product of a finite number of elementary row matrices is invertible. This fact follows from the fact that all
elementary matrices are invertible, and that the set of invertible matrices is closed under multiplication.

Given the last fact, we might ask whether the opposite is true: are all invertible matrices the product of a finite number of elementary
matrices? The answer is <b>yes</b>, as we will see further below.

<a name="3.6"></a>
<h3>Solving the equation %M %v = %w for %M with various properties</h3>

Recall that for %v,%w \in \R^n and %M \in \R^{n \times n}, an equation of the following form
can represent a system of equations:

  $$%M %v = %w$$

Notice that if %M is invertible, we can solve for %v by multiplying both sides by %M^{-1}. More generally, if %M is a member of
some of the sets in the above table, we can find straightforward algorithms for solving such an equation for %v.

<table class="fig_table">
 <tr>
  <td><b>%M is ... </b></td>
  <td><b>algorithm to solve %M %v = %w for %v</b></td>
 </tr>
 <tr>
  <td>the identity matrix</td>
  <td>%w is the solution</td>
 </tr>
 <tr>
  <td>an elementary matrix</td>
  <td>perform a row operation on %M to obtain \I;<br/>perform the same operation on %w</td>
 </tr>
 <tr>
  <td>a scalar matrix</td>
  <td>divide the components of %w by the scalar</td>
 </tr>
 <tr>
  <td>a diagonal matrix</td>
  <td>divide each component of %w by the<br/>corresponding matrix component</td>
 </tr>
 <tr>
  <td>an upper triangular matrix</td>
  <td>
    start with the last entry in %v, which is easily obtained;<br/>
    move backwards through %v, filling in the values by<br/>
    substituting the already known variables
  </td>
 </tr>
 <tr>
  <td>a lower triangular matrix</td>
  <td>
    start with the first entry in %v, which is easily obtained;<br/>
    move forward through %v, filling in the values by<br/>
    substituting the already known variables
  </td>
 </tr>
 <tr>
  <td>
    product of a lower triangular matrix<br/>
    and an upper triangular matrix</td>
  <td>
    combine the algorithms for upper and lower triangular<br/>
    matrices in sequence (see example below)
  </td>
 </tr>
 <tr>
  <td>an invertible matrix</td>
  <td>compute the inverse and multiply %w by it</td>
 </tr>
</table>

<b>Example:</b> We consider an equation %M %v = %w where %M is a diagonal matrix (the identity matrix and all scalar matrices are also diagonal
matrices).

\begin{eqnarray}
 #[ 4 #, 0 #, 0 #;  0 #, 3 #, 0 #;0 #, 0 #, 5 #] #[%x #; %y #; %z#] & = & #[2 #; 9 #; 10 #] \\
       4%x & = & 2 \\
       3%y & = & 9 \\
       5%z & = & 10 \\
       %x & = & 1/2 \\
       %y & = & 3 \\
       %z & = & 2
\end{eqnarray}

<b>Example:</b> We consider an equation %M %v = %w where %M is a lower triangular matrix.

\begin{eqnarray}
 #[ 4 #, 0 #, 0 #;  2 #, 4 #, 0 #; 4 #, 3 #, 5 #] #[%x #; %y #; %z#] & = & #[2 #; 9 #; 18 #] \\
       4%x & = & 2 \\
       2%x + 4%y & = & 9 \\
       4%x + 3%y + 5%z & = & 10 \\
       %x & = & 1/2 \\
       %y & = & 2 \\
       %z & = & 2
\end{eqnarray}



<b>Example:</b> Suppose that %M = %L %U where %L is lower triangular and %U is upper triangular. How can we solve %M %v = %w?

First, note that because matrix multiplication is associative, we have

\begin{eqnarray}
 %M %v & = & (%L %U) %v \\
       & = & %L (%U %v)
\end{eqnarray}

We introduce a new vector for the intermediate result %U %v, which we call %u. Now, we have a system of matrix equations.

\begin{eqnarray}
 %U %v & = & %u \\
 %L %u & = & %w
\end{eqnarray}

We first solve for %u using the algorithm for lower triangular matrices, then we solve for %v using the algorithm for upper triangular
matrices.

<b>Example:</b> The inverse of a matrix in \R^{2 \times 2} can be computed as follows.

\begin{eqnarray}
 #[ a #, b #;  c #, d #]^{#-1} & = & #[ d/(ad-bc) #, -b/(ad-bc) #;  -c/(ad-bc) #, a/(ad-bc) #]
\end{eqnarray}

Thus, if we find that the determinant of a matrix %M \in \R^{2 \times 2} is nonzero, the algorithm for solving %M %v = %w is
straightforward. Consider the following example.

\begin{eqnarray}
 #[ 1 #, 2 #;  3 #, 4 #] #[%x #; %y #] & = & #[ 5 #; 13 #] \\
 #[ 1 #, 2 #;  3 #, 4 #]^{#-1} #[ 1 #, 2 #;  3 #, 4 #] #[%x #; %y #] & = &  #[ 1 #, 2 #;  3 #, 4 #]^{#-1} #[ 5 #; 13 #] \\
 #[ 1 #, 0 #;  0 #, 1 #] #[%x #; %y #] & = &  
       #[ 4/((1\cdot4)-(2\cdot3)) #, -2/((1\cdot4)-(2\cdot3)) #;  -3/((1\cdot4)-(2\cdot3)) #, 1/((1\cdot4)-(2\cdot3)) #] #[ 5 #; 13 #] \\
 #[ 1 #, 0 #;  0 #, 1 #] #[%x #; %y #] & = &  #[ -2 #, 1 #;  3/2 #, 1/-2 #] #[ 5 #; 13 #] \\
 #[%x #; %y #] & = &  #[ 3 #;  1 #]
\end{eqnarray}

<a name="3.7"></a>
<h3>Row echelon form and reduced row echelon form</h3>

We define two more properties that a matrix may possess.

<table class="fig_table">
 <tr>
  <td>%M is in row echelon form</td>
  <td>
    <ul style="margin-right:0px; padding-right:-40px; margin-bottom:0px;">
      <li>all nonzero rows are above any rows consisting of all zeroes</li>
      <li>the first nonzero entry (from the left) of a nonzero row is strictly<br/>to the right of the first nonzero entry of the row above it</li>
      <li>all entries in a column below the first nonzero entry in a row are zero<br/>(the first two conditions imply this)</li>
    </ul>
  </td>
 </tr>
 <tr>
  <td>%M is in reduced row echelon form</td>
  <td>
    <ul style="margin-right:0px; padding-right:-40px; margin-bottom:0px;">
      <li>%M is in row echelon form</li>
      <li>the first nonzero entry in every row is 1; this 1 entry is the only nonzero entry in its column</li>
    </ul>
  </td>
 </tr>
</table>

We can obtain the reduced row echelon form of a matrix using a sequence of appropriately chosen elementary row operations.

<b>Example:</b> Suppose we want to find the reduced row echelon form of the matrix below. We list the steps of the procedure.

\begin{eqnarray}
 #[ 1 #, 2 #, 1 #;  -2 #, -3 #, 1 #;  3 #, 5 #, 0 #] \to  #[ 1 #, 2 #, 1 #;  0 #, 1 #, 3 #;  3 #, 5 #, 0 #]
                                                     \to  #[ 1 #, 2 #, 1 #;  0 #, 1 #, 3 #;  0 #, -1 #, -3 #]
                                                     \to  #[ 1 #, 2 #, 1 #;  0 #, 1 #, 3 #;  0 #, 0 #, 0 #]
                                                     \to  #[ 1 #, 0 #, -5 #;  0 #, 1 #, 3 #;  0 #, 0 #, 0 #]
\end{eqnarray}

<b>Fact:</b> For any matrix %M, there may be more than one way to reach the reduced row echelon form using elementary row operations. However,
it is always possible, and there is exactly one unique reduced row echelon form for that matrix %M. We do not prove this result in this course, 
but fairly short proofs by induction can be found elsewhere (such as <a href="http://mathdl.maa.org/images/cms_upload/Yuster19807.pdf">here</a>).
Because the reduced row echelon form of a matrix %M is unique, we use the following notation to denote it:

  $$ \rref %M.$$

<a name="lecture9"></a>
<b>Question:</b> For a given matrix in reduced row echelon form, how many different matrices can be reduced to it (one or more than one)?

<b>Fact:</b> It is always true that \rref %M = \rref (\rref %M). Thus, the \rref operation is <i>idempotent</i>.

<b>Fact:</b> If \rref %M is \I (the identity matrix), then %M is invertible. This is because \I is an elementary matrix, and the row operations 
used to obtain \rref %M from %M can be represented as a product of elementary (and thus, invertible) matrices %E_1 \cdot ... \cdot %E_n, so:

  $$%M = %E_1 \cdot ... \cdot %E_n \cdot \I.$$

<b>Fact:</b> If %M \in \R^{n \times n} is not invertible, then the bottom row of \rref %M must have all zeroes. This is because when all
rows of \rref %M \in \R^{n \times n} have nonzero values, it must be the identity matrix, and %M must then be invertible.

<b>Fact:</b> The reduced row echelon form of an invertible matrix %M \in \R^{n \times n} is \I \in \R^{n \times n}.

We can show this is true using a proof by contradiction. Suppose that %M is invertible, but \rref %M \neq \I. We know that \rref %M can be
obtained via a finite number of elementary row operations %E_1 \cdot ... \cdot %E_n:

  $$(%E_1 \cdot ... \cdot %E_n) %M = \rref %M.$$

If \rref %M is not \I, then the last row of \rref %M must consist of only zeroes. But because %M is invertible, we have:

\begin{eqnarray}
 ((%E_1 \cdot ... \cdot %E_n) %M) %M^{-1} & = & (\rref %M) %M^{-1}\\
     %E_1 \cdot ... \cdot %E_n & = & (\rref %M) %M^{-1}
\end{eqnarray}

Since the product of elementary matrices is invertible, (\rref %M) %M^{-1} is also invertible. But if the last row of \rref %M consists of only
zeroes, it cannot be that (\rref %M) %M^{-1} is invertible. Thus, we have a contradiction, so our assumption that \rref %M \neq \I is false.

The above result implies the following fact.

<b>Fact:</b> If a matrix %M is invertible, it is the finite product of a finite number of elementary matrices. This is because \rref %M is
the identity, which is an elementary matrix, and %M can be reduced via a finite number of invertible row operations to \I. Thus, the
elementary matrices can be used to generate every possible invertible matrix.

The following table summarizes the results.

<table class="fig_table">
 <tr>
  <td colspan="4"><b>fact</b></td>
  <td><b>justification</b></td>
 </tr>
 <tr>
  <td><b>(1)</b></td>
  <td>{%M | %M is a finite product of elementary matrices}</td>
  <td>=</td>
  <td>{%M | \rref %M = \I}</td>
  <td>\I is an elementary matrix;<br/>sequences of row operations<br/>are equivalent to multiplication by<br/>elementary matrices</td>
 </tr>
 <tr>
  <td><b>(2)</b></td>
  <td>{%M | %M is a finite product of elementary matrices}</td>
  <td>\subset</td>
  <td>{%M | %M is invertible}</td>
  <td>elementary matrices are invertible;<br/>products of invertible matrices are invertible</td>
 </tr>
 <tr>
  <td><b>(3)</b></td>
  <td>{%M | \rref %M = \I}</td>
  <td>\subset</td>
  <td>{%M | %M is invertible}</td>
  <td>fact <b>(1)</b> in this table;<br/>fact <b>(2)</b> in this table;<br/>transitivity of equality</td>
 </tr>
 <tr>
  <td><b>(4)</b></td>
  <td>{%M | %M is invertible}</td>
  <td>\subset</td>
  <td>{%M | \rref %M = \I}</td>
  <td>proof by contradiction;<br/>non-invertible %M implies<br/> \rref %M has all zeroes in bottom row</td>
 </tr>
 <tr>
  <td><b>(5)</b></td>
  <td>{%M | %M is invertible}</td>
  <td>=</td>
  <td>{%M | \rref %M = \I}</td>
  <td>for any sets %A,%B,<br/>%A \subset %B and %B \subset %A<br/>implies %A = %B</td>
 </tr>
 <tr>
  <td><b>(6)</b></td>
  <td>{%M | %M is a finite product of elementary matrices}</td>
  <td>=</td>
  <td>{%M | %M is invertible}</td>
  <td>fact <b>(1)</b> in this table;<br/>fact <b>(5)</b> in this table;<br/>transitivity of equality</td>
 </tr>
</table>

Given all these results, we can say that the properties for a matrix %M in the following table are all equivalent:
if any of these is true, then all of them are true. If any of these is false, then all of them are false.

<table class="fig_table">
 <tr><td>%M is invertible</td></tr>
 <tr><td>\det %M \neq 0</td></tr>
 <tr><td>the columns of %M are (setwise) linearly independent</td></tr>
 <tr><td>%M%v = %w has exactly one solution</td></tr>
 <tr><td>%M is a finite product of elementary matrices</td></tr>
</table>

<b>Fact:</b> For matrices %M \in \R^{n \times n}, \rref %M is guaranteed to be upper triangular. Notice that this means that for some finite
product of elementary matrices %E_1 \cdot ... \cdot %E_n, it is the case that

  $$%M = (%E_1 \cdot ... \cdot %E_n) \cdot %U.$$

If %E_1 ,..., %E_n are all lower triangular, then then %M has an %L%U decomposition. However, this will not always be the case.

<a name="3.8"></a>
<h3>Matrix transpose</h3>

<b>Definition:</b> The transpose of a matrix %M \in \R^{n \times n}, denoted %M^\top, is defined to be %A such that %A_{ij} = %M_{ji}.

<b>Fact:</b> If a matrix %M is scalar, diagonal, or symmetric, %M^\top = %M. If a matrix %M is upper triangular, %M^\top is lower triangular (and vice versa).

<b>Fact:</b> For %A,%B \in \R^{n \times n}, it is always the case that:

\begin{eqnarray}
  (%A^\top)^\top & = &  %A \\
  (%A + %B)^\top & = &  %A^\top + %B^\top \\
  %s(%B^\top) & = &  (%s%B)^\top
\end{eqnarray}

<b>Fact:</b> It is always the case that (%A%B)^\top = %B^\top %A^\top. If %M = %A%B, then:

\begin{eqnarray}
  (%A%B)_{ij} & = & %ith row of %A \cdot %jth column of %B \\
          & = & %ith column of %A^\top \cdot %jth row of %B^\top \\
          & = & %jth row of %B^\top \cdot %ith column of %A^\top  \\
          & = & (%B^\top %A^\top)_{ji}.
\end{eqnarray}

<b>Fact:</b> If %A is invertible, then so is %A^\top. This can be proven using the fact directly above.

@
\forall %A,%B \in \R^(2 \times 2),
    `(%A) is invertible` 
  \implies
   %[
   (%A^(-1)) * %A & = &  [1 #, 0; 0 #, 1] \and \\
   %A^\t * (%A^(-1))^\t & = & ((%A^(-1)) * %A)^\t \and \\
           `` & = & [1#,0;0#,1]^\t \and \\
           `` & = & [1#,0;0#,1]
   %]
/@

<b>Fact:</b> \det %A = \det %A^\top. We can see this easily in the %A \in \R^{2 \times 2} case.

@
\forall %a,%b,%c,%d \in \R,
  %[
  \det [%a#,%b;%c#,%d] & = & %a %d-%b %c \and \\
              `` & = & %a %d - %c %b \and \\
              `` & = & \det [%a#,%c;%b#,%d] \and \\
  \det [%a#,%b;%c#,%d] & = & \det [%a#,%c;%b#,%d]
  %]
/@

<!--  - how to interpret units? application? -->

<a name="3.9"></a>
<h3>Orthogonal matrices</h3>

<b>Definition:</b> A matrix %M \in \R^{n \times n} is orthogonal iff %M^\top = %M^{-1}.

<b>Fact:</b>  The columns of orthogonal matrices are always setwise orthogonal unit vectors. We can see this in the \R^{2 \times 2} case.

\begin{eqnarray}
  #[ %a #, %b #; %c #, %d #] #[ %a #, %b #; %c #, %d #]^\top  & = & #[ 1 #, 0 #; 0 #, 1 #] \\
  #[ %a #, %b #; %c #, %d #] #[ %a #, %c #; %b #, %d #]  & = & #[ 1 #, 0 #; 0 #, 1 #]\\
   #[ (%a,%b) \cdot (%a,%b) #, (%a,%b) \cdot (%c,%d) #; (%c,%d) \cdot (%a,%b) #, (%c,%d) \cdot (%c,%d) #] & = & #[ 1 #, 0 #; 0 #, 1 #].
\end{eqnarray}

Below, we provide a verifiable argument for the \R^{2 \times 2} case.

@
\forall %a,%b,%c,%d \in \R,
    \begin{eqnarray}
    [%a#,%b;%c#,%d] * [%a#,%b;%c#,%d]^\t & = & [1#,0;0#,1]
    \end{eqnarray}
  \implies
    %[
    [%a#,%b;%c#,%d] * [a#,c;b#,d] & = & [1#,0;0#,1] \and \\
    [%a %a + %b %b#, %a %c + %b %d; %c %a + %d %b#, %c %c + %d %d] & = & [1#,0;0#,1] \and \\

    %a %a + %b %b & = & 1 \and \\
    [%a;%b] * [%a;%b] & = & 1 \and \\
    ||[%a;%b]|| & = & 1 \and \\
    %]
    %[
    `([%a;%b]) is a unit vector` \and \\
    %]
    %[
    %c %c + %d %d & = & 1 \and \\
    [%c;%d] * [%c;%d] & = & 1 \and \\
    ||[%c;%d]|| & = & 1 \and \\
    %]
    %[
    `([%c;%d]) is a unit vector` \and \\
    %]
    %[
    %a %c + %b %d & = & 0 \and \\
    [%a;%b] * [%c;%d] & = & 0 \and \\
    %]
    %[
    `([%a;%b]) and ([%c;%d]) are orthogonal`
    %]
/@

<b>Fact:</b> Matrices representing rotations and reflections are orthogonal. We can show this for the general rotation matrix:

\begin{eqnarray}
  #[ \cos \theta #, \sin \theta #; -\sin \theta #, \cos \theta #] #[ \cos \theta #, \sin \theta #; -\sin \theta #, \cos \theta #]^\top 
          & = & #[ \cos \theta #, \sin \theta #; -\sin \theta #, \cos \theta #] #[ \cos \theta #, -\sin \theta #; \sin \theta #, \cos \theta #] \\
          & = & #[ \cos^2 \theta + \sin^2 \theta &nbsp;&nbsp;&nbsp;&nbsp;#, -\cos \theta \sin \theta + \sin \theta \cos \theta #; -\sin \theta \cos \theta + \cos \theta \sin \theta  &nbsp;&nbsp;&nbsp;&nbsp;#, \sin^2 \theta  + \cos^2 \theta #]  \\
          & = &  #[ 1 #, 0 #; 0 #, 1 #].
\end{eqnarray}

<b>Fact:</b> Orthogonal matrices are closed under multiplication. For orthogonal %A, %B \in \R^{n \times n} we have:

\begin{eqnarray}
  (%A%B)^\top & = & %B^\top %A^\top & = & %B^{-1} %A^{-1} & = & (%A%B)^{-1}.
\end{eqnarray}

<b>Fact:</b> Orthogonal matrices are closed under inversion and transposition. For orthogonal %A \in \R^{n \times n} we have:

\begin{eqnarray}
  (%A^{-1})^\top & = & (%A^\top)^{-1}.
\end{eqnarray}

Thus, we can show that both A^{-1} and A^\top are orthogonal.

\begin{eqnarray}
  (%A^{-1})^\top & = & (%A^{-1})^{-1} \\
  (%A^\top)^\top & = & %A.
\end{eqnarray}

\begin{eqnarray}
  (%A^\top)^\top & = & (%A^\top)^{-1} \\
                & = & %A.
\end{eqnarray}

We can summarize this by adding another row to our table of matrix subsets.


<table class="fig_table">
 <tr>
  <td><b>subset of \R^{n \times n}</b></td>
  <td><b>definition</b></td>
  <td><b>closed under<br/>matrix<br/>multiplication</b></td>
  <td><b>properties of<br/>matrix multiplication</b></td>
  <td><b>inversion</td>
 </tr>
 <tr>
  <td>orthogonal matrices</td>
  <td>%M^\top = %M^{-1}</td>
  <td>closed</td>
  <td>associative,<br/> distributive with addition, <br/>have identity</td>
  <td>nonzero members<br/>have inverses;<br/>closed under inversion</td>
 </tr>
</table>

<a name="3.10"></a>
<h3>Matrix rank</h3>

Yet another way to characterize matrices is by considering their rank.

<b>Definition:</b> We can define a the <i>rank</i> of a matrix %M, \rank %M, as the number of nonzero rows in \rref %M.

We will see in this course that \rank %M is related in many ways to the various characteristics of a matrix.

<b>Fact:</b> Matrix rank is preserved by elementary row operations. Why is this the case? Because \rref is idempotent and \rank is defined in
terms of it:

\begin{eqnarray}
  \rref %M & = & \rref (\rref %M) \\
  number of nonzero rows in \rref %M & = & number of nonzero rows in \rref (\rref %M) \\
  \rank (%M) & = & \rank (\rref %M)
\end{eqnarray}

<b>Fact:</b> If %M \in \R^{n \times n} is invertible, \rank %M = n. How can we prove this? Because all invertible matrices
have \rref %M = \I, and \I has no rows with all zeroes. If \I \in \R^{n \times n}, then \I has %n nonzero rows.

<b>Fact:</b> For %A \in \R^{n \times n}, \rank A = \rank (A^\top). We will not prove this in this course in general, but typical proofs
involve first proving that for all %A \in \R^{n \times n}, \rank A \leq \rank (A^\top). Why is this sufficient to complete the proof? Because we
can apply this fact for both %A and %A^\top and use the fact that (%A^\top)^\top = %A:

\begin{eqnarray}
  \rank(%A) & \leq & \rank(%A^\top) \\
  \rank(%A^\top) & \leq & \rank((%A^\top)^\top) \\
  \rank(%A^\top) & \leq & \rank(%A) \\
  \rank(%A^\top) & = & \rank(%A) \\
\end{eqnarray}

To get some intuition about the previous fact, we might consider the following question: if the columns of a matrix %M \in \R^{2 \times 2}
are linearly independent, are the rows also linearly independent?

<!-- <b>Fact:</b> The inverse of an orthogonal matrix is orthogonal. How can this be proven using matrix ranks? -->

<b>Fact:</b> The only matrix in \R^{n \times n} that has no nonzero rows in reduced row echelon form is \I.

<b>Fact:</b> If a matrix %M \in \R^{n \times n} has rank %n, then it must be that \rref %M = \I (and, thus, %M is invertible). This is derived from
the fact above.

<b>Fact:</b> Invertible matrices are closed under the transposition operation. In other words, if %M is invertible, then %M^\top is invertible. How
can we prove this using the rank operator? We know that \rank is preserved under transposition, so we have:

\begin{eqnarray}
  %n & = & \rank(%A) & = & \rank(%A^\top) & = & %n 
\end{eqnarray}

Thus, the rank of \rank(%A^\top) is %n, so it is invertible.

The following table summarizes the important facts about the \rank and \rref operators.


<table class="fig_table">
 <tr><td>\rank (\rref %M) = \rank %M</td></tr>
 <tr><td>\rref (\rref %M ) = \rref %M</td></tr>
 <tr><td>\rank (%M^\top) = \rank %M</td></tr>
 <tr><td>for %M \in \R^{n \times n}, %M is invertible iff \rank %M = %n</td></tr>
</table>

<a name="3.11"></a>
<h3>Matrix similarity</h3>

We introduce a relation on matrices called <i>similarity</i> in order to practice some matrix algebra. We will use the notion of similarity
later in the course in other ways.

<b>Definition:</b> We can define a relation on matrices called <i>similarity</i>. Two matrices %A and %B in \R^{n \times n} are <i>similar</i>,
which we write as

  $$ %A &sim; %B,$$

iff there exists an invertible matrix %C such that

  $$ %A = %C^{-1} %B %C.$$

The relation &sim; is reflexive, symmetric, and transitive. 

<b>Fact:</b> For any %A \in \R^{n \times n},

\begin{eqnarray}
  %A & = & \I %A \I^{-1} \\
  %A & \sim & %A
\end{eqnarray}

so the relation &sim; is reflexive.

<b>Fact:</b> For any %A,%B \in \R^{n \times n}, if %A &sim; %B then we have %C \in \R^{n \times n} such that

\begin{eqnarray}
  %A & = & %C^{-1} %B %C \\
  %C (%A) %C^{-1} & = & %C (%C^{-1} %B %C) C^{-1} \\
  %C (%A) %C^{-1} & = & (%C %C^{-1}) %B (%C C^{-1}) \\
  %C (%A) %C^{-1} & = & %B \\
               %B & = & %C %A %C^{-1} \\
               %B & = & (%C^{-1})^{-1} %A (%C^{-1}).
\end{eqnarray}

Thus, there exists an invertible matrix, namely %C^{-1}, that satisfies the definition for similarity, so %B &sim; %A.

<b>Fact:</b> For any %X,%Y,%Z \in \R^{n \times n}, if %X &sim; %Y and %Y &sim; %Z then we have %X &sim; %Z:

\begin{eqnarray}
  %X & = & %C^{-1} %Y %C \\
  %Y & = & %D^{-1} %Z %D \\
  %X & = & %C^{-1} (%D^{-1} %Z %D) %C \\
  %X & = & (%C^{-1} %D^{-1}) %Z (%D %C) \\
\end{eqnarray}

We know that invertibility is closed under multiplication and
 (%D %C)^{-1} = %C^{-1} %D^{-1}, so %D %C is the invertible matrix that satisfies the definition for similarity between %X and %Z:

\begin{eqnarray}
  %X & = & (%D %C)^{-1} %Z (%D %C)
\end{eqnarray}

Notice that because all invertible matrices can be expressed as a finite product of elementary matrices. So if two matrices are similar, does it
mean they have the same reduced row echelon form? <b>No.</b> We can provide a counterexample.

<a name="4"></a>
<a name="lecture10"></a>
<hr style="margin-bottom:120px;"/>
<h2>Review #1</h2>

This section contains a comprehensive collection of review problems going over the course material covered until this point. These problems are an
accurate representation of the kinds of problems you may see on an exam.

<b>Problem:</b> Find any %h \in \R such that the following two vectors are linearly independent.

\begin{eqnarray}
  #[ 5 #; -5 #; -2 #] , #[ 20 #; -20 #; %h #]
\end{eqnarray}

<solution>
There are many ways to solve this problem. One way is to use the definition of linear dependence and find an %h that does not satisfy it.

\begin{eqnarray}
  %s #[ 5 #; -5 #; -2 #] & = & #[ 20 #; -20 #; %h #]
\end{eqnarray}

Then, we have that any %h such that %h \neq -8 is sufficient to contradict linear dependence (and, thus, imply linear independence):

\begin{eqnarray}
  %s \cdot 5 & = & 20 \\
  %s & = & 4\\
  %s \cdot (-2) & = &  %h \\
  -8 & = &  %h
\end{eqnarray}

Another solution is to recall that orthogonality implies linear independence. Thus, it is sufficient to find %h such that
the two vectors are orthogonal.

\begin{eqnarray}
  #[ 5 #; -5 #; -2 #] \cdot #[ 20 #; -20 #; %h #] & = & 0
\end{eqnarray}

This implies %h = 100.

\begin{eqnarray}
   5(20) + (-5)(-20) + (-2)%h & = & 0\\
  %h & = & 100
\end{eqnarray}

</solution>

<b>Problem:</b> Suppose we have a matrix %M such that the following three equations are true:

\begin{eqnarray}
  %M #[ 1 #; 0 #; 0 #] & = & #[ 3 #; -2 #] \\
  %M #[ 0 #; 1 #; 0 #] & = & #[ 3 #; 0 #] \\
  %M #[ 0 #; 0 #; 1 #] & = & #[ -3 #; 1 #] 
\end{eqnarray}

Compute the following:

\begin{eqnarray}
  %M #[ 2 #; 1 #; -1 #] & = & ? 
\end{eqnarray}

<solution>
We should recall that multiplying a matrix by a canonical unit vector with 1 in the %ith row in the vector gives us the %ith column of the matrix.
Thus, we can immediately infer that:

\begin{eqnarray}
  %M & = & #[ 3 #, 3 #, -3 #; -2 #, 0 #, 1 #]
\end{eqnarray}

Thus, we have:

\begin{eqnarray}
  %M \cdot #[ 2 #; 1 #; -1 #] & = & #[ (3,3,-3) \cdot (2,1,-1) #; (-2,0,1) \cdot (2,1,-1) #] & = & #[ 12 #; -5 #]
\end{eqnarray}

</solution>

<b>Problem:</b> List at least three properties of the following matrix:

  $$ #[ 0 #, 1 #, 0 #; 1 #, 0 #, 0 #; 0 #, 0 #, 1 #] $$

<solution>
The matrix has many properties, such as:
<ul>
 <li>it is an elementary matrix</li>
 <li>it is an invertible matrix</li>
 <li>it is an orthogonal matrix</li>
 <li>it is a symmetric matrix</li>
 <li>it has rank %n</li>
 <li>its reduced row echelon form is the identity</li>
</ul>
</solution>

<b>Problem:</b> List all unit vectors orthogonal to:

  $$ #[ 4 #; -3 #] $$

<solution>
It is sufficient to solve for (%x,%y) that satisfy the following equations:

\begin{eqnarray}
  #[ %x #; %y #] \cdot #[ 4 #; -3 #] & = & 0 
\end{eqnarray}
\begin{eqnarray}
  ||(x,y)|| & = & 1 
\end{eqnarray}

</solution>

<b>Problem:</b> List all the unit vectors that are linearly dependent with:

  $$ #[ 5 #; 12 #] $$

<solution>
It is sufficient to solve for %s and (%x,%y) that satisfy the following equations:

\begin{eqnarray}
  %s #[ %x #; %y #] & = & #[ 5 #; 12 #] 
\end{eqnarray}
\begin{eqnarray}
  ||(x,y)|| & = & 1
\end{eqnarray}

One approach is to write %x and %y in terms of %s and then solve for %s using the second equation.
</solution>

<b>Problem:</b> Find the matrix %B \in \R^{2 \times 2} that is symmetric, has a constant diagonal, and satisfies

\begin{eqnarray}
  %B #[ 2 #; 1 #] & = & #[ 15 #; 6 #]
\end{eqnarray}

<solution>
We know that %B is symmetric and has a constant diagonal, so we need to solve for %a and %b in:

\begin{eqnarray}
  #[ a #, b #; b #, a#] \cdot #[ 2 #; 1 #] & = & #[ 15 #; 6 #]
\end{eqnarray}
</solution>

<b>Problem:</b> Compute the inverse of the following matrix:

  $$ #[ 2 #, 3 #; 1 #, 2 #] $$

<solution>
One approach is to set up the following equation and solve for %a,%b,%c, and %d:

\begin{eqnarray}
  #[ a #, b #; c #, d #] \cdot #[ 2 #, 3 #; 1 #, 2 #] & = & #[ 1 #, 0 #; 0 #, 1 #]
\end{eqnarray}

Another approach is to apply the formula for the inverse of a matrix in \R^{2 \times 2}:

\begin{eqnarray}
  #[ 2 #, 3 #; 1 #, 2 #]^{-1} & = & (1 / (2 \cdot 2 - 3 \cdot 1)) \cdot #[ 2 #, -3 #; -1 #, 2 #] & = & #[ 2 #, -3 #; -1 #, 2 #]
\end{eqnarray}
</solution>

<b>Problem:</b> Let %a \in \R be such that %a \neq 0. Compute the inverse of the following matrix:

  $$ #[ %a #, -%a #; -%a #, -%a #] $$

<solution>
As in the previous problem, we can either solve an equation or apply the formula:

\begin{eqnarray}
  #[ %a #, -%a #; -%a #, -%a #]^{-1} & = & (1 / (-%a^2 - %a^2)) \cdot #[ -%a #, %a #; %a #, %a #]
\end{eqnarray}
</solution>

<b>Problem:</b> In terms of %a, %b \in \R where %a \neq 0 and %b \neq 0, compute the inverse of:

  $$ #[ %a #, 0 #; 0 #, %b #] #[ %a #, 0 #; 0 #, %b #] #[ %a #, 0 #; 0 #, %b #] #[ %a #, 0 #; 0 #, %b #] $$

<solution>
While we could perform the instances of matrix multiplication step-by-step and then invert the result (either by solving an equation or
using the formula for the inverse of a matrix in \R^{2 \times 2}), it's easier to recall that diagonal matrices behave in a manner that is very
similar to the real numbers. Thus, the above product is equal to

  $$ #[ %a^4 #, 0 #; 0 #, %b^4 #], $$

and its inverse simply has the multiplicative inverses of the two diagonal entries as its diagonal entries:

  $$ #[ 1/%a^4 #, 0 #; 0 #, 1/%b^4 #].$$

</solution>

<b>Problem:</b> Let %A \in \R^{2 \times 2} be an orthogonal matrix. Compute %A %A^\top %A.

<solution>
We recall that because %A is orthogonal, %A^\top = %A^{-1}. Thus, we have that

  $$ %A %A^\top %A = (%A %A^{-1}) %A = \I %A = %A.$$
</solution>

The following problems are slightly more difficult. Only one or two such problems would be found on an exam.

<b>Problem:</b> Suppose %x \in \R is such that %x \neq 0. Compute the inverse of the following matrix:

  $$ #[ %x #, 0 #, -2%x #; 0 #, %x #, 0 #; 0 #, 0 #, 4%x #] $$

<solution>
Because we have an upper triangular matrix, computing the inverse by solving the following equation is fairly efficient. Start by considering
the bottom row and its product with each of the columns. This will generate the values for %g, %h, and %i. You can then proceed to the other
rows.

\begin{eqnarray}
  #[ %x #, 0 #, -2%x #; 0 #, %x #, 0 #; 0 #, 0 #, 4%x #] #[ %a #, %b #, %c #; %d #, %e #, %f #; %g #, %h #, %i #] & = & #[ 1 #, 0 #, 0 #; 0 #, 1 #, 0 #; 0 #, 0 #, 1 #]
\end{eqnarray}

The solution is:

  $$ #[ 1/%x #, 0 #, 1/(2%x) #; 0 #, 1/%x #, 0 #; 0 #, 0 #, 1/4%x #] $$

</solution>

<b>Problem:</b> Given an invertible upper triangular matrix %M \in \R^{2 \times 2}, show that %M^{-1} is also upper triangular. <b>Hint:</b> write out
the matrix %M explicitly.

<solution>
Suppose we have the following upper triangular matrix:

\begin{eqnarray}
  %M & = & #[ %x #, %y #; 0 #, %z #]
\end{eqnarray}

If it is invertible, then there exists an inverse such that:

\begin{eqnarray}
  #[ %x #, %y #; 0 #, %z #] \cdot #[ %a #, %b #; %c #, %d #] & = & #[ 1 #, 0 #; 0 #, 1 #] 
\end{eqnarray}

This implies that

\begin{eqnarray}
  %x%a + %y%b & = & 1 \\
  %x%c + %y%d & = & 0 \\
         %z%c & = & 0 \\
         %z%d & = & 1
\end{eqnarray}

Because %M is invertible, we know that %z \neq 0. Since %z%c = 0, This means that %c = 0. Thus, we have that the inverse is upper triangular.
</solution>

<b>Problem:</b> Find a matrix that is not the identity matrix, but is similar only to itself. Explain why it cannot be similar to any other matrix.

<solution>
The only such matrix is:

\begin{eqnarray}
   %M & = & #[ 0 #, 0 #; 0 #, 0 #] 
\end{eqnarray}

It cannot be similar to any other matrix because for any invertible matrix %C, %C^{-1} %M %C will be %M. Thus, the set of all matrices similar to
%M is {%M}:

   $$ {%A | %A \in \R^{2 \times 2}, %A ~ %M} = {%M} $$

</solution>

<b>Problem:</b> Find a matrix that is similar but not equivalent to the following matrix:

\begin{eqnarray}
   %M & = & #[ 2 #, 4 #; 1 #, 2 #] 
\end{eqnarray}

<solution>
It suffices to pick a non-trivial invertible matrix %E, such as an elementary matrix, and compute %E^{-1} %M %E. One option is:

\begin{eqnarray}
   %E^{-1} %M %E & = & #[ 1 #, 1 #; 0 #, 1 #] #[ 2 #, 4 #; 1 #, 2 #] #[ 1 #, -1 #; 0 #, 1 #] & = & #[ 3 #, 3 #; 1 #, 2 #]
\end{eqnarray}

</solution>

<b>Problem:</b> Suppose that a gram of gold costs $50, while a gram of silver costs $10. After purchasing some of each, you have spent $350 on
15 grams of material. How many grams of each commodity have you purchased?
<ol>
 <li>Write down four dimensions describing this system.
<solution>
  <ul>
   <li>grams of gold</li>
   <li>grams of silver</li>
   <li>cost</li>
   <li>total weight in grams</li>
  </ul>
</solution>
 </li>
 <li>
  Define a matrix %A that can be used to convert a description of a system state that specifies only the amount of gold and silver purchased into
  a description of the system state that specifies only the cost and total weight.
<solution>
\begin{eqnarray}
   %A & = & #[ 1 #, 1 #; 50 #, 10 #] 
\end{eqnarray}
</solution>
 </li>
 <li>Write down a matrix equation describing this problem and solve it to find the solution.
<solution>
\begin{eqnarray}
   #[ 1 #, 1 #; 50 #, 10 #] \cdot #[ %x #; %y #] & = & #[ 15 #; 350 #] \\
                                  #[ %x #; %y #] & = & #[ 5 #; 10 #]
\end{eqnarray}
</solution>
 </li>
 <li>
  Define a matrix %B such that for any description of a system state %v that specifies only the total weight and amount spent, %B%v
  is a description of that system state that specifies the amount of gold and silver in that system state.
<solution>
We are looking for the inverse of %A. 
\begin{eqnarray}
   %A^{-1} & = & #[ -1/4 #, 1/40 #; 5/4 #, -1/40 #]
\end{eqnarray}
</solution>
 </li>
 <li>
  Suppose that the per gram price of gold and silver were the same. What can you say about the properties of the matrix %A?
<solution>
  The matrix %M is not invertible (it is singular).
</solution>
 </li>
</ol>

<a name="lecture11"></a>
<a name="5"></a>
<hr style="margin-bottom:120px;"/>
<h2>Vector Spaces</h2>

<a name="5.1"></a>
<h3>Sets of vectors and their notation</h3>

We are interested in studying <i>sets of vectors</i> because they can be used to model sets of system states (and, thus, entire systems), observations
and data that might be obtained about systems, geometric shapes and regions, and so on. As we did with vectors and matrices, we can introduce a symbolic
language for sets of vectors consisting of symbols, operators, and predicates. And, as with vectors and matrices, we can study the 
algebraic laws that govern symbolic expressions.

We will consider three kinds of sets of vectors in this course; they are listed in the table below.

<table class="fig_table">
 <tr>
  <td><b>kind of set (of vectors)</b></td>
  <td><b>maximum<br/>cardinality<br/>("quantity of<br/>elements")</b></td>
  <td><b>solution space of a...</b></td>
  <td><b>examples</b></td>
 </tr>
 <tr>
  <td>finite set of vectors</td>
  <td>finite</td>
  <td></td>
  <td>
   <ul style="padding-left:14px;">
    <li>{(0,0)}</li>
    <li>{(2,3),(4,5),(0,1)}</li>
   </ul>
  </td>
 </tr>
 <tr>
  <td>vector space</td>
  <td>infinite</td>
  <td>homogenous system of<br/>linear equations:<br/>%A%v = \0</td>
  <td>
   <ul style="padding-left:14px;">
    <li>{(0,0)}</li>
    <li>\R</li>
    <li>\R^2</li>
    <li>\span{(1,2),(2,3),(0,1)}</li>
    <li>any point, line, or plane<br/>intersecting the origin</li>
   </ul>
  </td>
 </tr>
 <tr>
  <td>affine space</td>
  <td>infinite</td>
  <td>nonhomogenous<br/>system of<br/>linear equations:<br/>%A%v = %w</td>
  <td>
   <ul style="padding-left:14px;">
    <li>{ %a + %v | %v \in %V} where<br/>%V is a vector space and %a is a vector</li>
    <li>any point, line, or plane</li>
   </ul>
  </td>
 </tr>
</table>

To represent finite sets of vectors symbolically, we adopt the convention of simply listing the vectors between a pair of braces (as with any
set of objects). However, we need a different convention for symbolically representing vector spaces and affine spaces. This is because we
must use a symbol of finite size to represent a vector space or affine space that may contain an infinite number of vectors.

We first recall the <a href="#2.2">definition for what constitutes a vector space</a> (there are many equivalent definitions).

<b>Definition:</b> A <i>vector space</i> is a set of vectors that contains \0, is closed under vector addition and scalar multiplication, and is
such that all the elements in the set satisfy the vector space <a href="#2.2">axioms</a> governing vector addition and scalar multiplication.

<b>Fact:</b> Any set of linear combinations of a collection of vectors is closed under vector addition and scalar multiplication, contains
\0, and satisfies the vector space axioms. In other words, for any collection of vectors %v_1, ..., %v_n, \span{%v_1, ..., %v_n} is a vector space.

<b>Fact:</b> For any set of vectors %V in \R^{n \times n} that satisfies the vector space axioms, there exists a finite set of at most %n vectors
 %v_1, ..., %v_k (where %k \leq %n) such that \span{%v_1, ..., %v_k} = %V.

Given the two facts above, we can safely adopt \span{%v_1, ..., %v_n} as a standard notation for vector spaces. In addition to this notation,
we will also often use the notation \R^n for specific values of %n (e.g., \R^2 is equivalent to \span{[1;0],[0;1]}), and {\0} for specific
vector \0 (e.g., {[0;0]} is equivalent to \span{[0;0]}).

<a name="5.2"></a>
<h3>Equality of sets of vectors and vector spaces</h3>

Recall that many of the algebraic laws we saw governing operators on vectors and matrices involved equality of vectors and matrices. In fact, one
can view these laws as collectively <i>defining</i> the semantic equality of the symbols (i.e., they specify when two symbols refer to the same
<i>object</i>). Thus, the <i>meaning</i> of these symbols is closely tied to the equality relation we define over them.

Because we are considering <i>sets</i> of vectors, and equality of pairs of vectors has already been defined, it is relatively straightforward
to provide a definition of equality for sets of vectors (both finite and infinite).

<b>Definition:</b> We have the following definitions:

\begin{eqnarray}
 %w \in %V & iff & \exists %v \in %V, %v = %w \\
 %W \subset %V & iff & \forall %w \in %W, %w \in %V \\
 %W = %V & iff & %W \subset %V and %V \subset %W
\end{eqnarray}

It is easy to use the above to compute whether two finite sets of vectors are equivalent. However, while the above definition is mathematically
adequate for any two infinite sets of vectors (including vector spaces), it does not help us define a practical algorithm for computing equality
for our chosen notation for vector spaces (spans of finite sets of vectors). Suppose we take the definition above and replace the vector spaces
with explicit spans of finite sets of vectors. 

<b>Definition:</b>  Let %W and %V be finite sets of vectors.

\begin{eqnarray}
 %w \in \span %V & iff & \exists %v \in \span %V, %v = %w \~ (1)\\
 \span %W \subset \span %V & iff & \forall %w \in \span %W, %w \in \span %V \~ (2)\\
 \span %W = \span %V & iff & \span %W \subset \span %V and \span %V \subset \span %W \~ (3)
\end{eqnarray}

We still have a problem: how do we try all possible vectors %w in \span %W (see the \forall quantifier on the right-hand side
of the statement (2))? This can be addressed
by observing that if %w_1,...,%w_n \in %V, then \span %w_1,...,%w_n \in %V. Thus, it is sufficient to check that the finite number of
vectors in %W are found in %V.

There is another problem. How do we determine whether a given vector %w is in the vector space that we call \span %V? Notice that determining this
is equivalent to determining whether %w is a <i>linear combination</i> of the vectors in %V (we assume that the size of the finite set %V is %n).
Because %V is finite set of %n vectors, we can always find a matrix %M_V that has the vectors in %V as its columns.

\begin{eqnarray}
 %w \in \span %V & iff & %w \in {%v | \exists %a_1,...,%a_n \in \R, %v_1,...,%v_n \in %V, &nbsp;&nbsp; %v = %a_1%v_1 + ... + %a_n%v_n } \\
                 & iff & \exists %a_1,...,%a_n \in \R, %v_1,...,%v_n \in %V, &nbsp;&nbsp; %w = %a_1%v_1 + ... + %a_n%v_n \\
                 & iff & \exists %x \in \R^n, &nbsp;&nbsp; %M_V \cdot %x = %w
\end{eqnarray}

Thus, determining whether %w \in \span %V is equivalent to determining whether there is a solution to the equation %M_V \cdot %x = %w. We
can solve such equations in general by building an augmented matrix and then finding its reduced row echelon form.

<a name="lecture12"></a>

<b>Example:</b> Consider the following example of how one can solve an equation %M%v = %w by finding the reduced row echelon form of an augmented
matrix.

@
%[
M & := & [1#,2#,4; 5#,6#,8; -3#,1#,4] \\
w & := & [4;5;8]
%]

\rref \augment(M,w) # the solution x to Mx=w
/@

We have now found a process that allows us to determine for a particular %W and %V whether \span %W = \span %V:

\begin{eqnarray}
 \span %W = \span %V & iff & \span %W \subset \span %V and \span %V \subset \span %W \\
 \span %W \subset \span %V & iff & \forall %w \in \span %W, %w \in \span %V \\
 \forall %w \in \span %W, %w \in \span %V & iff & \forall %w \in %W, %w \in \span %V \\
 %w \in \span %V & iff & \exists %x \in \R^n, &nbsp;&nbsp; %M_V \cdot %x = %w 
\end{eqnarray}

The above implies that a matrix can be used to represent a particular vector space. We will see in other sections
further below that a given vector space can be represented in more than one way by a matrix, and that more than one matrix can be used
to represent a vector space.

<a name="5.3"></a>
<h3>Vector spaces as abstract structures</h3>

Our definitions of a vector space so far have explicitly referenced sets of concrete vectors as we usually understand them. However, this is not
necessary.

<b>Definition:</b> A <i>vector space</i> is a set of objects %S such that
<ul>
 <li>there is a unique additive identity \0 \in %S</li>
 <li>addition (+) is an operation on elements of %S under which %S is closed, and which satisfies the vector space axioms</li>
 <li>scalar multiplication (\cdot) is an operation on elements of %S under which %S is closed, and which satisfies the vector space axioms</li>
</ul>

The above definition specifies conditions under which any set of objects %S can be studied as a vector space. 


<b>Fact:</b> The affine space %A = {%a + %v | %v \in %V} is a vector space for appropriate definitions of addition, scalar multiplication, and identity.
<ul>

 <li>addition (\oplus) can be defined as follows. It is an operation on elements of %A under which %A is closed, and which satisfies the vector space axioms:

  $$%v \oplus %w = %u \~ where \~ %u = (%v - %a) + (%w - %a) + %a = %v + %w - 2%a + %a = %v + %w - %a.$$

 </li>
 <li>scalar multiplication (\otimes) can be defined as follows. It is an operation on elements of %A under which %A is closed, 
     and which satisfies the vector space axioms:

  $$%s \otimes %v = %u \~ where \~ %u = (%s \cdot (%v - %a)) + %a = %s%v - %s%a  + %a = %s%v + (1-%s)%a.$$

 </li>
 <li>there is a unique additive identity in %A; it is the vector %a:

  $$%v \oplus %a = %v + %a - %a = %v.$$

 </li>
</ul>

An abstract definition of vector spaces is useful because it also allows
us to study by analogy and learn about other objects that are not necessarily sets of vectors. All the properties we can derive about vector
spaces using the above definition will apply to other sets of objects that also satisfy the above definition.

<b>Example:</b> We consider the set of functions %F = {%f | %f(%x) = %c%x, %c \in \R}. How do we show that %F is a vector
space?
<ul>
 <li>there is a unique additive identity in %F:  %f(%x) = 0%x</li>
 <li>addition (+) is an operation on elements of %F under which %F is closed, and which satisfies the vector space axioms:

  $$%f + %g = %h \~ where \~ %h(%x) = %f(%x) + %g(%x)$$

 </li>
 <li>scalar multiplication (\cdot) is an operation on elements of %F under which %F is closed, and which satisfies the vector space axioms:

  $$%s \cdot %f = %h \~ where \~ %h(%x) = %s \cdot %f(%x)$$

 </li>
</ul>

<b>Exercise:</b> Show that {%f | %f(%x) = %b%x^2 + %c%x, %b,%c \in \R} is a vector space.

<b>Exercise:</b> Find two elements in {%f | %f(%x) = %b%x^2 + %c%x + %d, %b,%c,%d \in \R} that are linearly independent. What does it mean for
two functions to be linearly independent?

<b>Exercise:</b> What is a finite set of functions that spans {%f | %f(%x) = %b%x^2 + %c%x + %d, %b,%c,%d \in \R}? One such set is:

  $${%f,%g,%h} \~ where \~ %f(%x) = 1, \~ %g(%x) = %x, \~ %h(%x) = %x^2$$

<b>Example:</b> We consider the set of functions {%f | %f(%x) = x^k, %k \in \R}. The following definitions of addition and scalar multiplication
make this set of functions a vector space.
<ul>
 <li>there is a unique additive identity:  %f(%x) = %x^0</li>
 <li>addition (+) is defined as:

  $$%f + %g = %h \~ where \~ %h(%x) = %f(%x) \cdot %g(%x)$$

 </li>
 <li>scalar multiplication (\cdot) is an operation on elements of %S under which %S is closed, and which satisfies the vector space axioms:

  $$%s \cdot %f = %h \~ where \~ %h(%x) = %f(%x)^s$$

 </li>
</ul>

<b>Example:</b> Just as with any vector space, we can set up and solve problems involving linear combinations of vectors. Suppose we want to find a
function in the space {%f | %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d, %a,%b,%c,%d \in \R} that fits a certain collection of points, such as:

  $${(1,3), (-1,13), (2,1), (-2,33)}$$

We can interpret each point as a pair (%x, %f(%x)). We can then set up the following system of equations:

\begin{eqnarray}
 %f(1) & = & 3 \\
 %f(-1) & = & 13 \\
 %f(2) & = & 1 \\
 %f(-2) & = & 33
\end{eqnarray}

Expanding, we have:

\begin{eqnarray}
 %a(1)^3 + %b(1)^2 + %c(1) + %d & = & 3 \\
 %a(-1)^3 + %b(-1)^2 + %c(-1) + %d & = & 13 \\
 %a(2)^3 + %b(2)^2 + %c(2) + %d & = & 1 \\
 %a(-2)^3 + %b(-2)^2 + %c(-1) + %d & = & 33 \\
\end{eqnarray}

Notice that we can rewrite this in the form of an equation %M %v = %w:

\begin{eqnarray}
 #[ (1)^3 #, (1)^2 #, (1) #, 1 #; (-1)^3 #, (-1)^2 #, (-1) #, 1 #; (2)^3 #, (2)^2 #, (2) #, 1 #; #; (-2)^3 #, (-2)^2 #, (-2) #, 1 #] 
  \cdot  #[ %a #; %b #; %c #; %d #] & = & #[ 3 #; 13 #; 1 #; 33 #] 
\end{eqnarray}

This shows us that we can interpret individual objects in {%f | %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d, %a,%b,%c,%d \in \R} as vectors in \R^4.
We can also view this problem in terms of systems, system states, and observations. If the function %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d that solves
the above system is a state of the system, then (3, 13, 1, 33) is a <i>partial</i> observation of that state along four distinct "dimensions".

Note that finding the equation for a line that crosses two points is just a special case of the above.

<b>Example:</b>  We want to find a function %f(%x) = %c%x + %d such that the points (2,3) and (5,-4) fall on the line the equation represents. Thus,
we have:

\begin{eqnarray}
 #[ (2) #, 1 #; (5) #, 1 #] \cdot #[ %c #; %d #] & = & #[ 3 #; -4 #] 
\end{eqnarray}

We can consider many spaces of functions beyond spaces of polynomial functions. For example, suppose \exp(%x) = %e^x.
Then we can define the following vector space:

  $$ \span{ \exp, \cos, \sin }.$$

We can also generalize this approach to families of related functions.

<a name="lecture12"></a>
<b>Example:</b> Consider the following set of vectors of functions, where %f' denotes the derivative of a function %f:

  $$ %F = { (%f, %f, %f') \~ | \~ %f(%x) = %b%x^2 + %c%x + %d }$$

The set %F is a vector space. Now, suppose we have the following problem. Find the function for a parabola in %F such that 
(1,4) and (2,-3) lie on the parabola, and the maximum or minimum point on the parabola is at %x = 1. Such a function is
represented by the solution to the following equation:

\begin{eqnarray}
 #[ (1)^2 #, (1) #, 1 #; (2)^2 #, (2) #, 1 #; 2(1) #, 1 #, 0 #] \cdot #[ %b #; %c #; %d #] & = & #[ 4 #; -3 #; 0 #]
\end{eqnarray}

Note that the number of rows in the matrix (and result vector) corresponds to the number of data points for which
a model is being sought, and is not bounded.

<b>Example:</b> Find the order 5 polynomial that exactly fits the points [-1;45], [8;4], [6;5], [2;-8], [-10;8], [15;6].

@
%[
P & := & {
  [-1;45], 
  [8;4], 
  [6;5], 
  [2;-8], 
  [-10;8], 
  [15;6] }
%]

%[
M & := & (\augment { [x^5, x^4, x^3, x^2, x, 1]^\t | [x;y] \in P })^\t \\
C & := & (\augment { [y] | [x;y] \in P })^\t \\
B & := & \augment (M, C) \\
A & := & \rref B
%]
/@

The disadvantage to using this particular approach to fitting functions to data points is that the data must
match an actual function perfectly (or, equivalently, the space of functions being considered must be rich 
enough to contain a function that can fit the data points exactly). We will see further below
how to find functions that have the "best" fit (for one particular definition of "best") without necessarily matching the points exactly.

<a name="5.4"></a>
<h3>Subspace relation on vector spaces</h3>

<b>Definition:</b> Given two vector spaces %W and %V, we have:

  $$ %W is a (vector) subspace of %V \~ iff \~ %V \subset %W.$$

<a name="5.5"></a>
<h3>Finding a basis and an orthonormal basis of a vector space</h3>

We have adopted \span{%v_1,...,%v_n} as our notation for vector spaces. However, this makes it possible to represent a given vector space
in a variety of ways: 

  $$\span{(0,1),(1,0)} = \span{(0,1),(1,0),(1,0)} = \span{(0,1),(1,0),(1,0),(1,0)}, and so on.$$

We might naturally ask: given
a vector space %V, what is the <i>smallest</i> possible size for a finite set of vectors %W such that %V = \span %W?

<b>Definition:</b> A finite set of vectors %B is a <i>basis</i> of the vector space %V if \span %B = %V and for all finite 
sets of vectors %W such that \span %W = %V, |%B| \leq |%W|.

<b>Fact:</b> A finite set of vectors %v_1,...,%v_n is a basis for %V iff \span {%v_1,...,%v_n} = %V and the vectors %v_1,...,%v_n
are setwise linearly independent.

<b>Fact:</b> Given a finite set of vectors %v_1,...,%v_n, we can find the basis for \span {%v_1,...,%v_n} by creating a matrix %M in which the
rows are the vectors %v_1,...,%v_n and computing \rref %M. The set of distinct nonzero rows in \rref %M is a basis for \span {%v_1,...,%v_n}.

<b>Example:</b> Below, we find the basis of \span {[-3;0;7;0], [-9;0;21;0], [0;0;2;8], [0;0;1;4], [3;0;13;4], [0;0;1;0]}.

@
%[
A & := & \augment{[-3;0;7;0], [-9;0;21;0], [0;0;2;8], [0;0;1;4], [3;0;13;4], [0;0;1;0]} \\
B & := & A^\t \\
%]

%[
\decompose ((\rref B)^\t) - {[0;0;0;0]}
%]
/@

<a name="lecture13"></a>

<b>Definition:</b> A finite set of vectors %B is an <i>orthonormal basis</i> of the vector space %V if %B is a basis of %V,
all the vectors in %B are unit vectors, and the vectors in %B are setwise orthogonal.

Recall that for any vector %v \in \R^n and any unit vector %u \in \R^n, (%v \cdot %u) \cdot %u is the projection of %v onto the line parallel to
%u (i.e., the vector space {%a %u | %a \in \R}). We can use this fact to define a process for turning an arbitrary basis into an orthonormal basis.

<a name="algorithmgramschmidt"></a>
<b>Fact:</b> Given a basis %B = {v_1,...,v_n}, it is possible to turn it into an orthonormal basis by using the Gram-Schmidt process. This
algorithm can be summarized as follows.

\begin{eqnarray}
 %u_1 & = & %v_1 & \~ & %e_1 = %u_1 / ||%u_1 || \\
 %u_2 & = & %v_2 - ((%v_2 \cdot %e_1) \cdot %e_1) & \~ & %e_2 = %u_2 / || %u_2 || \\
 %u_3 & = & %v_3 - ((%v_3 \cdot %e_1) \cdot %e_1) - ((%v_3 \cdot %e_2) \cdot %e_2) & \~ & %e_3 = %u_3 / || %u_3 || \\
 %u_4 & = & %v_4 - ((%v_4 \cdot %e_1) \cdot %e_1) - ((%v_4 \cdot %e_2) \cdot %e_2) - ((%v_4 \cdot %e_3) \cdot %e_3)  & \~ & %e_4 = %u_4 / || %u_4 || \\
      & \vdots &    & \~ &  \\
 %u_n & = & %v_n - &Sigma;_{i=1}^{n-1} ((%v_n \cdot %e_i) \cdot %e_i) & \~ & %e_n = %u_n / || %u_n ||
\end{eqnarray}

Intuitively, for each vector %v_i \in %B, the algorithm substracts out the contributions of the already-computed orthogonal unit vectors from
%v_i, obtaining %u_i, and then rescales %u_i to make it into a unit vector.

Many computational environments for mathematics that support linear algebra provide an operator for turning a basis into an orthonormal basis. An example is
provided below.

@
%[
\orthonormal {[4;0], [1;2]}
%]
/@

Why is an orthonormal basis useful? Once again, we appeal to the fact that (%v \cdot %u) \cdot %u is the projection of %v onto %u, where
%v \cdot %u is the length of that projection. Suppose we want to determine how to express an arbitrary vector %v using an orthonormal
basis %u_1,...,%u_n.

<b>Fact:</b> Given a vector %v \in \R^n, and an orthonormal basis %B =  {%u_1,...,%u_n} for \R^n, we can determine what linear combination of the vectors
%u_1,...,%u_n yields %v by computing:

  $$%a = (%M_B)^\top \cdot %v.$$

Notice that the components of the vector %a, which we will call %a_1,...,%a_n, are the scalars needed to compute %v as a 
linear combination of the vectors %u_1,...,%u_n:

  $$%v = %a_1 %u_1 + ... + %a_n %u_n = %M_B %a.$$

<b>Fact:</b> Given a vector %v \in \R^n, some %k < %n, and an orthonormal basis %B = {%u_1,...,%u_k} for a subspace %W \subset \R^n,
the product (%M_B \cdot (%M_B)^\top \cdot %v) is the <i>projection</i> of %v onto %W.

<b>Example:</b> Notice the relationship of the above fact to projection onto the span of a single unit vector %u = (%x,%y) in \R^2.
Suppose we have a basis %B = {%u} for a subspace {%s \cdot %u | %s \in \R^2} \subset \R^2. Then we have

\begin{eqnarray}
  %M_B & = & #[ %x #; %y#]
\end{eqnarray}
  
Thus, for an arbitrary %v \in \R^2 not necessarily in {%s \cdot (%x,%y) | %s \in \R^2} we have:

\begin{eqnarray}
 %M_B \cdot (%M_B)^\top \cdot %v & = & #[ %x #; %y#] \cdot #[ %x #; %y#]^\top \cdot %v \\
                                     & = &   #[ %x #; %y#] \cdot ( #[ %x #; %y#] \cdot %v )  \\
                                     & = & ( #[ %x #; %y#] \cdot %v ) \cdot #[ %x #; %y#] \\
                                     & = & ( %v            \cdot #[ %x #; %y#] ) \cdot #[ %x #; %y#] \\
                                     & = & ( %v \cdot %u ) \cdot %u
\end{eqnarray}

Thus, we see that the projection of a vector onto another unit vector in \R^2 is just a special case of the general approach to projection.

<!--assignment3-->
<br/><hr/>
<a name="5.5.b"></a>
<a name="assignment3"></a>
<b>Assignment #3: Vector Spaces</b> <!--span class="btn_assignment">(<a href="materials.php?hw=3">show only this assignment</a>)</span-->

       <p>In this assignment you will work with vector spaces, including vector spaces of functions.</p>

<ol>
  <li>
    <ol style="list-style-type:lower-alpha;">
      <li> Use the appropriate operators to calculate a basis for the following vector spaces.

@
%[
V1 := \span {
        [3;2;5;0;8], 
        [1;0;-6;7;5], 
        [0;0;1;0;1], 
        [-6;-4;-10;0;-16]
      }
%]

%[
V2 := \span {
        [1;0;-4;0],
        [2;0;3;1],
        [1;0;-6;7],
        [0;0;1;0],
        [4;0;8;-5],
        [-1;0;-1;0]
      }
%]

%[
V3 := \span { [1;3;-9], [-2;-6;18], [4;0;0] }
%]
/@

<solutionHomework>

@
%[
S1 := { [3;2;5;0;8], [1;0;-6;7;5], [0;0;1;0;1], [-6;-4;-10;0;-16] }
%]

%[
S2 := { [1;0;-4;0], [2;0;3;1], [1;0;-6;7], [0;0;1;0], [4;0;8;-5], [-1;0;-1;0] }
%]

%[
S3 := { [1;3;-9], [-2;-6;18], [4;0;0] }
%]

%[
\decompose ((\rref (\augment S1)^\t)^\t) - {[0;0;0;0;0]} \\
\decompose ((\rref (\augment S2)^\t)^\t) - {[0;0;0;0]} \\
\decompose ((\rref (\augment S3)^\t)^\t) - {[0;0;0]} \\
%]
/@

</solutionHomework>

      </li>
      <li> Complete the following argument showing that \span{ [1;2], [2;4] } = \span {[3;6]} using the propositions provided in the verifier. <b>Hint:</b> 
      find an explicit vector %x \in \R^2 and add it to the second premise in place of <b>undefined</b>; use a similar approach (and a similar
      but distinct proposition) when building the part of the argument for the other direction of \subset.

@
\forall x \in \R^2,
     %[
    x = \undefined # Find an explicit vector to put here.
    %]
  \implies

    # ...
    %[
    \augment ( [1;2], [2;4] ) * x = [3;6] \and  # Hint (keep this in your argument).
    %]
    # ...

    %[
    \span{ [1;2], [2;4] } = \span {[3;6]}
    %]
/@

<solutionHomework>

@
\forall x \in \R^2,
    %[
    { [1;2], [2;4] } \subset \span {[3;6]} \and \\
    %]
    %[
    \augment ( [1;2], [2;4] ) * x & = & [3;6] \and \\
    x & = & [1;1]  \\
    %]
  \implies
    %[
    \augment ( [1;2], [2;4] ) * [1;1] & = & [3;6] \and \\
    [3;6] & \in & \span { [1;2], [2;4] } \and \\
    {[3;6]} & \subset & \span { [1;2], [2;4] } \and \\
    \span{ [1;2], [2;4] } & \subset & \span {[3;6]} \and \\
    \span {[3;6]} & \subset & \span{ [1;2], [2;4] } \and \\
    \span{ [1;2], [2;4] } & = & \span {[3;6]}  \\
    %]
/@

</solutionHomework>

      </li>
      <li> Complete the following argument: show that the assumptions imply that the spans are equivalent.

@
\forall a,b,c,d,v,v',w,w' \in \R^2, \forall A,B \in \R^(2 \times 2),
    %[
    A & = & \augment(w,w') \and \\
    B & = & \augment(v,v') \and \\
    A * a & = & v \and \\
    A * b & = & v' \and \\
    B * c & = & w \and \\
    B * d & = & w' \\
    %]
  \implies
    # ...
    %[
    \span {w,w'} = \span {v,v'}
    %]
/@


<solutionHomework>

@

\forall a,b,c,d,v,v',w,w' \in \R^2, \forall A,B \in \R^(2 \times 2),
    %[
    A & = & \augment(w,w') \and \\
    B & = & \augment(v,v') \and \\
    A * a & = & v \and \\
    A * b & = & v' \and \\
    B * c & = & w \and \\
    B * d & = & w' \\
    %]
  \implies
    %[
    \augment(w,w') * a & = & v \and \\
    \augment(w,w') * b & = & v' \and \\
    \augment(v,v') * c & = & w \and \\
    \augment(v,v') * d & = & w' \and \\
    v & \in & \span {w,w'} \and \\
    v' & \in & \span {w,w'} \and \\
    w & \in & \span {v,v'} \and \\
    w' & \in & \span {v,v'} \and \\
    {v,v'} & \subset & \span {w,w'} \and \\
    {w,w'} & \subset & \span {v,v'} \and \\
    \span {v,v'} & \subset & \span {w,w'} \and \\
    \span {w,w'} & \subset & \span {v,v'} \and \\
    \span {w,w'} & = & \span {v,v'} \\
    %]

/@

</solutionHomework>

      </li>
    </ol>
  </li>
  <li> <p>Find the order 9 polynomial that exactly fits the following data. Your solution must be in the form of a vector
          in \R^{10} that represents the coefficients of the polynomial.</p>

@
%[
P := {
  [-1;  31],
  [-4;  268327],
  [ 1;  17],
  [-5; -1520701],
  [ 3;  389147],
  [-3;  78869],
  [-6; -19979509],
  [ 5;  29571949],
  [ 0; -1],
  [-2;  5027] } \\
%]
/@

       <p><b>Hint:</b> you can make your answer shorter by using a comprehension to generate a one-row matrix containing the different powers of %x.
       For example:</p>

@
x := 2
\augment ([x^k] |  k \in {0 ... 9})
/@

<solutionHomework>

@
%[
P := {
  [-1;  31],
  [-4;  268327],
  [ 1;  17],
  [-5; -1520701],
  [ 3;  389147],
  [-3;  78869],
  [-6; -19979509],
  [ 5;  29571949],
  [ 0; -1],
  [-2;  5027] } \\
%]

# This is the right-hand side of the equation.
%[
w := (\augment ( [y] | [x;y] \in P))^\t \\
%]

# We generate the matrix using a nested comprehension.

%[
S := ( (\augment ([x^(9-k)] |  k \in {0 ... 9}))^\t | [x;y] \in P )
%]

# This is the matrix in the left-hand side of the equation.
M := (\augment S)^\t

# The matrix is invertible, so we can multiply both sides
# of the equation by the inverse of M to obtain the
# solution.
(M^(-1)) * w

# Alternatively, we can find the rref of the augmented
# matrix.
\rref \augment(M,w)

# The solution vector has the x^9 coefficient as its first entry.
# If we had used [x^k] instead of [x^(9-k)] in our formula for S,
# this would be reversed.
/@

</solutionHomework>

  </li>
  <li> Consider the order 3 polynomial %f(%x) = 2%x^3 + 5%x^2 - 3%x + 1.
    <ol style="list-style-type:lower-alpha;">

      <li> <p>Find the order 2 polynomial that is the closest least squares approximation of %f on the set {-2,0,2,4,8,16}.</p>

@
{ 2 (x^3) + 5 (x^2) - 3 x + 1 | x \in {-2,0,2,4,8,16} }
/@

      </li>
      <li> Compute the error of the approximation from part (a) above on the domains {-2,0,2,4,8,16} and {-16,-8,-4,-2,-1,0}.
      </li>
    </ol>

<br/><b>Solution to #3 (both parts):</b><br/>
<solutionHomework>

@
# The points to which we're trying to find an approximately
# fitting function.
%[
Points3a := { [x; 2 (x^3) + 5 (x^2) - 3 x + 1] | x \in {-2,0,2,4,8,16} } \\
%]

# The right-hand side of the overdetermined system.
%[
v := (\augment { [y]^\t | [x;y] \in Points3a })^\t \\
%]

# The matrix on the left-hand side of the overdetermined system.
%[
M := (\augment { [x^2, x, 1]^\t | [x;y] \in Points3a})^\t \\
%]

# The orthonormal matrix that can be used to compute projections
# onto the span of the columns of M.
A := \augment \orthonormal \decompose (\rref M^\t)^\t

# The projection of v onto the span of the columns of M.
w := A * A^\t * v


# Part (a):
# The solution to the system below contains the coefficients
# for the least squares approximation. The top entry corresponds
# to the x^2 coefficient.
FullMatrix := \rref \augment (M, w)

# We could isolate the actual coefficients in a 3-component
# vector, but this is not required.
%[
approx := (\augment (\decompose ((FullMatrix * [0;0;0;1])^\t) - {[0]}))^\t
%]

# Part (b):
# The error on the initial domain is defined as follows.
||v - w||

# Below is an alternative formula.
||v - M * approx||

# To compute the error of the approximation on the second
# domain, we must first generate the points and build the
# corresponding matrix:

%[
Points3b & := & { [x; 2 (x^3) + 5 (x^2) - 3 x + 1] | x \in {-16,-8,-4,-2,-1,0} } \\
%]
%[
M3b :=  (\augment { [x^2#, x#, 1]^\t | [x;y] \in Points3b})^\t \\
%]

# Then, M3b * approx gives us the outputs of the function
# on the new data points. We compute the error in the same way.
||v - M3b * approx||
/@

</solutionHomework>

  </li>
</ol>
<br/>

<hr/><br/>
<!--/assignment3-->

<a name="5.6.a"></a>
<h3>Homogenous, non-homogenous, overdetermined, and underdetermined systems</h3>

While we have usually considered %n \times %n matrices, in real-world applications (especially those involving data), system states usually have
a shorter description than the corpus of data (e.g., observational data) being used to determine the system state. Thus, problems in such applications
usually involve matrices in which the number of rows is very different from the number of columns. Furthermore, the data may be noisy,
which means that there may be no exact system state that matches the data. Informally, this might mean that the system of equations being
used to determine a system state is <i>overdetermined</i>. On the other hand, if the amount of data is not sufficient to determine a single system
state, a system is <i>underdetermined</i>.

<b>Definition:</b> For any matrix %M \in \R^{n \times m} and vector %v \in \R^m, the system %M %x = %v is <i>overdetermined</i> if there exist 
no solutions to the equation %M %x = %v.

<b>Definition:</b> For any matrix %M \in \R^{n \times m} and vector %v \in \R^m, the system %M %x = %v is <i>undetermined</i> if there exist 
two or more solutions for %x that satisfy the equation %M %x = %v.

<b>Definition:</b> For any matrix %M \in \R^{n \times m}, the system %M %x = \0 is <i>homogenous</i>.

<b>Definition:</b> For any matrix %M \in \R^{n \times m} and vector %v \in \R^m where %v \neq \0, the system %M %x = %v is <i>nonhomogenous</i>.

<b>Fact:</b> A homogenous system %M %x = \0 has at least one solution.

<b>Fact:</b> A homogenous system %M %x = \0 is never overdetermined.

<b>Fact:</b> The solution space of a homogenous system is a vector space:
<ul>
 <li>there is a unique additive identity: \0 is a solution</li>
 <li>adding two solutions yields a solution:
\begin{eqnarray}
 %M %x & = & \0 \\
 %M %y & = & \0 \\
 %M (%x + %y) & = & %M %x + %M %y \\
              & = & \0 
\end{eqnarray}
 </li>
 <li>multiplying a solution by a scalar yields a solution:
\begin{eqnarray}
 %M %x & = & \0 \\
 %M (%s %y) & = & %s (%M %y) \\
            & = & %s \0\\
            & = & \0 
\end{eqnarray}
 </li>
</ul>

<b>Fact:</b> The solution space of a nonhomogenous system is an affine space.

<b>Motivating Example:</b> Suppose we have a nonhomogenous system %M %x = %v where %v \in \R^{1000000000000}. 

We may not know ahead of time whether or
not it is overdetermined. Suppose we do not want to incur the cost of attempting to solve %M %x = %v exactly. This might be because we only want to make
one pass of the data to reduce costs, but it might also be because this is a data stream that is too voluminous to store, so the data <i>will be
gone</i> unless we solve the system right away (e.g., data from a telescope, a particle collider, or Twitter). 

In such cases, we can typically assume the data will have noise, so the system is overwhelmingly likely to be overdetermined and have no solution.
Thus, we are okay with finding an approximate solution.

We do know, however, that %M %x = \0 has at least one solution, and that the solution space is a vector space (so, for example, we
could try to "search" for a better solution using some other technique involving addition and scalar multiplication
once we find at least one non-trivial one). Thus, we might instead
choose to solve the system %M %x = \0, knowing that even in the worst case, a very poor approximate solution of \0 will always be found
in the worst case.

Is there a better compromise between these two strategies of keeping %v or changing it to \0? Can we find some other space of
solutions that won't be overdetermined, but will
still allow us to find a non-trivial (nonzero) approximate solution? One possibility is to <i>project</i> the vector %v onto a subspace
of the solution space, ensuring that the system is no longer overdetermined. This approach is covered in the section below.

<a name="5.6.b"></a>
<h3>Application: approximating overdetermined systems</h3>

<b>Fact:</b> We first review the triangle inequality in the case of a right triangle. Suppose we have a triangle with a height %a, a base length of %b, and
a hypotenuse of length %c. Then we have:

\begin{eqnarray}
 %a^2 + %b^2 & \geq & %a^2 \\
 %c^2 & \geq & %a^2 \\
 %c & \geq & %a
\end{eqnarray}

This implies that the orthogonal projection of a vector %v \in %V onto a subspace %W \subset %V is the closest point in %W to %v.

<b>Definition:</b> Given %v \in %V and a subspace %W \subset %V, the vector %w within the subspace %W that is <i>closest</i> to %v is
a vector %w^\ast \in %W such that ||%v - %w^\ast|| is minimal. In other words, for all %w' \in W,

  $$||%v - %w^\ast|| \leq ||%v - %w'||.$$

<b>Definition:</b> Given %v \in %V and a subspace %W \subset %V, the orthogonal projection of %v onto %W is the <i>closest</i> vector in %W to %v.

We can show the above is true by appealing to the above fact related to the triangle inequality.

<b>Fact:</b> Let %B = {%w_1,...,%w_n} be a basis for %W \subset %V. Recall that %M_B is the matrix whose columns are the vectors in %B. Given 
%v \in %V where %v \not\in %W, %M_B %x = %v has no solution. This is because %M_B %x \in \span %B; thus, if a solution existed, we would have
%v \in \span %B and %v \in %W, which contradicts our assumption.

<b>Fact:</b> Let %B = {%w_1,...,%w_n} be an orthonormal basis for %W \subset %V. Given %v \in %V, the <i>error</i> of an approximate solution 
%x to the equation %M_B %x = %v is defined to be

  $$||%v - %M_B %x||.$$

<b>Fact:</b> Let %B = {%w_1,...,%w_n} be a basis for %W \subset %V. Given %v \in %V, the approximate solution 
%x^\ast to the equation %M_B %x = %v with smallest error is defined to be the <i>least-squares</i> approximate solution to %M_B %x = %v. In other words,
the <i>least-squares</i> approximate solution %x^\ast to %M_B %x = %v is such that for any other %x',

  $$||%v - %M_B %x^\ast|| \leq ||%v - %M_B %x'||.$$

Why is it called a "least-squares" approximation?

<b>Fact:</b> Let %B = {%w_1,...,%w_n} be a basis for %W \subset %V. Given %v \in %V and its projection %w^\ast \in %W, the <i>least-squares</i> 
approximate solution to %M_B %x = %v is the solution %x^\ast to the system

  $$%M_B %x^\ast = %w^\ast.$$

That is, if we assume the above equality, then for all possible  and %x' and %w' = %M_B %x', we have that:

\begin{eqnarray}
 ||%v - %M_B %x^\ast|| & \leq & ||%v - %M_B %x'|| \\
 ||%v - %w^\ast|| & \leq & ||%v - %w'||
\end{eqnarray}

In the table below, we summarize the correspondences used in the above facts.

<table class="fig_table">
 <tr>
  <td><b>concept related to<br/>solving %M_B %x = %v</b></td>
  <td align="center"><b>notation</b></td>
  <td align="center"><b>relationship</b></td>
  <td align="center"><b>notation</b></td>
  <td><b>geometric concept</b></td>
 </tr>
 <tr>
  <td>the space of values %W with<br/>which we can replace %v in<br/>the overdetermined system %M_B %x = %v<br/>to make a system %M_B %x = %w<br/>that has solutions</td>
  <td>{%M_B %x | %x \in \R^n}</td>
  <td>the span of the<br/>columns of %M_B</td>
  <td>\span %B</td>
  <td>the subspace %W of %V<br/>spanned by %B</td>
 </tr>
 <tr>
  <td>the error of an approximate<br/>solution %x' to %M_B %x = %v</td>
  <td>||%v - %M_B %x' ||</td>
  <td>%M_B %x' = %w'</td>
  <td>||%v - %w'||</td>
  <td>the distance between<br/>%w' \in %W and %v \in %V</td>
 </tr>
 <tr>
  <td>%M_B %x^\ast where %x^\ast is<br/>the minimum error solution<br/>to %M_B %x = %v</td>
  <td>for all %x',<br/>||%v - %M_B %x^\ast || \leq ||%v - %M_B %x' ||</td>
  <td>%M_B %x^\ast = %w^\ast</td>
  <td>for all %w' \in %W,<br/>||%v - %w^\ast|| \leq ||%v - %w'||</td>
  <td>the orthogonal projection<br/>%w^\ast of  %v \in %V onto %W<br/>(the closest vector in %W<br/>to %v)</td>
 </tr>
</table>

Notice that we know a solution to %M_B %x = %w^\ast exists, and that we can show this in two different ways. Since %w^\ast \in %W, it
must be that %w is a linear combination of the vectors in %B, so it is a linear combination of the columns in %M_B. Alternatively, if %M_B is
a square matrix and we know that %B is a basis, then the columns of %M_B are linearly independent, which means %M_B is invertible, so

  $$%x^\ast = %M_B^{-1} %w^\ast.$$

<b>Fact:</b> We can compute the least-squares solution to any equation %M %x = %v by using the following process:
<ol>
  <li>find an orthonormal basis %B of the span of the column vectors of %M to make %M_B</li>
  <li>use %M_B to compute the projection %w^\ast of %v onto \span %B</li>
  <li>solve the system %M %x = %w^\ast by finding the \rref of an augmented matrix</li>
</ol>

Later in the course we will consider a slightly different way of solving this problem that uses algebraic properties of linear transformations.

<b>Exercise:</b> Suppose a matrix %M is such that %M %x = \0 has exactly one solution. What can we say about the error of the least-squares 
approximation of any system %M %x = %v?

<b>Exercise:</b> Suppose a matrix %M is such that %M %x = \0 has \R^n as its space of solutions. What can we say about any system %M %x = %v?
What can we say about the error of any least-squares approximation of a solution for %M %x = %v?

<!-- Notice that in the above discussion, %W is a vector space, while {%v} is an affine space. 
More generally, the set of solutions to %M%x = %v is always an affine space. It is always possible to find a least-squares approximation within
the vector space {%x | %M%x = 0}. -->

<b>Exercise:</b> We know that the solution space for an undetermined system %M%x = 0 is a vector space.
Thus, this space must have a basis. How can we use a process similar to the Gram-Schmidt algorithm to find a basis of this space?

<a name="5.6.b.i"></a>
<h4>Relationship to curve fitting and linear regressions in statistics</h4>

The method described above does overlap with standard curve fitting and linear regression techniques you see
in statistics. There are many variants to these approaches, and the one considered above corresponds to a fairly
basic variant that has no specialized characteristics (i.e., it makes no special assumptions about the data points, 
the relative importance of the data points, their distribution, or the way they can be interpreted).
<ul>
 <li>
  The approach above is known as <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">
  ordinary least squares</a> and as <a href="http://en.wikipedia.org/wiki/Linear_least_squares">
  linear least squares</a>.
 </li>
 <li> 
  The case in which the function space is {%f | %f(%x) = %a%x + %b, %a,%b \in \R}
  corresponds to a <a href="http://en.wikipedia.org/wiki/Simple_linear_regression">simple linear regression</a>.
  This involves fitting a line to a collection of points while minimizing the sum of the squares of the
  distances <i>parallel to the %y-axis</i> between all the data points and the approximate line.
  <a href="http://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Motivational_example">This diagram</a>
  is an illustration of this. The method in these notes is more general in that it is not restricted to
  polynomials of order 1, but can be used to fit polynomials of any order to data.
 </li>
 <li>
  We can restate the least squares method described in these notes as finding %x^\ast such that
  the vector &epsilon; in %M %x^\ast = %v - &epsilon; is minimized. Notice that &epsilon; + %w = %v.
  The length of the vector &epsilon; can be viewed as the sum of squares of %y-axis-aligned distances from the
  estimate curve to the actual data points:

  $$ ||&epsilon;|| = ||%v - %M %x^\ast||. $$

 </li>
 <li>
  <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares#Geometric_approach">This diagram</a> illustrates the
  projection method being used.
 </li>
 <li>
 The matrix %M in %M %x = %v is sometimes called the
 <a href="http://en.wikipedia.org/wiki/Design_matrix">design matrix</a>.
 The same term is used to refer to the 
 <a href="http://en.wikipedia.org/wiki/Design_matrix#Simple_Regression">corresponding
 matrix in a simple linear regression.</a>
 </li>
</ul>

<a name="5.6.c"></a>
<h3>Application: approximating a model of system state dimension relationships</h3>

The approach described in the previous section can be used to approximate a solution to the equation %M %x = %v. Usually, if %M describes
a model of a system (in this sense, <i>system</i> refers to a system of states that are described by values along some number of dimensions, as
in an <a href="#3.2.a">earlier section</a>), solving %M %x = %v corresponds to determining a system state %x given some other information
about a system state %v.

What if we instead have a large collection of pairs of observations of system states of the form (%x,%v) (or, more generally, any collection of
observations of a system along multiple dimensions). Can we approximate a model of a set of relationships between some two subsets of the
dimensions in the system? In other words, if we take all our observations (x_1, ..., x_n) and divide them into pairs of descriptions
%x = (x_1,...,x_k) and %v = (%x_{k+1},...,%x_n), can we find the best %M that approximates the relationship between incomplete
system state descriptions %x and %v?

<b>Example:</b> Suppose we have the following dimensions in our system:
<ul>
 <li>number of red dwarf stars</li>
 <li>number of G type stars</li>
 <li>number of pulsars</li>
 <li>units of infrared (IR) radiation observed</li>
 <li>units of ultraviolet (UV) radiation observed</li>
 <li>units of gamma (&gamma;) radiation observed</li>
</ul>

Suppose that we already have some set of confirmed (but possibly noisy) observations of systems along <i>all</i> the dimensions (the dimensions
of the quantities in each point correspond to the list of dimensions above):

@
%[
O := {
  [ 4;  5;  1; 243; 3341;  700], 
  [ 3;  6; 21; 125; 1431; 1465], 
  [ 4;  2; 13; 533; 3432;  334], 
  [16;  4;  4; 334;  143;  762], 
  [13;  8; 13; 235; 1534;  513], 
  [34; 16; 17; 333; 3532;  450]
 } \\ 
%]
/@

For the above observations, we want to find a matrix that relates the first three dimensions (the number of each type of star) to the last
three dimensions (the amount of each kind of radiation). In other words, we are looking for a matrix %M \in \R^{3 \times 3} of the
form:

  $$#[ a <i style="color:gray;">units IR/red dwarf</i> #, b <i style="color:gray;">units IR/G type</i> #, c <i style="color:gray;">units IR/pulsar</i> #; 
       d <i style="color:gray;">units UV/red dwarf</i> #, e <i style="color:gray;">units UV/G type</i> #, f <i style="color:gray;">units UV/pulsar</i> #; 
       g <i style="color:gray;">units &gamma;/red dwarf</i> #, h <i style="color:gray;">units &gamma;/G type</i> #, i <i style="color:gray;">units &gamma;/pulsar</i> #] $$

Multiplication by the above matrix represents a system state description transformation with the following units:

  $$ (<i style="color:gray;"># red dwarf stars</i>, <i style="color:gray;"># G type stars</i>, <i style="color:gray;"># pulsars</i>) 
     \to (<i style="color:gray;">units IR</i>, <i style="color:gray;">units UV</i>, <i style="color:gray;">units &gamma;</i>) $$

We can split the data into two collections of points: those that specify the number of each type of star (the inputs to the above transformation),
and those that specify the amount of each kind of radiation (the output of the above transformation). Below, we call these %P and %Q. We then
turn these two sets of data points into matrices. In order to turn this problem into the familiar form %M %x = %v, we transpose the two
matrices.

@
%[
O := {
  [ 4;  5;  1; 243; 3341;  700], 
  [ 3;  6; 21; 125; 1431; 1465], 
  [ 4;  2; 13; 533; 3432;  334], 
  [16;  4;  4; 334;  143;  762], 
  [13;  8; 13; 235; 1534;  513], 
  [34; 16; 17; 333; 3532;  450]
 } \\ 
%]

%[
P & := & {[a;b;c] | [a;b;c;x;y;z] \in O} \\
Q & := & {[x;y;z] | [a;b;c;x;y;z] \in O} \\
%]

M := (\augment P)^\t
v := (\augment Q)^\t    # Actually a matrix.

A := \augment \orthonormal \decompose (\rref M^\t)^\t

w := A * (A^\t) * v

\rref \augment (M, w)
/@

<a name="5.7"></a>
<h3>The dimension of a vector space</h3>

<b>Definition:</b> The <i>dimension</i> of a vector space %V, which we write as \dim %V, is the size of the basis for %V.

<b>Example:</b> What is the dimension of {%f | %f(%x) = %a%x^3 + %b%x^2 + %c%x + %d, %a,%b,%c,%d \in \R}?

<b>Example:</b> Consider the vector space consisting of finite sets of real numbers:
<ul>
 <li>there is a unique additive identity:  {}</li>
 <li>addition (+) is defined as:

  $$%P + %T = %Q \~ where \~ %Q = %P \cup %T$$

 </li>
 <li>scalar multiplication is defined as:

  $$%s \cdot %T = %Q \~ where \~ %Q = {%s \cdot %r | %r \in %T}$$

 </li>
</ul>
What are the basis and dimension of this vector space?

<a name="lecture14"></a>
<a name="5.8"></a>
<h3>Orthogonal complements of vector spaces</h3>

<b>Definition:</b> The orthogonal complement %W^\bot of a vector subspace %W \subset %V is the set of all vectors in %V that are orthogonal
to all the vectors in %W:

  $$%W^\bot = {%v | %v \in %V, \forall %w \in %W, %v \cdot %w = 0}$$

Notice that the orthogonal complement of a subspace is always taken within the context of some specified space %V.

<a name="5.9"></a>
<h3>Algebra of vector spaces</h3>

In this section, we consider vector spaces, their orthogonal complements, and common set operations: set union, set intersection, and set product.

<b>Exercise:</b> Given any two vectors spaces %V and %W, is %V \cup %W a vector space? <b>No.</b> For example, consider

  $$%V = \span{[1;0]} \cup \span{[0;1]}.$$

We have that [1;0] \in %V and [0;1] \in %V, but not [1;0] + [0;1] = [1;1] \in %V.

<b>Exercise:</b> Given any two vectors spaces %V and %W, is %V \cap %W a vector space? <b>Yes.</b>
<ul>
 <li>there is a unique additive identity:  \0 \in %V and \0 \in %W, so \0 \in %V \cap %W</li>
 <li>%V \cap %W is closed under addition (+): if %v,%w \in %V \cap %W then we have that %v \in %V and %w \in %V, and we also have that
     %v \in %W and %w \in %W, so we have that

  $$%v + %w \in %V \~ and \~ %v + %w \in %W, \~ so \~ %v + %w \in %V \cap %W.$$
  
  Since addition satisfies the vector space axioms for elements %V and %W, it also does so for elements in %V \cap %W.
 </li>
 <li>%V \cap %W is closed under scalar multiplication (\cdot): if %v \in %V \cap %W then we have that %v \in %V and %w \in %V, so we have that

  $$%s%v \in %V \~ and \~ %s%v \in %W, \~ so \~ %s%v \in %V \cap %W.$$

  Since scalar multiplication satisfies the vector space axioms for elements %V and %W, it also does so for elements in %V \cap %W.
 </li>
</ul>

<b>Exercise:</b> Given any two vectors spaces %V and %W, is %V \times %W a vector space? <b>Yes.</b>
<ul>
 <li>there is a unique additive identity:  (\0_V,\0_W) where \0_V \in %V and \0_W \in %W</li>
 <li>there is an addition operation (+):

  $$ (%v,%w) + (%v',%w') = (%v+%v', %w+%w') $$

 </li>
 <li>there is a scalar multiplication (\cdot) operation:

  $$ %s (%v,%w) = (%s%v, %s%w) $$

 </li>
</ul>

<b>Exercise:</b> For a vector space %W \subset %V, compute %W \cap %W^\bot.

<b>Fact:</b> For a vector subspace %W \subset %V,

  $$\dim %W + \dim %W^\bot = \dim %V.$$

<b>Fact:</b> For a vector subspace %W \subset %V,

  $$%W \times %W^\bot = %V.$$

<a name="6"></a>
<hr style="margin-bottom:120px;"/>
<h2>Linear Transformations</h2>

<a name="lecture15"></a>
<a name="6.1"></a>
<h3>Maps (a.k.a., functions) and their properties</h3>

In addition to the <a href="#A.4">definitions involving relations and maps we introduce in the Appendix</a>, we consider several more definitions.

First, we introduce a notation for specifying the set of inputs a function accepts, and the set of outputs it may produce. We use the notation

  $$ %f: %A \to %B $$

to indicate that a function takes elements in the set %A and maps them to elements in the set %B. This notation is equivalent to

  $$ %f \in %A \to %B $$

where %A \to %B is the <i>set</i> of functions that map elements from %A to elements in %B:

  $$%A \to %B \~ = \~ { %f | \forall %a \in %A, %f(%a) \in %B }.$$

<b>Definition:</b> Given a map %f: %A \to %B, we call %A the <i>domain</i> of %f.

<b>Definition:</b> Given a map %f: %A \to %B, we call %B the <i>codomain</i> of %f. We can also call it the <i>range</i> of %f, but this can be
ambiguous (because sometimes it is used to refer to the <i>image</i>, defined below) so we avoid using it.

We also introduce some properties of a map that can be used to describe more precisely describe its set of outputs and the relationship
between those outputs and the inputs the can generate them.

<b>Definition:</b> Given a map %f: %A \to %B, we call {%f(%a) | %a \in %A} the <i>image</i> of %f.

<b>Definition:</b> Given a map %f: %A \to %B, for any subset %A' \subset %A, we call {%f(%a) | %a \in %A'} the image of %A' under %f. We sometimes write
this as %f(%A).

<b>Definition:</b> Given a map %f: %A \to %B, for any %a \in %A, we call %f(%a) the image of %a under %f.

<b>Definition:</b> Given a map %f: %A \to %B, for any subset %B' \subset %B, we call {%a | %a \in %A, %f(%a) \in %B} the <i>pre-image</i> of %B under %f. We sometimes write this as
%f^{-1}(%B).

<b>Definition:</b> Given a map %f: %A \to %B, for any %b \in %B, we call {%a | %a \in %A, %f(%a) = %b} the <i>pre-image</i> of %b under %f. 
We sometimes write this as %f^{-1}(%b).

Finally, we consider a few properties of maps that deal with how the map relates elements in its domain to elements in its codomain.

<b>Definition:</b> A map %f: %A \to %B is <i>surjective</i> (or <i>onto</i>) if for all %b \in %B, there exists %a \in %A such that %f(%a) = %b.

<b>Definition:</b> A map %f: %A \to %B is <i>injective</i> (or <i>one-to-one</i>) if for all %a,%a' \in %A, %f(%a) = %f(%a') implies %a = %a'.

<b>Definition:</b> A map %f: %A \to %B is <i>bijective</i> if it is surjective and injective.

Visual illustrations of examples of the above definitions can be found in the <a href="#A.4">Appendix</a>.

<b>Fact:</b> A map %f: %A \to %B is <i>injective</i> iff for all %a,%a' \in %A, %a \neq %a' implies %f(%a) \neq %f(%a'). We can prove this
by appealing to the logical fact that given two propositions %P and %Q, not %P implies not %Q iff %Q implies %P.

<b>Fact:</b> If %B is the codomain of %f, %B' is the image of %f, and %f is surjective, then %B = %B'.

<b>Fact:</b> If %f: %A \to %B is bijective, then %f^{-1} is bijective.

<b>Fact:</b> If %f: %A \to %B is not injective, then %f^{-1} is not a function. If %f is not injective, then there
exist %a,%a' \in %A such that %a \neq %a' but %f(%a) = %f(%a'). In this case, we cannot define a single
value to which %f^{-1}(%f(%a)) can evaluate, so %f^{-1} cannot be a function.

<b>Exercise:</b> Describe with as much detail as you can the domains, codomains, and images of the following maps (there is more than one right
answer for these, as the definitions do not specify the domains of the functions). Determine whether each map is injective, surjective, or both
given the domains and codomains you chose.

\begin{eqnarray}
 %f(%x) & = & 3%x & \~ & \\
 %f(%x) & = &  %x^2 & \~ & \\
 %f(%x) & = &  -%x & \~ & \\
 %f(%x) & = &  %x \cdot \sqrt(-1) & \~ & \\
 %f(%x) & = &  #[%x #, 0 #; 0 #, %x#] & \~ & \\
 %f( %M, %N ) & = &  %M \cdot %N & \~ & where %M and %N are matrices
\end{eqnarray}
\begin{eqnarray}
 %f( #[%x #; %y #] ) & = &  #[1 #, 2 #; 3 #, 4#] #[%x #; %y #]
\end{eqnarray}

<b>Fact:</b> Given some %f:%A \to %B, the following process can be used to show a map %f is injective:
<ol>
 <li>assume %a,%a' \in %A and %a \neq %a';</li>
 <li>it is necessary to show that %f(%a) \neq %f(%a'):
  <ol type="a">
   <li>expand %f(%a) and %f(%a') using the definition of %f,</li>
   <li>use algebraic properties or other facts to derive %f(%a) \neq %f(%a').</li>
  </ol>
 </li>
</ol>
Alternatively, we can use the following approach:
<ol>
 <li>assume %a,%a' \in %A;</li>
 <li>assume %f(%a) = %f(%a'):
  <ol type="a">
   <li>expand %f(%a) and %f(%a') using the definition of %f,</li>
   <li>use algebraic properties or other facts to derive %a = %a'.</li>
  </ol>
 </li>
</ol>

<b>Fact:</b> Given some %f:%A \to %B, we can show a map %f is not injective by
picking some explicit %a,%a' \in %A such that %a \neq %a' and showing that %f(%a) = %f(%a'). Alternatively, pick
some explicit %b \in %B and find two or more distinct solutions %a and %a' \in %A to the equation %f(%x) = %b. 

<b>Fact:</b> Given some %f:%A \to %B, the following process can be used to determine whether a map %f is surjective:
<ol>
 <li>let %b \in %B;</li>
 <li>solve %f(%x) = %b:</li>
  <ul>
   <li>if you can solve the equation (or derive an equation %x = %g(%b)) such that %x \in %A, %f:%A \to %B is surjective,</li>
   <li>if the equation has no solutions in %A, %f:%A \to %B is not surjective.</li>
  </ul>
 </li>
</ol>
Notice that this does not mean that there is not some %A' where %A \subset %A' that does have a solution
to the equation %f(%x) = %b. For example, %f(%x) = 3%x is surjective if %f \in \R \to \R but not surjective
if %f \in \Z \to \Z.


<b>Exercise:</b> Show that %f: \R \to \R where %f(%x) = %x + 1 is injective.

<b>Exercise:</b> Show that %f: \R \to \R where %f(%x) = %x + 1 is surjective.

<b>Definition:</b> Given two maps %f: %B \to %C and %g: %A \to %B, we define the <i>composition</i> of %f and %g, which we denote %f \circ %g,
as the map %h:%A \to %C defined by:

  $$ %h(%x) = %f(%g(%x)) $$

<b>Exercise:</b> Show that if %f: %B \to %C and %g: %A \to %B are injective, then their composition (%f \circ %g):%A \to %C is injective.

<a name="6.2"></a>
<h3>Homomorphisms and isomorphisms</h3>

The previous section defined a number of properties of functions %f: %A \to %B that only address the equality of elements in %A and %B. However,
what if %A and %B have relations and operators? For example, what if %A and %B are vector spaces with an identity, an addition operation, and a
scalar multiplication operation?

We can restrict our notion of a map or function to maps or functions %f: %A \to %B that somehow preserve or consistently transform the properties 
of operators and relations we may have already defined on %A and %B. For example, suppose we have %A = {%a_1, %a_2, %a_3}, %B = {%b_1, %b_2, %b_3},
an operator \oplus such that %a_1 \oplus %a_2 = %a_3, and an operator \oplus over elements of %B such that %b_3 \oplus %b_2 = %b_1. A map %f from
%A to %B that preserves (or respects, or consistently transforms) \oplus would be such that

\begin{eqnarray}
 %f(%a_3) = %b_1 \\
 %f(%a_2) = %b_2 \\
 %f(%a_1) = %b_3
\end{eqnarray}

Notice that the above map is such that the operation \oplus is respected by the map %f:

\begin{eqnarray}
 %f(%a_3) \~ = \~ %f(%a_1 \oplus %a_2) \~ = \~ %f(%a_1) \oplus %f(%a_2) \~ = \~ %b_3 \oplus %b_2 \~ = \~ %b_1 \\
\end{eqnarray}


<b>Definition:</b> Given a binary operator \otimes over %A and another operator \oplus over %B, we say that a map %f: %A \to %B is a <i>homomorphism</i>
if we have that

  $$\forall %a,%a' \in A, \~ %f(%a \otimes %a') = %f(%a) \oplus %f(%a').$$

Notice that a homomorphism might have, but does not necesserily have, any of the properties we introduced for maps: it could be injective, surjective,
and so on.

<b>Definition:</b> A bijective homomorphism %f: %A \to %B is called an <i>isomorphism</i>.

<a name="6.3"></a>
<h3>Linear transformations</h3>

In this course we are interested in a specific kind of homomorphism.

<b>Definition:</b> For any two vector spaces %V and %W, a map %f: %V \to %W is a <i>linear transformation</i> iff we have that for all %v,%v' \in V and
scalars %s \in \R,

\begin{eqnarray}
 %f(%v + %v') & = & %f(%v) + %f(%v') \\
 %f(%s \cdot %v) & = & %s \cdot %f(%v)
\end{eqnarray}

In other words, a linear transformation is a homomorphism between vector spaces that preserves vector addition and scalar multiplication.
If a map %f does not satisfy the above properties, it is <i>not</i> a linear transformation.

<b>Fact:</b> For any linear transformation %f:%A \to %B and appropriate constants \0 \in %A and \0' \in %B,

  $$ %f(\0) = \0' $$

We can show this in the following way: given any %v \in %V,

\begin{eqnarray}
  \0 & = & 0 \cdot %v \\
 %f(\0) & = & %f(0 \cdot %v) \\
        & = & 0 \cdot %f(%v) \\
        & = & \0'
\end{eqnarray}

<b>Fact:</b> For any linear transformation %f:%A \to %B and the corresponding additive inversion operations of
%A and %B, it is the case that %f respects additive inversion:

  $$ %f(-%v) = -%f(%v) $$

We can derive this fact in two ways. One approach is to use the previous fact that %f(\0) = \0' to show that %f(-%v) is indeed the inverse of %f(%v):

\begin{eqnarray}
  %f(%v) + %f(-%v) & = & %f(%v + (-%v)) \\
                   & = & %f(\0) \\
                   & = & \0'
\end{eqnarray}

The other approach is to use the fact that in both vector spaces,
%v = -1 \cdot %v:

\begin{eqnarray}
  %v & = & 1 \cdot %v \\
 %v + (-1 \cdot %v) & = & (1 \cdot %v) + ((-1) \cdot %v) \\
            & = & (1 + (-1)) \cdot %v \\
            & = & 0 \cdot %v \\
            & = & \0
\end{eqnarray}

Then, because %f respects scalar multiplication, we have that: 

\begin{eqnarray}
  %f(-1 \cdot %v) & = & -1 \cdot %f(%v)
\end{eqnarray}

Thus, the argument is:

\begin{eqnarray}
 %f(%v) + %f(-%v) & = & %f(%v) + %f(-1 \cdot %v) \\
                  & = & %f(%v) + ((-1) \cdot %f(%v)) \\
                  & = & %f(%v) + (-%f(%v)) \\
                  & = & \0
\end{eqnarray}

<b>Fact:</b> For any two linear transformations %f: %V \to %W, %g: %U \to %V, the composition %f \circ %g: %U \to %W is also a linear transformation.
To show this, we must demonstrate that if %h(%x) = %f(%g(%x)) then %h respects vector addition and scalar multiplication. For any %u,%u' \in %U, and
%s \in \R, we have:

\begin{eqnarray}
 %h(%v + %v') & = & %f(%g(%v + %v')) \\
              & = & %f(%g(%v) + %g(%v')) \\
              & = & %f(%g(%v)) + %f(%g(%v')) \\
              & = & %h(%v) + %h(%v')
\end{eqnarray}

and

\begin{eqnarray}
 %h(%s \cdot %v) & = &  %f(%g(%s \cdot %v)) \\
              & = &  %f(%s \cdot %g(%v)) \\
              & = & %s \cdot  %f(%g(%v)) \\
              & = & %s \cdot %h(%v)
\end{eqnarray}


Because linear transformations are homomorphisms (and, thus, maps), we can ask whether they have properties of maps (i.e., whether they are injective,
surjective, bijective, and so on). We will do so further below.

<a name="lecture16"></a>
<a name="6.4"></a>
<h3>Data modelling as a linear transformation</h3>

Recall that for a system that can be described using quantities along multiple dimensions, subsets of a vector space \R^n can be used to
model that system's possible states (or observations of that system's states). Recall also that a matrix can be used to model the relationships
between the dimensions describing that system, and that finding an approximate model of such relationships involves finding a matrix in a vector
space of matrices.

In a similar manner, linear transformations (and, more generally, homomorphisms) can be used to represent the relationship
between actual data from (or observations of) a system, and possible mathematical models of that system. The problem of finding a
mathematical model of data in a domain %D can be viewed as the problem of finding an appropriate mathematical structure %M and
an appropriate homomorphism (or isomorphism) %f:%D \to %M from the data domain %D to that mathematical model.
If we assume that the data domain can be modelled using a vector space, then %M can be a vector space and %f can be a linear transformation.

<b>Example:</b> As an illustration, we consider a very simple (and fairly contrived) example from chemistry. Suppose we have three substances:
iron (I), sulfur (S), and troilite (T). Through experiments, we find that mixing 8 units of iron and 1 unit of sulfur (together with some energy)
produces 8 units of troilite. We write this down as:

  $$ 8 I \oplus 1 S = 8 T.$$

We could take our data and define a set with a an operation \oplus that corresponds to whatever we do to produce the chemical reaction:

\begin{eqnarray}
 %S & = & {I, S, T, 2 I, 2 S, 2 T, 3 I, 3 S, 3 T, ...} \\
 8 I \oplus 1 S  & = & 8 T \\
 16 I \oplus 2 S  & = & 16 T \\
  & \vdots &
\end{eqnarray}

We do not know the internal structure of individual units of iron, sulfur, and troilite. Even if individual units of each can be
represented using multiple dimensions, we do not know how many dimensions there might be and what those dimensions might represent. 

Nevertheless, we can make hypotheses about what structure might be appropriate to model this system. We can then test these hypotheses
by using them to make new predictions and collecting data to verify or negate those predictions.

We hypothesize that there exists a non-trivial vector space %V with a vector addition operation + such that we can construct a homomorphism from our
data to %V. We assume it is non-trivial (nonzero) because a trivial space like this always exists: let I, S, and T all map to \0 \in %V. We are looking
for a vector space %V and homomorphism %f such that

  $$ %f(8 I \oplus 1 S) \~ = \~ %f(8 I) + %f(1 S) \~ = \~ 8 %f(I) + 1 %f(S) \~ = \~ 8 %f(T) $$

Thus, we are looking %V with non-trivial (nonzero) %u, %v, %w \in %V such that:

  $$ 8 %u + 1 %v = 8 %w $$

How many dimensions should %V have in order for our model to be interesting? What is the minimum number of dimensions %V must have so that
%u, %v, and %w are nonzero? What number of dimensions would make it trivially easy to find %u, %v, and %w that fit the above equation?

We choose \dim %V = 2 and %V = \R^2. We can also make some additional assumptions to simplify our problem: let %u and %v be orthogonal and
axis-aligned. Then we have the following equation:

\begin{eqnarray}
 8 #[ %x #; 0 #] + 1 #[ 0 #; %y #] & = & 8 #[ %a #; %b #]
\end{eqnarray}

Thus, we have that:

\begin{eqnarray}
 %x & = & %a \\
 %y & = & 8 %b
\end{eqnarray}

Assume that %a = 1 and %b = 1, we get %x = 1 and %y = 8. Thus, we have found %u,%v, and %w \in \R^2 such that %f respects \oplus by mapping it to +:

\begin{eqnarray}
 %f(I) & = & #[ 1 #; 0 #] \\
 %f(S) & = & #[ 0 #; 8 #] \\
 %f(T) & = & #[ 1 #; 1 #]
\end{eqnarray}

Notice that we made several assumptions, but this is okay. We can test the quality and fitness of these assumptions later by making predictions
and collecting more data. This corresponds to the usual scientific method:
hypothesize what kind of mathematical model fits the data, add assumptions until the model is instantiated to match or approximate the data
(the model should not be so simple that it can fit any data, or so complex that it only fits the already collected data), make a prediction
using the model about something which has not yet been observed, and try to confirm or falsify that prediction by collecting more data.

How do we interpret the two dimensions of %V? We do not necessarily know when we first create the model to fit the data. They may correspond
to actual entities, or they may correspond to abstract characteristics that can be broken down further once more data is collected.
In this case, they happen to correspond to the number of iron and sulfur atoms in the molecules of iron, sulfur, and troilite.

What is a prediction we can make using our model? Let us consider a scenario in which we introduce a different mix of iron and sulfur into our
reaction; how much troilite do we expect to make? We can write down the equation:

  $$ 8 I + 2 S = %c T $$
  
Applying our homomorphism, we get:

\begin{eqnarray}
 8 #[ 1 #; 0 #] + 2 #[ 0 #; 8 #] & = & %c #[ 1 #; 1 #]
\end{eqnarray}

This implies:

\begin{eqnarray}
 8 + 0 & = & %c \\
 0 + 2 \cdot 8 & = & %c
\end{eqnarray}

Since there is no solution, our model predicts that it is not possible to convert all 8 units of iron and all 2 units of sulfur into troilite.
This negative prediction is already testable, but we can do better by looking for a more positive prediction. Our mathematical model %V is a
vector space, and this suggests that the right-hand side of the equation must represent a linear combination of the three vector %f(I), %f(S),
and %f(T). This suggests we could write down a slightly different equation:

\begin{eqnarray}
 8 #[ 1 #; 0 #] + 2 #[ 0 #; 8 #] & = & %c #[ 1 #; 1 #] + %d #[ 1 #; 0 #] + %e #[ 0 #; 8 #]
\end{eqnarray}

This implies:

\begin{eqnarray}
 8 + 0 & = & %c + %d \\
 0 + 2 \cdot 8 & = & %c + 8 %e
\end{eqnarray}

Notice that there exist multiple solutions to these equations. Which correspond to reasonable interpretations within the physical system we are
modelling? All three variables should be positive; we might also require that they should be integers, and that %c is nonzero. We have that 

  $$ %e = (16-%c)/8,$$

so %c is either 16 or 8. It cannot be 16 because %d > 0, so %c = 8. Thus, %e = 1 and %d = 0. This predicts that 8 units of iron and 2 
units of sulfur should produce 8 units of troilite and 1 unit of sulfur. This is a testable prediction that we can use to direct 
further experiments to evaluate the quality of the model.

We will see similar examples in later sections.

<a name="6.5"></a>
<h3>Kernels of linear transformations</h3>

<b>Definition:</b> For a linear transformation %f:%A \to %B, the <i>kernel</i> of %f of the set of elements in the domain of %f that map to \0 \in %B:

  $$ \ker %f \~ = \~ {%a | %a \in %A, %f(%a) = \0 }. $$

<b>Fact:</b> For a linear transformation %f:%A \to %B, is the case that:
<ul>
 <li> \ker %f \subset %A </li>
 <li> the image of \ker %f under %f is {\0}</li>
 <li> \ker %f is the pre-image of \0 under %f</li>
</ul>

<b>Definition:</b> For a linear transformation %f:%A \to %B, \ker %f is a vector space (and, thus, a subspace of %A). How do we show this formally?

<b>Exercise:</b> Suppose we are given the linear transformation %f:\R^4 \to \R^3 defined as:

\begin{eqnarray}
 %f( #[ %x #; %y #; %z #; %t #] ) & = & #[ %x + %y #; %z + %t #; 0 #].
\end{eqnarray}

What is the codomain of %f? What is the image of %f? What is the kernel of %f? Provide a set comprehension to define each.

<b>Exercise:</b> Suppose we are given the linear transformation %f:\R^4 \to \R^4 defined as:

\begin{eqnarray}
 %f( #[ %x #; %y #; %z #; %t #] ) & = & #[ %x #; %x + %y #; %x + %y + %z #; %x + %y + %z + %t #].
\end{eqnarray}

Show that %f is surjective.

<a name="6.5"></a>
<h3>Matrices as symbolic representations of linear transformations</h3>

Matrices are the conventional concrete representation we adopt for linear transformations between vector spaces that can be represented 
symbolically using the usual vector and span notation (i.e., subspaces of \R^n). Thus, we can interpret any matrix %M \in \R^n \to \R^m as
the linear transformation %f: \R^m \to \R^n with the definition %f(%v) = %M \cdot %v.

<b>Fact:</b> For any matrix %M \in \R^{n \times m}, the map %f: \R^m \to \R^n defined as %f(%v) = %M \cdot %v is a linear transformation.

<b>Fact:</b> The image of the linear transformation %f(%v) = %M \cdot %v represented by %M \in \R^{n \times m} is the span of the column vectors of %M.
Let %w_1,...,%w_n be the columns of %M. We can see this by noting that

\begin{eqnarray}
 %f( #[ %a_1 #; \vdots #; %a_n #] ) & = & %M \cdot #[ %a_1 #; \vdots #; %a_n #] & = & %a_1 %w_1 + ... + %a_n %w_n.
\end{eqnarray}

<b>Fact:</b> The image of the linear transformation represented by %M \in \R^{n \times m} is a vector space (and a subspace of \R^m). Why is this?

<b>Fact:</b> If a matrix %M \in \R^{n \times n} is invertible and %f(%v) = %M \cdot %v, then %f is injective.

<b>Fact:</b> If a matrix %M \in \R^{n \times n} is invertible and %f(%v) = %M \cdot %v, then \ker %f = {\0}.

<b>Fact:</b> For a matrix %M \in \R^{n \times n} and %f(%v) = %M \cdot %v where \ker %f = {\0}, then %M is invertible.

<b>Fact:</b> If a matrix %M \in \R^{n \times n} represents a linear transformation %f(%v) = %M \cdot %v and \ker %f = {\0},
then the system %M %v = 0 has exactly one solution.

<a name="7"></a>
<a name="lecture17"></a>
<hr style="margin-bottom:120px;"/>
<h2>Review #2</h2>

This section contains a comprehensive collection of review problems going over all the course material covered until this point. These problems
are an accurate representation of the kinds of problems you may see on an exam.

<b>Problem:</b> Given %u \in \R^n, what is the orthogonal projection of %v \in \R^n onto \span{%u}?

<solution>
It is sufficient to compute the orthogonal projection of %v onto %u. Given a unit vector %e parallel to %u, the projection of %v onto %e would
be:

  $$ (%e \cdot %v) \cdot %e.$$

However, we cannot assume %u is a unit vector. Thus, we scale %u to obtain a unit vector %e = %u/||%u||. Then, the solution is:

  $$((%u/||%u||) \cdot %v) \cdot (%u/||%u||).$$
</solution>

<b>Problem:</b> Assume a matrix %M \in \R^{2 \times 2} is symmetric, has constant diagonal, and all its entries are nonzero. Show that %A cannot
be an orthogonal matrix.

<solution>
Let %a \neq 0 and %b \neq 0, and let us define:

\begin{eqnarray}
%M & = & #[ %a #, %b #; %b #, %a #].
\end{eqnarray}

Suppose that %M is an orthogonal matrix; then, we have that:

\begin{eqnarray}
             %M^{-1} & = & %M^\top\\
%M \cdot %M^\top & = & \I \\
#[ %a #, %b #; %b #, %a #] \cdot #[ %a #, %b #; %b #, %a #] & = & #[ 1 #, 0 #; 0 #, 1 #].
\end{eqnarray}

Thus, we have %b%a + %a%b = 0, so 2%a%b = 0. This implies that either %a = 0 or %b = 0. But this contradicts the assumptions, so %M cannot be
orthogonal.
</solution>

<b>Problem:</b> Let %M \in \R^{17 \times 9} and %f(%v) = %M \cdot %v. What is the largest possible value of \dim(\im(%f))?

<solution>
We know that we can only compute %M %v if %v has as many rows as %M has columns. Thus, if %f(%v) = %M \cdot %v, then %v \in \R^9. We
also know that %M \cdot %v will be a vector with 17 rows because %M has 17 rows, so %f(%v) \in \R^{17}. Thus, %f \in \R^9 \to \R^{17}.

This means that \dim(\im(%f)) cannot be greater than \dim(\R^{17}), so it cannot exceed 17. However, we also need to note that %M
has 9 columns, and that any value in the image of %f is thus a linear combination of at most 9 vectors. Thus, any basis of \im(%f) has
at most 9 distinct vectors in it. Since \dim(\im(%f)) is the size of the basis of \im(%f), \dim(\im(%f)) can be at most 9.
</solution>

<b>Problem:</b> Find an orthonormal basis for the following vector space:

\begin{eqnarray}
 \span { #[ 1 #; 0 #; 2 #; 0 #] , #[ 0 #; 1 #; 2 #; 3 #] , #[ 0 #; 0 #; 0 #; 1 #] }.
\end{eqnarray}

<solution>
We can use the <a href="#algorithmgramschmidt">algorithm</a> for computing the vectors in an orthonormal basis. We can work with the vectors in any order, so suppose we have:

\begin{eqnarray}
 %v_1 = #[ 0 #; 0 #; 0 #; 1 #] , \~ %v_2 = #[ 0 #; 1 #; 2 #; 3 #] , and \~ %v_3 = #[ 1 #; 0 #; 2 #; 0 #] .
\end{eqnarray}

According to the algorithm, we then let %u_1 = %v_1 and %e_1 = %u_1 / ||%u_1||. In this case, we still have %e_1 = %v_1. Next, we compute:

\begin{eqnarray}
 %u_2 & = & %v_2 - ((%v_2 \cdot %e_1) \cdot %e_1) & = & #[ 0 #; 1 #; 2 #; 3 #] - #[ 0 #; 0 #; 0 #; 3 #] & = & #[ 0 #; 1 #; 2 #; 0 #]
 & \~ & %e_2 & = & #[ 0 #; 1/\sqrt(5) #; 2/\sqrt(5) #; 0 #]
\end{eqnarray}

\begin{eqnarray}
 %u_3 & = & %v_3 - ((%v_3 \cdot %e_1) \cdot %e_1) - ((%v_3 \cdot %e_2) \cdot %e_2) 
      & = & #[ 1 #; 0 #; 2 #; 0 #] - #[ 0 #; 0 #; 0 #; 0 #] - #[ 0 #; 4/5 #; 8/5 #; 0 #]  & = & #[ 1 #; -4/5 #; 2/5 #; 0 #]
 & \~ & %e_3 & = & (3\sqrt(5))/5 \cdot #[ 1 #; -4/5 #; 2/5 #; 0 #]
\end{eqnarray}

Thus, {e_1, e_2, e_3} is an orthonormal basis for \span{v_1, v_2, v_3}.
</solution>

<b>Problem:</b> Suppose we are given a linear transformation %f:\R^2 \to \R^2 where:

\begin{eqnarray}
 %f(%v) & = & #[ 2 #, 7 #; 3 #, 1 #] \cdot %v.
\end{eqnarray}

<ol type="a">
<li>Find an orthonormal basis for \im(%f).</li>
<solution>
We know that the column vectors of the matrix are linearly independent. Thus, \dim(\im(%f)) as at least 2, and since
the codomain of %f is \R^2, \im(%f) = \R^2. Thus, any orthonormal basis of \R^2 is appropriate, such as:

\begin{eqnarray}
 { #[ 1 #; 0#] , #[ 0 #; 1#] }.
\end{eqnarray}
</solution>
<li>Find \ker(%f).</li>
<solution>
Let the matrix in the definition of %f be %M. 
We know from lecture that %M is invertible iff \ker(%f) = {0}. Since \det %M \neq 0, %M is invertible, so \ker(%f) = {0}.

Alternatively, we could recall the definition of a kernel:

\begin{eqnarray}
 \ker(%f) & = & {%v | %f(%v) = \0}.
\end{eqnarray}

Thus, it is sufficient to find the set of solutions to the equation %f(%v) = \0, which is the set of solutions (expanding the
definition of %f) of:

\begin{eqnarray}
 #[ 2 #, 7 #; 3 #, 1 #] \cdot %v & = & #[ 0 #; 0 #].
\end{eqnarray}

Since %M is invertible, we can multiply both sides by %M^{-1} to obtain:

\begin{eqnarray}
  %v & = & 1/(3-21) \cdot #[ 1 #, -7 #; -3 #, 2 #] \cdot #[ 0 #; 0 #] \\
  %v & = & #[ 0 #; 0 #].
\end{eqnarray}

Thus, there is only one solution, so the kernel contains a single element, and \ker(%f) = {0}.
</solution>

<li>Show that %f is surjective.</li>
<solution>
To show that %f is surjective, it is sufficient to show that for every %v in the codomain, there exists %x such that:

\begin{eqnarray}
%f(%x) & = & %v.
\end{eqnarray}

In other words, we want a formula for %x in terms of %v that is defined for any %v. Let the matrix in the definition of %f be %M. Since %M
is invertible, we can define:

\begin{eqnarray}
  %x & = & 1/(3-21) \cdot #[ 1 #, -7 #; -3 #, 2 #] \cdot %v
\end{eqnarray}

This formula for %x is always defined, so %f is surjective.
</solution>
</ol>

<b>Problem:</b> Suppose we are given a linear transformation %f:\R^2 \to \R^2 where:

\begin{eqnarray}
 %f(%v) & = & #[ 2 #, 1 #; 8 #, 4 #] \cdot %v.
\end{eqnarray}

<ol type="a">
<li>Find \dim(\im(%f)).</li>
<solution>
Recall that the dimension of a space is the size of any basis of that space (all bases of a space have the same size). Let %M be the
matrix in the definition of %f. The space \im(%f) is equivalent to the span of the column vectors of the matrix:

\begin{eqnarray}
  \im(%f) & = & \span{ #[ 2 #; 8 #] , #[ 1 #; 4 #] }
\end{eqnarray}

To find a basis of a space spanned by a collection of vectors, we create a matrix whose rows are the vectors in that collection, find its
reduced row echelon form, and keep only the nonzero rows in the basis:

\begin{eqnarray}
 #[ 2 #, 8 #; 1 #, 4 #] \to #[ 1 #, 4 #; 1 #, 4  #] \to #[ 1 #, 4 #; 0 #, 0 #]
\end{eqnarray}
\begin{eqnarray}
  \im(%f) & = & \span{ #[ 1 #; 4 #] }
\end{eqnarray}

Thus, the dimension of \im(%f) is 1.
</solution>
<li>Show that %f is not injective.</li>
<solution>
To show that a map is not injective, it is sufficient to find %v and %v' such that %v \neq %v' but %f(%v) = %f(%v').

One approach to finding such %v and %v' is to expand %f(%v) = %f(%v') until the constraints are simple enough that
it is straightforward to try some inputs and easily check that they satisfy the constraints.

\begin{eqnarray}
  %f(%x) & = & %f(%x') \\
   #[ 2 #, 1 #; 8 #, 4 #] \cdot #[ %x #; %y #] & = & #[ 2 #, 1 #; 8 #, 4 #] \cdot #[ %x' #; %y' #]
\end{eqnarray}

Since the top row of the matrix is a multiple of the bottom row, we get one equation:

\begin{eqnarray}
  %x & \neq & %x' \\
  %y & \neq & %y' \\
  2%x + %y & = & 2%x' + %y'
\end{eqnarray}

One possible pair of vectors in the domain that satisfies the above is:

\begin{eqnarray}
  #[%x #; %y #] & = & #[ 0 #; 2 #] \\
  #[%x' #; %y' #] & = & #[ 1 #; 0 #]
\end{eqnarray}

Thus, %f is not injective.
</solution>
<li>Find \ker(%f).</li>
<solution>
We write down the definition of \ker(%f) for this particular map %f:

\begin{eqnarray}
  \ker(%f) & = & { #[ %x #; %y #] | %f( #[ %x #; %y #] ) =  #[ 0 #; 0 #] } 
           & = & { #[ %x #; %y #] | #[ 2 #, 1 #; 8 #, 4 #] \cdot #[ %x #; %y #] = #[ 0 #; 0 #] }
\end{eqnarray}

The above would be sufficient, but suppose we want to find the basis of \ker(%f) (recall that \ker(%f) is a subspace of the domain
of the linear transformation %f). The constraints imposed above can be summarized as:

\begin{eqnarray}
  \ker(%f) & = & { #[ %x #; %y #] | 2%x + %y = 0} \\
           & = & { #[ %x #; %y #] | %y = -2%x}
\end{eqnarray}

Thus, \ker(%f) is the line in \R^2 defined by the equation %y = -2%x. We can choose any vector on this line and take the span of the set
containing only that vector to provide an explicit definition for \ker(%f):

\begin{eqnarray}
  \ker(%f) & = & \span { #[ -1 #; 2 #] } .
\end{eqnarray}
</solution>
</ol>

<b>Problem:</b> Find a matrix %M such that for %f(%v) = %M \cdot %v,

\begin{eqnarray}
  \im(%f) & = & (\span{ #[ 2 #; 0 #; 0 #; 0 #] , #[ 0 #; 0 #; 2 #; 0 #] })^\bot
\end{eqnarray}

<solution>
One way to approach this problem is to expand the definition on the right-hand side using the
definition of the orthogonal complement operation:

\begin{eqnarray}
  \im(%f) & = & (\span{ #[ 2 #; 0 #; 0 #; 0 #] , #[ 0 #; 0 #; 2 #; 0 #] })^\bot
          & = & { #[ x #; y #; z #; t #] | #[ 2 #; 0 #; 0 #; 0 #] \cdot #[ x #; y #; z #; t #] = 0, \~ #[ 0 #; 0 #; 2 #; 0 #] \cdot #[ x #; y #; z #; t #] = 0 } \\
\end{eqnarray}
\begin{eqnarray}
          & = & { #[ x #; y #; z #; t #] | 2%x  = 0, \~ 2%z = 0, \~ %y,%t \in \R } \\
          & = & { #[ x #; y #; z #; t #] | %x  = 0, \~ %z = 0, \~ %y,%t \in \R } \\
          & = & { #[ 0 #; y #; 0 #; t #] | %y,%t \in \R }.
\end{eqnarray}

Thus, 

\begin{eqnarray}
  \im(%f) & = & \span{ #[ 0 #; 1 #; 0 #; 0 #] , #[ 0 #; 0 #; 0 #; 1 #] }.
\end{eqnarray}

Recall that the image of %f is the span of the columns of %M. Thus, one possible solution is:

\begin{eqnarray}
  %M & = & #[ 0 #, 0 #; 1 #, 0 #; 0 #, 0 #; 0 #, 1 #] .
\end{eqnarray}
</solution>

<b>Problem:</b> Suppose that %a^2 + %b^2 = 1. What is the orthogonal projection of %v onto \im(%f) if %f(%v) = %M \cdot %v,

\begin{eqnarray}
 %v & = & #[ 1  #; 2 #; 3 #], and %M & = & #[ %a #, 0 #, 0 #; %b #, 0 #, 0 #; 0 #, %c #, 0 #].
\end{eqnarray}

<solution>
We have that:

\begin{eqnarray}
  \im(%f) & = & \span{ #[ %a #; %b #; 0 #] , #[ 0 #; 0 #; %c #] }.
\end{eqnarray}

Because %a^2 + %b^2 = 1, the first vector is already a unit vector and orthogonal to the second vector. By rescaling the second vector to be
a unit vector, we can obtain an orthonormal basis:

\begin{eqnarray}
  \im(%f) & = & \span{ #[ %a #; %b #; 0 #] , #[ 0 #; 0 #; 1 #] }.
\end{eqnarray}

We can now find the orthogonal projection of %v onto each vector in the orthonormal basis, and add these to find the orthogonal projection of the
vector onto \im(%f):

\begin{eqnarray}
  ( #[ 1 #; 2 #; 3 #] \cdot #[ %a #; %b #; 0 #] ) \cdot #[ %a #; %b #; 0 #] +
    ( #[ 1  #; 2 #; 3 #] \cdot #[ 0 #; 0 #; 1 #] ) \cdot #[ 0 #; 0 #; 1 #]
  & = &  #[ %a^2 + 2%b%a #; %a%b + 2%b^2 #; 3 #].
\end{eqnarray}
</solution>

<b>Problem:</b> Suppose we are given the following points:

\begin{eqnarray}
 #[ 0 #; 6 #] , #[ 2 #; 6 #] , #[ 2 #; 12 #].
\end{eqnarray}

<ol type="a">
<li>Find a function of the form %f(%x) = %a%x^2 + %b%x + %c that is the best least-squares fit for these points.
<solution>
We begin by writing down the equations in terms of %f for each point:

\begin{eqnarray}
 %f(0) & = & 6 \\
 %f(2) & = & 6 \\
 %f(2) & = & 12
\end{eqnarray}

We construct the matrix equation that represents the above system:

\begin{eqnarray}
 #[ (0)^2 #, (0) #, 1 #; (2)^2 #, (2) #, 1  #; (2)^2 #, (2) #, 1 #] \cdot  #[ %a #; %b #; %c #] & = & #[ 6 #; 6 #; 12 #] 
\end{eqnarray}

There is no solution to the above equation. However, we are looking for the least-squares best fit approximation. Thus, we first find an
orthonormal basis of the image of the matrix. If the matrix is %M and %f(%v) = %M \cdot %v, we have that:

\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 4 #; 4 #] , #[ 0 #; 2 #; 2 #] , #[ 1 #; 1 #; 1 #] }.
\end{eqnarray}

Using the <a href="#algorithmgramschmidt">algorithm</a> for finding an orthonormal basis, we obtain:

\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 1/\sqrt{2} #; 1/\sqrt{2} #] , #[ 1 #; 0 #; 0 #] }.
\end{eqnarray}

\begin{tabular}
We now find the orthogonal projection of #[ 6 #; 6 #; 12 #] onto \im(%f) and use it to rewrite the equation so that it is not overdetermined:
\end{tabular}

\begin{eqnarray}
 #[ (0)^2 #, (0) #, 1 #; (2)^2 #, (2) #, 1  #; (2)^2 #, (2) #, 1 #] \cdot  #[ %a #; %b #; %c #] & = & #[ 6 #; 9 #; 9 #]
\end{eqnarray}

This system is underdetermined. The space of solutions implied by the above equation is:

\begin{eqnarray}
 { %f \~ | \~ %f(%x) = %a%x^2 + %b%x + %c, \~ %c = 6, \~ 4%a + 2%b + %c = 9}
\end{eqnarray}

This means that any solution to the system is a least-squares best fit approximation. One possible best fit approximation is:

  $$ %f(%x) = %x^2 + (-1/2)%x + 6 $$

</solution>
</li>

<li>Find a function of the form %f(%x) = %b%x + %c that is the best least-squares fit for these points.
<solution>
We follow the same process as in part (a) above. The matrix equation is:

\begin{eqnarray}
 #[ (0) #, 1 #; (2) #, 1  #; (2) #, 1 #] \cdot  #[ %b #; %c #] & = & #[ 6 #; 6 #; 12 #] 
\end{eqnarray}

There is no solution to the above equation. However, we are looking for the least-squares best fit approximation. Thus, we first find an
orthonormal basis of the image of the matrix. If the matrix is %M and %f(%v) = %M \cdot %v, we have that:

\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 2 #; 2 #] , #[ 1 #; 1 #; 1 #] }.
\end{eqnarray}

Using the <a href="#algorithmgramschmidt">algorithm</a> for finding an orthonormal basis, we obtain:

\begin{eqnarray}
 \im(%f) & = & \span { #[ 0 #; 1/\sqrt{2} #; 1/\sqrt{2} #] , #[ 1 #; 0 #; 0 #] }.
\end{eqnarray}

\begin{tabular}
We again find the orthogonal projection of #[ 6 #; 6 #; 12 #] onto \im(%f) and use it to rewrite the equation so that it is not overdetermined:
\end{tabular}

\begin{eqnarray}
 #[ (0) #, 1 #; (2) #, 1  #; (2) #, 1 #] \cdot  #[ %b #; %c #] & = & #[ 6 #; 9 #; 9 #]
\end{eqnarray}

This yields %c = 6 and %b = 3/2. Thus, the least-squares best fit approximation is:

  $$ %f(%x) = (3/2)%x + 6 $$

</solution>

</li>
</ol>

<b>Problem:</b> Show that for %M \in \R^{n \times n}, the space of solutions to %M %x = \0 is a vector space (you do not need to show that
all the axioms hold; it is sufficient to show the appropriate closure properties).

<solution>
Let %S be the space of solutions.

We show that \0 \in %S:
\begin{eqnarray}
 %M \cdot \0 = \0.
\end{eqnarray}

We show that if %v,%v' \in %S, then %v + %v' \in %S:
\begin{eqnarray}
 %M \cdot %v & = & \0 \\
 %M \cdot %v' & = & \0 \\
 %M \cdot (%v + %v') & = & %M \cdot %v + %M \cdot %v' \\
                     & = & \0 + \0 \\
                     & = & \0.
\end{eqnarray}

We show that if %v \in %S, then for any scalar %s \in \R, %s %v \in %S:
\begin{eqnarray}
 %M \cdot %v & = & \0 \\
 %M \cdot (%s \cdot %v) & = & %s \cdot (%M \cdot %v) \\
                        & = & %s \cdot \0 \\
                        & = & \0.
\end{eqnarray}
</solution>

<b>Problem:</b> Compute the orthogonal projection of %v onto \im(%f) where %f(%v) = %M \cdot %v,

\begin{eqnarray}
 %v & = & #[ 1  #; -2 #; 3 #] , and %M & = & #[ 1 #, 1 #; 2 #, 8 #; 1 #, 5 #].
\end{eqnarray}
<solution>
Normally, the way to approach such problems is to find an orthonormal basis for \im(%f) where %f(%v) = %M \cdot %v, then use
that orthonormal basis to project %v onto \im(%f). However, it's a good idea to check for special cases before delving into a large
computation. In this particular example, we have that:

\begin{eqnarray}
 #[ 1  #; -2 #; 3 #] \cdot #[ 1  #; 2 #; 1 #] & = & 0 \\
 #[ 1  #; -2 #; 3 #] \cdot #[ 1  #; 8 #; 5 #] & = & 0.
\end{eqnarray}

Thus, %v is orthogonal to both vectors, so the orthogonal projection of %v onto \im(%f) is \0 \in \R^3.

It is worth noting that %v is in the orthogonal complement of \im(%f). In general, given a vector subspace %W \subset %V,
for any %w \in %W^\bot, the orthogonal projection of %w onto %W is \0.
</solution>


<b>Problem:</b> Is the system below overdetermined or underdetermined? Does it have a solution?

\begin{eqnarray}
 #[ 1 #, 2 #, 0 #; 0 #, 1 #, -4 #; 0 #, 0 #, 1 #] \cdot %v & = &  #[ 8 #; 3 #; 0 #].
\end{eqnarray}

<solution>
The system is neither overdetermined nor underdetermined. It has a solution. Since the matrix is upper triangular, we can use the
algorithm for solving a system with an upper triangular matrix to obtain %v:

\begin{eqnarray}
 %v & = & #[ 2 #; 3 #; 0 #].
\end{eqnarray}
</solution>

<b>Problem:</b> Determine whether the system below has a solution for all possible %v \in \R^3. If it does not, describe exactly
the set of vectors %v for which the system below has a solution and determine whether this set a vector space.

\begin{eqnarray}
 #[ 1 #, 2 #, 2 #; 0 #, 2 #, -1 #; 1 #, 4 #, 1 #] \cdot %x & = & %v .
\end{eqnarray}

<solution>
Notice that determining whether the system has a solution for any %v is equivalent to determining whether the linear
transformation represented by the matrix is surjective. It is also equivalent to determining whether the matrix is invertible.

To determine whether the matrix is invertible, we could compute its reduced row echelon form by finding an appropriate series of row operations.

\begin{eqnarray}
 #[ 1 #, 2 #, 2 #; 0 #, 2 #, -1 #; 1 #, 4 #, 1 #] \to  #[ 1 #, 2 #, 2 #; 0 #, 1 #, -1/2 #; 1 #, 4 #, 1 #]
                                                     \to #[ 1 #, 0 #, 3 #; 0 #, 1 #, -1/2 #; 1 #, 4 #, 1 #]
                                                     \to #[ 1 #, 0 #, 3 #; 0 #, 1 #, -1/2 #; 0 #, 4 #, -2 #]
                                                     \to #[ 1 #, 0 #, 3 #; 0 #, 1 #, -1/2 #; 0 #, 0 #, 0 #]
\end{eqnarray}

Given the above, we see that the matrix is not invertible.

Let the matrix be %M. The set of vectors for which there is a solution to the above equation is \im(%f) where %f(%x) = %M \cdot %x. Thus,
we want to find \im(%f) explicitly. We could say:

\begin{eqnarray}
 \im(%f) = \span { #[ 1 #; 0 #; 1 #] , #[ 2 #; 2 #; 4 #] , #[ 2 #; -1 #; 1 #] }.
\end{eqnarray}

If we want to be more precise, we could compute \rref(%M^\top) to find a basis for \im(%f).

Since the set of possible vectors %v for which the equation has a solution is \im(%f) and %f is a linear transformation, the set of such %v is a vector
space.
</solution>



























<!--assignment4-->
<br/><hr/>
<a name="6.6.b"></a>
<a name="assignment4"></a>
<b>Assignment #4: Algebra of Linear Transformations</b> <!--span class="btn_assignment">(<a href="materials.php?hw=4">show only this assignment</a>)</span-->

       <p>In this assignment you will work with vector spaces and linear transformations.</p>

<ol>
  <li> Complete the following argument.

@
\forall %f,%g \in \R^2 \to \R^2, \forall %x,%y \in \R^2,
    \~ `(%f \circ %g) is bijective` \and
    \~ %x \neq %y
  \implies
    \~ # ...
    %[
     (%g \circ %f)(%x) & \neq & (%g \circ %f)(%y)
    %]
/@

  </li>
  <li> Complete the following argument.

@
\forall %f \in \R^2 \to \R^2,
    %[ `([1#,2;3#,4]) represents (%f)` %]
  \implies
    \~ # ...
    \~ `(%f) is bijective`
/@

  </li>
  <li> Complete the following argument.

@
\forall %f,%g \in \R^2 \to \R^2,
    %[
    `([1#,2;3#,6]) represents (f)` \and \\
    `([2#,4;-1#,-2]) represents (g)`
    %]
  \implies
    \~ # ...
    %[
     \img(g) & \subset & \ker(f)
    %]
/@

  </li>
  <li> Complete the following argument.

@
\forall %f \in \R^2 \to \R^2,
    %[
    `([4#,7;2#,5]) represents (f)`
    %]
  \implies
    \~ # ...
    \~ `(f) is surjective`
/@

  </li>
</ol>
<br/>

<hr/><br/>
<!--/assignment4-->











<a name="lecture18"></a>
<a name="6.7"></a>
<h3>Orthogonal Projections are Linear Transformations</h3>

<b>Fact:</b> For %v \in \R^n, an orthogonal projection onto a one-dimensional vector space \span{%v} is a linear transformation. First,
note that the dot product of two vectors can be rewritten as matrix multiplication of two matrices:

  $$ %u \cdot %v = %u^\top \cdot %v.$$

Also, notice that

\begin{eqnarray}
   ||%v|| & = & \sqrt(%v \cdot %v) \\
   ||%v||^2 & = & %v \cdot %v
\end{eqnarray}

Finally, notice that for nonzero %r \in \R where %r \neq 0, the matrix #[%r#] is invertible and has inverse #[1/%r#].

Consider the formula for the projection of %w \in \R^n onto \span(%v). We can rewrite the formula to use only multiplication of matrices.

\begin{eqnarray}
(%v/||%v|| \cdot %w) \cdot %v/||%v|| & = & 1/||%v||^2 \cdot (%v \cdot %w) \cdot %v \\
                                     & = & %v \cdot 1/||%v||^2 \cdot (%v \cdot %w)\\
                                     & = & %v \cdot 1/(%v \cdot %v) \cdot (%v \cdot %w) \\
                                     & = & %v \cdot (%v^\top \cdot %v)^{-1} \cdot (%v^\top \cdot %w) \\
                                     & = & (%v \cdot (%v^\top \cdot %v)^{-1} \cdot %v^\top) \cdot %w
\end{eqnarray}

Thus, we have a product of the matrices %v \in \R^{n \times 1}, (%v^\top \cdot %v)^{-1} \in \R^{1 \times 1}, and %v^\top \in \R^{1 \times n}.
This product is a matrix in \R^{n \times n}. Call it %M<sub>\span{v}</sub>. Then we can define the linear transformation %f \in \R^n \to \R^n 
that performs the orthogonal onto \span{v} as:

  $$%f(%w) = %M<sub>\span{v}</sub> \cdot %w.$$

We used the above approach because we will see later that it can be generalized to orthogonal projections onto multidimensional vector spaces.
An alternative way to show the above is to notice that %v/||%v|| can be rewritten as a matrix %u \in \R^{n \times 1}. In that case, we have:

\begin{eqnarray}
  (%v/||%v|| \cdot %w) \cdot %v/||%v|| & = & (%u \cdot %w) \cdot %u \\
                                       & = & %u \cdot (%u \cdot %w) \\
                                       & = & (%u \cdot %u^\top) \cdot %w \\
\end{eqnarray}

Here, %u \cdot %u^\top \in \R^{n \times n}.

<b>Fact:</b> Any orthogonal projection onto a subspace \span{%v_1,...,%v_m} \subset \R^n is a linear transformations. We know that we
can find an orthonormal basis of \span{%v_1,...,%v_m}. Let %M be a matrix the columns of which form an orthonormal basis of \span{%v_1,...,%v_m}.
Then, we can define %f \in \R^n \to \R^n as:

  $$%f(%w) = %M \cdot %M^\top \cdot %w.$$

In fact, by generalizing the formula for projections onto a one-dimensional subspace, we can in certain cases avoid relying on our ability to
compute an orthonormal basis for \span{%v_1,...,%v_m}. However, before we can derive the generalized formula, we need a few preliminary results.

<b>Example:</b> Consider the case where we have

\begin{eqnarray}
 %w & = & #[ 1 #; 2 #; 3 #] \\
 %M & = & #[ 1 #; 2 #; 3 #] \\
 %f(%v) & = & %M \cdot %v \\
 %g(%v) & = & %M^\top \cdot %v.
\end{eqnarray}

Let %V = \im(%f). Then V^\bot is the set of vectors orthogonal to %w. A vector %u \in \R^3 is orthogonal to %w iff %w \cdot %v = 0. But this
means %M^\top \cdot %w = 0, which means %w \in \ker(%g).

<b>Fact:</b> Take any matrix %M \in \R^{n \times m} and linear transformations %f \in \R^m \to \R^n and %g \in \R^n \to \R^m defined 
as 
\begin{eqnarray}
 %f(%v) & = &  %M \cdot %v \\
 %g(%v) & = &  %M^\top \cdot %v.
\end{eqnarray}

We have that

  $$\im(%f)^\bot = \ker(%g).$$

To show these two sets are equal, we can make two arguments:
\begin{eqnarray}
 \im(%f)^\bot & \subset &  \ker(%g) \\
 \ker(%g) & \subset & \im(%f)^\bot.
\end{eqnarray}

 

@
\forall %f,%g \in \R^2 \to \R^2, \forall %M \in \R^(2 \times 2), \forall %v \in \R^2,
  \~ `(%M) represents (%f)` \and
  \~ `(%M^\t) represents (%g)` \and
  \~ %v \in \ker(%g)
  \implies
  %[
     %g(%v) & = & [0;0] \and \\
     %g(%v) & = & %M^\t * %v \and \\
     %M^\t * %v & = & [0;0] \and \\
     %v & \in & (\im(%f))^\bot
  %]
/@

  

@
\forall %f,%g \in \R^2 \to \R^2, \forall %M \in \R^(2 \times 2), \forall %v \in \R^2,
  \~ `(%M) represents (%f)` \and
  \~ `(%M^\t) represents (%g)` \and
  \~ %v \in (\im(%f))^\bot
  \implies
  %[
     %M^\t * %v & = & [0;0] \and \\
     %g(%v) & = & %M^\t * %v \and \\
     %g(%v) & = & [0;0] \and \\
     %v & \in & \ker(%g)
  %]
/@

Because the subset relation applies to the two sets in both directions, the two sets must be equal.

<b>Fact:</b> We show that if a vector is in the kernel of a linear transformation %f \in \R^n \to \R^m, for any %g \in \R^m \to \R^k, it is also
in the kernel of the linear transformation %g \circ %f. In other words, \ker(%f) \subset \ker(%g \circ %f).

@
\forall f,g \in \R^2 \to \R^2, \forall A,B \in \R^(2 \times 2), \forall v \in \R^2,
  \~ `(A) represents (f)` \and
  \~ `(B) represents (g)` \and
  \~ v \in \ker(f)
  \implies
  \~ #...
  \~ v \in \ker(g \circ f)
/@

<b>Fact:</b> Let %M \in \R^{n \times m} and %f,%g \in \R^m \to \R^n be defined as  

\begin{eqnarray}
 %f(%v) & = &  %M \cdot %v \\
 %g(%v) & = &  %M^\top \cdot %v.
\end{eqnarray}

We have that:

  $$ \ker(%f) = \ker(%g \circ %f). $$

We know that \ker(%f) \subset \ker(%g \circ %f) from the previous fact. It suffices to show that 

  $$ \ker(%g \circ %f) \subset \ker(%f). $$

The argument below does just that. Notice that we use the fact that \ker(%g) = \im(%f)^\bot (for appropriate %f and %g) that we derived
above.

@
\forall f,g \in \R^2 \to \R^2, \forall M \in \R^(2 \times 2), \forall v \in \R^2,
  \~ `(M) represents (f)` \and
  \~ `(M^\t) represents (g)` \and
  \~ v \in \ker(g \circ f)
  \implies
  %[
  (g \circ f)(v) & = & [0;0] \and \\
  (g \circ f)(v) & = & g(f(v)) \and \\
  g(f(v)) & = & [0;0] \and \\
  g(f(v)) & = & M^\t * f(v) \and \\
  f(v) & = & M*v \and \\
  (g \circ f)(v) & = & M^\t * (M * v) \and \\
  M^\t * (M * v) & = & [0;0] \and \\
  g(M*v) & = & [0;0] \and \\
  (M*v) & \in & \ker(g) \and \\
  M*v & \in & \im(f) \and \\
  \ker(g) & = & (\im(f))^\bot \and \\
  M*v & \in & (\im(f))^\bot \and \\
  M*v & = & [0;0] \and \\
  f(v) & = & [0;0] \and \\
  v & \in & \ker(f)
  %]
/@

<b>Fact:</b> Suppose that for %M \in \R^{n \times m} and %f \in \R^m \to \R^n where %f(%x) = %M \cdot %x we have an overdetermined system:

  $$ %M \cdot %x = %v. $$

Suppose that \ker(%f) = {\0}. Then for %g \in \R^n \to \R^m where %g(%y) = %M^\top \cdot %y, we have that

  $$ \ker(%g \circ %f) = \ker(%f) = {\0}. $$

Then we have that %g \circ %f is invertible, so %M^\top \cdot %M is invertible and (%M^\top \cdot %M)^{-1} is defined. Thus, we can say that

\begin{eqnarray}
 %M^\top \cdot (%M \cdot %x) & = & %M^\top \cdot %v \\
 (%M^\top \cdot %M) \cdot %x & = & %M^\top \cdot %v \\
 (%M^\top \cdot %M)^{-1} \cdot (%M^\top \cdot %M) \cdot %x & = & (%M^\top \cdot %M)^{-1} %M^\top \cdot %v \\
 %x & = & (%M^\top \cdot %M)^{-1} %M^\top \cdot %v\\
 %M \cdot x & = & %M \cdot (%M^\top \cdot %M)^{-1} %M^\top \cdot %v.
\end{eqnarray}

Thus, even if %M \cdot %x = %v is overdetermined, %M^\top \cdot (%M \cdot %x) = %M^\top \cdot %v has a solution %x such that %M \cdot %x corresponds
to the projection of %v onto \im(%f).

<b>Fact:</b> If a matrix %M \in \R^{n \times m} represents a linear transformation %f(%v) = %M \cdot %v, then we have
that

  $$ \dim(\im(%f)) = \rank(%M). $$

<b>Fact (Rank-Nullity Theorem):</b> If a matrix %M \in \R^{n \times m} represents a linear transformation %f(%v) = %M \cdot %v, then we have
that

  $$ \dim(\ker(%f)) + \dim(\im(%f)) = n. $$

There is more than one way to understand the above.

<!-- <b>Fact:</b> For the linear transformation %f represented by %M \in \R^{n \times m}, if %f -->


<a name="8"></a>
<hr style="margin-bottom:120px;"/>
<h2>Applications to Linear Systems</h2>

<a name="lecture19"></a>
<a name="8.1"></a>
<h3>Interpreting and Using Vector Spaces and Linear Transformations in Applications</h3>

Let %V,%W \subset \R^n be vector spaces, and let %f: %V \to %W be a linear transformations. The following table reviews the operators we introduced
for working with sets of vectors, vector spaces, and linear transformations.

<table class="fig_table">
 <tr>
  <td><b>operator</b></td>
  <td><b>input</b></td>
  <td><b>output</b></td>
  <td><b>additional properties</b></td>
 </tr>  
 <tr>
  <td>\span %V</td>
  <td>set of vectors</td>
  <td>vector space</td>
  <td></td>
 </tr>
 <tr>
  <td>\basis %V</td>
  <td>vector space</td>
  <td>set of vectors</td>
  <td>the output is finite if<br/>the %V has finite dimension<br/>(since \dim %V = |\basis %V|)</td>
 </tr>
 <tr>
  <td>\dim %V</td>
  <td>vector space</td>
  <td>integer</td>
  <td></td>
 </tr>
 <tr>
  <td>%V^\bot</td>
  <td>vector space</td>
  <td>vector space</td>
  <td></td>
 </tr>
 <tr>
  <td>\im %f</td>
  <td>linear transformation</td>
  <td>vector space</td>
  <td>subspace of the codomain %W</td>
 </tr>
 <tr>
  <td>\ker %f</td>
  <td>linear transformation</td>
  <td>vector space</td>
  <td>subspace of the domain %V</td>
 </tr>
</table>

In this section, we consider how these operators and their properties are interpreted within particular applications, and how they can be used to
represent and solve problems within these application domains.


<b>Example:</b> Suppose there are eight islands labelled A,B,C,D, and E,F,G,H, and the following adjacency matrix %M \in \R^{4 \times 4} 
represents whether bridges exist between pairs of islands:

<table cellpadding="0" cellspacing="0" style="font-size:12px;">
 <tr>
  <td></td>
  <td></td>
  <td align="center">E</td>
  <td align="center">F</td>
  <td align="center">G</td>
  <td align="center">H</td>
  <td></td>
 </tr>
 <tr>
  <td>A \~ </td>
  <td><table style="border-left:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>1 <i style="color:gray;">bridges from A to E</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from A to F</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from A to G</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from A to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td>B \~ </td>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">bridges from B to E</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from B to F</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from B to G</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from B to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td>C \~ </td>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">bridges from C to E</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from C to F</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from C to G</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from C to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td>D \~ </td>
  <td><table style="border-left:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>1 <i style="color:gray;">bridges from D to E</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from D to F</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from D to G</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from D to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
</table>

We can extract information from this matrix by multiplying it by a vector in \R^4.


<table><tr>
<td>
<table cellpadding="0" cellspacing="0" style="font-size:12px;">
 <tr>
  <td><table style="border-left:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>1 <i style="color:gray;">bridges from A to E</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from A to F</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from A to G</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from A to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">bridges from B to E</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from B to F</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from B to G</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from B to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">bridges from C to E</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from C to F</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from C to G</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from C to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>1 <i style="color:gray;">bridges from D to E</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from D to F</i> \~ </td>
  <td>1 <i style="color:gray;">bridges from D to G</i> \~ </td>
  <td>0 <i style="color:gray;">bridges from D to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
</table>
</td>
<td>
\cdot
</td>
<td>
<table cellpadding="0" cellspacing="0" style="font-size:12px;">
 <tr>
  <td><table style="border-left:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>1 <i style="color:gray;">count bridges to E</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">ignore bridges to F</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>1 <i style="color:gray;">count bridges to G</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">ignore bridges to H</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
</table>
</td>
<td>
=
</td>
<td>
<table cellpadding="0" cellspacing="0" style="font-size:12px;">
 <tr>
  <td><table style="border-left:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>1 <i style="color:gray;">bridges from A to E or G</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">bridges from B to E or G</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0 <i style="color:gray;">bridges from C to E or G</i> \~ </td>
  <td><table style="border-right:1px solid #000000; solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td><table style="border-left:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>2 <i style="color:gray;">bridges from D to E or G</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
</table>
</td>
</tr></table>

Let %f: \R^4 \to \R^4 be defined as %f(%v) = %M \cdot %v. 

<ol type="a">
 <li>
  What is \ker(%f)? What does this mean about the connectedness of the islands?
  <solution>
  Since \ker(%f) = {\0}, there must be at least one bridge to every destination E,F,G, or H. In other words, if we choose to count
  bridges to even one destination in %v, the product of %M \cdot %v will be nonzero.
  </solution>
 </li>
 <li>
  Suppose for some other adjacency matrix %M' and %g(%v) = %M' \cdot %v, we have that \ker(%g) \neq {\0}. 
  Does this necessarily imply that at least one island has no bridges going to it?
  <solution>
  No. We could have 1 for all entries of %M`, but the kernel would then have dimension 3 (and thus be nonzero). For example,we could let
  \begin{eqnarray}
    %v & = & #[ 1 #; -1 #; 0 #; 0 #].
  \end{eqnarray}
  Then, %v \in \ker(%g) so \ker(%g) \neq {\0}, but all destination islands have a bridge going to them.
  </solution>
 </li>
 <li>
  In terms of \ker(%f) and other sets and set operators, provide sufficient conditions such that %M must describe a situation in which
  each destination island has at least one bridge going to it.
  <solution>
  It must be that \ker(%f) \cap (\R^+)^4 \neq {\0}.
  </solution>
 </li>
</ol>

<b>Example:</b> Suppose we have some noisy observations of a system (radiation at three wavelengths on a logarithmic scale
and the quantity of particles observed). We have only sampled this system on four points:

<table cellpadding="4" style="text-align:center; padding-left:10px;">
 <tr>
  <td><b>IR</b></td>
  <td><b>UV</b></td>
  <td><b>&gamma;</b></td>
  <td><b>quantity</b></td>
 </tr>
 <tr>
  <td>1</td>
  <td>-1</td>
  <td>2</td>
  <td>1</td>
 </tr>
 <tr>
  <td>2</td>
  <td>0</td>
  <td>0</td>
  <td>4</td>
 </tr>
 <tr>
  <td>0</td>
  <td>2</td>
  <td>-4</td>
  <td>2</td>
 </tr>
 <tr>
  <td>3</td>
  <td>1</td>
  <td>-2</td>
  <td>5</td>
 </tr>
</table>

Suppose we believe that the quantity of particles is a linear function of the three radiation quantities. Find the least-squares
approximation of a linear relationship between the three dimensions describing the radiation output and the quantity of particles.

<solution>
One way to use our existing technique for finding approximate functions that fit data is to imagine that there are three unknown
functions (e.g., polynomials) %p,%q,%r \in \R \to \R whose inputs are defined to be the observations. Then, we are looking for a linear
combination of these,

  $$ %f(%x) = %a \cdot %p(%x) + %b \cdot %q(%x) + %c \cdot %r(%x),$$

that best approximates the data:

<table cellpadding="15" style="text-align:center; padding-left:10px;">
 <tr>
  <td><b>IR</b></td>
  <td><b>UV</b></td>
  <td><b>&gamma;</b></td>
  <td><b>quantity</b></td>
 </tr>
 <tr>
  <td>%p(0) = 1</td>
  <td>%q(0) = -1</td>
  <td>%r(0) = 2</td>
  <td>%f(0) = %a \cdot %p(0) + %b \cdot %q(0) + %c \cdot %r(0) \~ = \~ 1</td>
 </tr>
 <tr>
  <td>%p(1) = 2</td>
  <td>%q(1) = 0</td>
  <td>%r(1) = 0</td>
  <td>%f(1) = %a \cdot %p(1) + %b \cdot %q(1) + %c \cdot %r(1) \~ = \~ 4</td>
 </tr>
 <tr>
  <td>%p(2) = 0</td>
  <td>%q(2) = 2</td>
  <td>%r(2) = -4</td>
  <td>%f(2) = %a \cdot %p(2) + %b \cdot %q(2) + %c \cdot %r(2) \~ = \~ 2</td>
 </tr>
 <tr>
  <td>%p(3) = 3</td>
  <td>%q(3) = 1</td>
  <td>%r(3) = -2</td>
  <td>%f(3) = %a \cdot %p(3) + %b \cdot %q(3) + %c \cdot %r(3) \~ = \~ 5</td>
 </tr>
</table>

Notice that this is just a generalization of our approach to modelling data points using polynomials: in those cases, we might have had
%p(%x) = %x^2, %q(%x) = %x, %r(%x) = 1.

We can also view this problem as a search for a linear transformation %g:\R^3 \to \R that will be a least-squares approximation of the data; it can be
defined as:

\begin{eqnarray}
%g(#[%x#;%y#;%z#]) = #[%a#,%b#,%c#] \cdot #[%x#;%y#;%z#]$$
\end{eqnarray}

In a manner similar to previous examples, we can represent the problem as a matrix of inputs and a vector of dependent outputs (one row per data point):

\begin{eqnarray}
 %M = #[ 1 #, -1 #, 2 #; 2 #, 0 #, 0 #; 0 #, 2 #, -4 #; 3 #, 1 #, -2 #], \~ %v = #[ 1 #; 4#; 2 #; 5 #].
\end{eqnarray}

Thus, we want a least-squares approximate solution to the overdetermined system:

\begin{eqnarray}
 #[ 1 #, -1 #, 2 #; 2 #, 0 #, 0 #; 0 #, 2 #, -4 #; 3 #, 1 #, -2 #] \cdot %u = #[ 1 #; 4#; 2 #; 5 #].
\end{eqnarray}

Let %h,%h' \in \R^3 \to \R^4 be defined as %h(%u) = %M \cdot %u and %h'(%u) = %M^\top \cdot %u. We can find the approximation 
for the above overdetermined system by first finding the orthogonal projection of %v onto \im(%h). However, We cannot directly use the formula 
%u = (%M^\top \cdot %M)^{-1} %M^\top %v because \ker(%h) \neq {\0}:

\begin{eqnarray}
 #[ 1 #, -1 #, 2 #; 2 #, 0 #, 0 #; 0 #, 2 #, -4 #; 3 #, 1 #, -2 #] \cdot #[%a#;%b#;%c#]  & = & #[ 0 #; 0 #; 0 #; 0 #] \\
   2 %a & = & 0 \\
   2 %b - 4 %c & = & 0 \\
   %a - %b + 2 %c & = & 0 \\
   %a & = & 0 \\
   %b & = & 2 \\
   %c & = & -1 \\
 #[ 1 #, -1 #, 2 #; 2 #, 0 #, 0 #; 0 #, 2 #, -4 #; 3 #, 1 #, -2 #] \cdot #[0#;2#;-1#]  & = & #[ 0 #; 0 #; 0 #; 0 #] \\
 \~ \~ \~ \~ #[0#;2#;-1#] & \in & \ker(%h) \\
 \ker(%h) & \neq & {\0}
\end{eqnarray}

This means \ker(%h' \circ %h) \neq {\0}. This means %M^\top \cdot %M is not invertible, so (%M^\top \cdot %M)^{-1} is undefined.

If the matrix of data observations %M has a nonzero kernel, it must have at least one column vector that is a linear combination of the
other column vectors. But within the context of the problem this means that for any data point, the value of the dependent dimension
can be determined using the values along the other dimensions. Thus, we need not include this dimension explicitly in our attempt
to model the observations.

The above calculation indicates that the third column vector is a scalar multiple of the second column vector.
Suppose we remove the third column vector from our data. Then, we have:

\begin{eqnarray}
 %N = #[ 1 #, -1 #; 2 #, 0 #; 0 #, 2 #; 3 #, 1#], \~ %l(%u) = %N \cdot %u, \~ %l'(%u) = %N^\top \cdot %u.
\end{eqnarray}

Thus, we can instead look for an approximate solution to the following system:

\begin{eqnarray}
 #[ 1 #, -1 #; 2 #, 0 #; 0 #, 2 #; 3 #, 1#] \cdot #[ %a #; %b #] = #[ 1 #; 4#; 2 #; 5 #].
\end{eqnarray}

This system is overdetermined. However, we have:

\begin{eqnarray}
 #[ 1 #, -1 #; 2 #, 0 #; 0 #, 2 #; 3 #, 1 #] \cdot #[%a#;%b#]  & = & #[ 0 #; 0 #; 0 #; 0 #] \\
 2 %a & = & 0 \\
 2 %b & = & 0 \\
 \ker(%l) & = & {#[%a#;%b#] | %a = 0, %b = 0} = {#[0 #;0 #]} \\
 \ker(%l) & = & {\0}
\end{eqnarray}

Thus, \ker(%l' \circ %l) = {\0}, which means (%N^\top \cdot %N)^{-1} is defined. We can then compute our approximation using the closed formula:

\begin{eqnarray}
  %u^\ast & = & (%N^\top \cdot %N)^{-1} \cdot %N^\top \cdot %v \\
          & = & ( #[ 14 #, 2 #; 2 #, 6 #])^{-1} \cdot #[ 1 #, 2 #, 0 #, 3 #; -1 #, 0 #, 2 #, 1 #] \cdot  #[ 1 #; 4#; 2 #; 5 #] \\
          & = & (1/80) #[ 6 #, -2 #; -2 #, 14 #] \cdot #[ 1 #, 2 #, 0 #, 3 #; -1 #, 0 #, 2 #, 1 #] \cdot  #[ 1 #; 4#; 2 #; 5 #]
\end{eqnarray}
</solution>

<a name="8.2"></a>
<h3>Communications: Applications of Curve Approximations and Properties of Linear Transformations</h3>

<b>Example:</b> Suppose that Alice wants to transmit some data to Bob, and this data consists of the following table:

@

%[
%T & := & [ 3#,-1#,5#,10#,7#,-7#,3#,0#,-15;
            7#,0#,2#,4#,14#,-2#,14#,7#,-35;
            5#,2#,0#,0#,8#,4#,16#,11#,-25;
            2#,1#,3#,6#,3#,-1#,7#,5#,-10;
            0#,4#,-1#,-2#,-4#,9#,12#,12#,0]
%]

/@

Since %T \in \R^{5 \times 9}, the straightforward solution would be to transmit 5 \cdot 9 = 40 scalar values. However, it is
not necessary to transmit all five scalar values found in every column.
<ol type="a">
<li>
Devise a different approach for sending such data that requires
sending fewer than 40 scalar values for the above example; the approach should be such that if
Alice and Bob can agree on how it works, they can reliably transmit all the data.
</li>
<li>
Suppose the matrix of data to transmit is %T \in \R^{m \times n}. Using the \dim and \im operators, write a formula
describing exactly how many scalar values will need to be sent in order to transmit all the data in %T using the method you devised.
</li>
</ol>

<b>Example:</b> Suppose that Alice wants to transmit some data (a sequence of numbers) to Bob without revealing what they are to Eve,
who can read anything Alice sends. One simple encryption scheme involves splitting the message into vectors %v \in \R^n and using
a matrix %M \in \R^{n \times n} to convert them into something that would be unrecognizable to an eavesdropper. For example, suppose 
the message is:

  $$ 1, 2, 3, 4 $$

Alice could split the above message into two vector in \R^2:

\begin{eqnarray}
  #[1#;2#], #[3#;4#]
\end{eqnarray}

She could then use a matrix %M to convert these into something unrecognizable:

\begin{eqnarray}
  #[ 3 #, 1 #; 2 #, 1 #] #[1#;2#], #[ 3 #, 1 #; 2 #, 1 #] #[3#;4#]
\end{eqnarray}

This yields the following vectors, which Alice then transmits to Bob:

\begin{eqnarray}
  #[5#;4#], #[13#;10#]
\end{eqnarray}

<ol type="a">
<li>
If Alice sends a message to Bob using the above method, how can Bob decode the message?
<solution>
Bob can compute the inverse of the encoding matrix and apply it to the messages he receives.
</solution>
</li>
<li>
Suppose Eve finds out what matrix Alice and Bob are using. If Alice and Bob choose a new matrix, what conditions must
be satisfied by the matrix (or the corresponding linear transformation) that Alice will use to encode her messages?
<solution>
The matrix and corresponding linear transformation must be invertible in order for Bob to be able to recover the message in full.
</solution>
</li>
<li>
Suppose that Eve does not know %M, but <i>does</i> know at least one sequence of two consecutive numbers
that were encoded using the matrix (for example, suppose Eve knows that every morning Alice sends a message telling Bob whether it is
sunny (unencoded sequence 1,2) or cloudy (unencoded sequence 3,4) at her location). Show that if Eve is near Alice's location, Eve
will eventually be able to decode any message Alice sends using that matrix.
<solution>
Once Eve has seen an encoded message for both a sunny day and a cloudy day, she can solve the following system to obtain the components
of the matrix:

\begin{eqnarray}
  #[ %a #, %b #; %c #, %d #] #[1#;2#] & = & #[5#;4#] \\
  #[ %a #, %b #; %c #, %d #] #[1#;2#] & = & #[13#;10#]
\end{eqnarray}
</solution>
<li>
Let %s be the unencoded vector Alice uses to represent a sunny day, let %c be the unencoded vector she uses to represent a cloudy day,
and let %f be the linear transformation she uses to encode her messages. Provide sufficient conditions on %s and %c such that Eve will
not be able to recover the entire matrix %M using the method in part (c) above; express you conditions in terms of %s, %c, and %f by using \im
and \span.

<solution>
It must be that \span{%s,%c} = \im(%f).
</solution>

</li>
</li>
</ol>

<a name="lecture20"></a>
<b>Example:</b> More generally, a flaw with Alice's approach is that Eve can detect when Alice sends the same message twice. This leaks information to
Eve about the content of her messages (for example, if Alice is encoding English sentences, the most commonly occurring letter in those
sentences will be "e").
<br/>
<br/>In order to address this, Alice can extend her encoding process. She can take all her vectors in \R^2 and convert them into vectors in
\R^3, and then use a matrix in \R^3 to encode them:

\begin{eqnarray}
  %E & = & #[ 3 #, 1 #; 2 #, 1 #; 0 #, 0 #] $$
\end{eqnarray}

Bob could then decode any such message using the following matrix (notice that the top-left 2 \times 2 portion of this matrix is the
inverse of the top-left 2 \times 2 portion of the matrix above):

  $$ #[ 1 #, -1 #, 0 #; -2 #, 3 #, 0 #; 0 #, 0 #, 0 #] $$

In fact, Bob can decode the messages using any matrix of the form:

\begin{eqnarray}
  %D & = & #[ 1 #, -1 #, %a #; -2 #, 3 #, %b #; 0 #, 0 #, %c #]
\end{eqnarray}

Let %g(%v) = %D \cdot %v. Then, because %g is a linear transformation, for any %w \in \ker(%g) we have that:

\begin{eqnarray}
  %g(%v + %w) & = & %g(%v) + %g(%w) \\
             & = & %g(%v) + \0
\end{eqnarray}

But this means that Alice can take each encoded vector %E \cdot %v that she creates and add <i>any</i> %w \in \ker(%g) to it in order to obfuscate
any patterns that Eve might see in the encoded messages. Alice can choose a random %w \in \ker(%g) for each message.
Bob will easily be able to continue decoding Alice's messages using the same transformation %g.

<ol type="a">
<li>
Suppose Alice has a random vector generator that takes as an input a matrix %M \in \R^{3 \times 3} and returns a random vector in \im(%h)^\bot
where %h(%v) = %M \cdot %v. How can she use this generator to create obfuscated encrypted message vectors?
</li>
<li>
If Alice wants to hide <i>any</i> patterns in the messages she sends over time (including any patterns in parts of her messages),
what is a necessary property of \ker(%g)?
</li>
<li>
Given an encoding matrix %E \in \R^{3 \times 2}, how can Alice and Bob find %D such that \ker(%g) \neq {\0} and such that the
desired property from part (b) above holds?
<solution>
Let the column vectors of %D be %d_1, %d_2, and %d_3. What is needed is a vector %d_3 that satisfies the following:

  $$ %d_3 \in (\span {%d_1, %d_2} - \span {%d_1} - \span {%d_2}) $$
</solution>
</li>
<li>
Suppose Alice has a random number generator that can only generate a single scalar %r \in \R at any given moment.
Find a single matrix %E' \in \R^{3 \times 3} that Alice can use to convert message vectors with a random number, of the following form:

\begin{eqnarray}
  %v & = & #[ %x #; %y #; %r #]
\end{eqnarray}

so that %E' \cdot %v is always an obfuscated encrypted message vector.
</li>
</ol>

<b>Example:</b> Suppose Alice has an unreliable communication device that can transmit any vector %v \in \R^2 to Bob, but will always add
an error vector %e to %v where ||%e|| is bounded by 5:

\begin{eqnarray}
%v & \mapsto & %v + %e \\
||%e|| & < & 5
\end{eqnarray}

Find two matrices %A and %B such that if Alice wants to send %v to Bob, she can send %A \cdot %v using the device and after receiving
the transmission (call it %w), Bob can always recover a close approximation %B \cdot %w that has at most error 0.25 
(that is, ||%B \cdot %w - %v|| \leq 0.25).

<solution>
It is sufficient to scale the vector being sent to make the error negligible, and then to scale it back down once it is received. Thus,

\begin{eqnarray}
%A & = & #[ 100 #, 0 #; 0 #, 100 #] \\
%B & = & #[ 0.01 #, 0 #; 0 #, 0.01 #]
\end{eqnarray}

Then, we have for any %v being sent:

\begin{eqnarray}
%A %v & \mapsto & (100 \cdot %v) + %e
\end{eqnarray}

When Bob decodes it, it will be:

\begin{eqnarray}
%B (%A %v + %e) & = & 0.01 ((100 \cdot %v) + %e) \\
                & = & %v + 0.01 %e \\
\end{eqnarray}

Then we have in the worst case ||%e|| = 5, which means:

\begin{eqnarray}
|| 0.01 \cdot %e || & = & 0.01 \cdot ||%e|| \\
                   & = & 0.05
\end{eqnarray}

</solution>

<a name="lecture21"></a>
<b>Example:</b> Alice has two unreliable communication devices for transmitting vectors in \R^2. 
Given a vector %v \in \R to transmit, the first device will add an <i>arbitrarily large</i> error vector during transmission (\varepsilon varies between
transmissions):

\begin{eqnarray}
%v \~ \mapsto \~ %v + \varepsilon \cdot #[ 1#; 2#]
\end{eqnarray}

The second device will add a different <i>arbitrarily large</i> error vector during transmission:

\begin{eqnarray}
%v \~ \mapsto \~ %v + \varepsilon' \cdot #[ 6#; -3#]
\end{eqnarray}

Alice wants to send a vector %v \in \R^2 to Bob by sending only two messages, one with each device. Devise a way for Alice to send %v \in \R^2 
in this way so that Bob can recover it without any error. This method should not rely in any way on \varepsilon, and 
should work for any \varepsilon.
<solution>
We can observe that the two error vectors are orthogonal. This means that each device can be used to reliably
transfer the projection of a vector onto the orthogonal complement of the span of the error vector for that device. That is,

\begin{eqnarray}
  projection of %v onto (\span { #[ 1#; 2#] })^\bot & = &  projection of %v + \varepsilon \cdot #[ 1#; 2#] onto (\span { #[ 1#; 2#] })^\bot \\
  projection of %v onto (\span { #[ 6#; -3#] })^\bot & = &  projection of %v + \varepsilon' \cdot #[ 6#; -3#] onto (\span { #[ 6#; -3#] })^\bot
\end{eqnarray}

Notice that because the two vectors are orthogonal complements, we have:

\begin{eqnarray}
  (\span { #[ 6#; -3#] })^\bot & = &  \span { #[ 1#; 2#] } \\
  (\span { #[ 1#; 2#] })^\bot & = &  \span { #[ 6#; -3#] }
\end{eqnarray}

Thus, we can rewrite our first set of equalities:

\begin{eqnarray}
  projection of %v onto \span { #[ 6#; -3#] } & = &  projection of %v + \varepsilon \cdot #[ 1#; 2#] onto \span { #[ 6#; -3#] } \\
  projection of %v onto \span { #[ 1#; 2#] } & = &  projection of %v + \varepsilon' \cdot #[ 6#; -3#] onto \span { #[ 1#; 2#] }
\end{eqnarray}

We can now show that the above is true because, as we have seen <a href="#lecture18">further above</a>, projection onto a vector space (including
the span of a single vector) is a linear transformation. Suppose we define the following two projections:

\begin{eqnarray}
  %p(%v) & = &  (#[ 6#; -3#] \cdot (#[ 6#; -3#]^\top \cdot #[ 6#; -3#])^{-1} \cdot #[ 6#; -3#]]^\top) \cdot %v \\
  %q(%v) & = &  (#[ 1#; 2#] \cdot (#[ 1#; 2#]^\top \cdot #[ 1#; 2#])^{-1} \cdot #[ 1#; 2#]^\top) \cdot %v
\end{eqnarray}

Thus, we have that:

\begin{eqnarray}
  %p(%v + \varepsilon \cdot #[ 1#; 2#]) & = &  %p(%v) + \varepsilon \cdot %p(#[ 1#; 2#]) \\
                                        & = &  %p(%v) + \varepsilon \cdot (((#[ 6#; -3#] \cdot (#[ 6#; -3#]^\top \cdot #[ 6#; -3#])^{-1} \cdot #[ 6#; -3#]]^\top) \cdot #[ 1#; 2#]) \\
                                        & = &  %p(%v) + \varepsilon \cdot (#[ 6#; -3#] \cdot (#[ 6#; -3#]^\top \cdot #[ 6#; -3#])^{-1} #[0#]) \\
                                        & = &  %p(%v) + \varepsilon \cdot #[0#;0#] \\
                                        & = &  %p(%v) \\
  %q(%v + \varepsilon \cdot #[ 6#; -3#]) & = &  %q(%v) + \varepsilon \cdot %q(#[ 6#; -3#]) \\
                                         & = &  %q(%v) + \varepsilon \cdot #[0#;0#] \\
                                         & = &  %q(%v)
\end{eqnarray}

Thus, Alice can simply use both devices to send %v to Bob. When Bob receives %w_1 (from the first device) and %w_2 (from the second device),
he must interpret them appropriately:

\begin{eqnarray}
  %w_1 & = & %v + \varepsilon \cdot #[ 1#; 2#] \\
  %w_2 & = & %v + \varepsilon \cdot #[ 6#; -3#]
\end{eqnarray}

Using our equalities from above, we have that:

\begin{eqnarray}
  projection of %v onto \span { #[ 6#; -3#] } & = &  projection of %w_1 onto \span { #[ 6#; -3#] } \\
  projection of %v onto \span { #[ 1#; 2#] } & = &  projection of %w_2 onto \span { #[ 1#; 2#] } \\
   %p(%v) & = & %p(%w_1) \\
   %q(%v) & = & %q(%w_2) \\
\end{eqnarray}

Thus, to retrieve the original %v sent by Alice it is sufficient for Bob to perform the two projections and to add the results:

\begin{eqnarray}
  %v & = & %p(%v) + %q(%v) \\
     & = & %p(%w_1) + %q(%w_2)
\end{eqnarray}

</solution>


<b>Example:</b> Let polynomials in %F = {%f | %f(%t) = %a %t^4 + %b %t^3 + %c %t^2 + %d %t + %e } represent a space of possible radio signals.
To send some information to Bob, Alice must use that information to generate a radio signal that is described by some %f.
To receive that information, Bob must listen to the radio signals he is receiving, determine what %f Alice chose, and decode the message.

Suppose that a given area is saturated with noise or signals (e.g., communication by others) that are linear combinations of the following
three polynomials:

\begin{eqnarray}
  %g_1(%t) & = & 4 %t^2 - 2 %t + 1 \\
  %g_2(%t) & = & - 4 %t - 2 \\
  %g_3(%t) & = & 2 %t^3 + 6 %t + 3
\end{eqnarray}

Devise a communication method that Alice and Bob can use that would allow Alice to send vectors in \R^2 to Bob despite the interference from
the noise.

<solution>
First, let us consider the communication method in this environment when no noise is present. In such a scenario, Alice can choose a vector
%v \in \R^5 to transmit. For example:

\begin{eqnarray}
  %v & = & #[-2 #; -1 #; 0#; 1#; 2#]
\end{eqnarray}

She then has a radio transmitter generate the signal corresponding to the polynomial whose coefficients are represented by %v:

\begin{eqnarray}
  %f(%t) = -2 %t^4 - %t^3 + %t + 2
\end{eqnarray}

To receive the message, Bob samples the signal at five different values of %t:

\begin{eqnarray}
  %f(0) & = & 2 \\
  %f(1) & = & 0 \\
  %f(2) & = & -12 \\
  %f(3) & = & -184 \\
  %f(4) & = & -570 
\end{eqnarray}

In order to recover the message %v, Bob must find the coefficients of the curve in %F that fits the above points. Thus, it is sufficient to solve
the following equation using any appropriate technique. Notice that the solution is the vector Alice %v that was is sending to Bob.

\begin{eqnarray}
  #[ 0 #, 0 #, 0 #, 0 #, 1 #; 1 #, 1 #, 1 #, 1 #, 1 #; 16 #, 8 #, 4#, 2 #, 1 #; 81 #, 27 #, 9 #, 3 #, 1 #; 256 #, 64 #, 16 #, 4#, 1 #] \cdot #[%a #; %b #; %c#; %d#; %e#] & = & #[2 #; 0 #; -12#; -184#; -570#] \\
\end{eqnarray}

Next, suppose that noise is present in the environment. In this scenario, Alice will not be able to send a vector in \R^5 directly. This is
because for any signal polynomial %f that Alice generates, Bob will only be able to sample some linear combination of polynomials
that includes both Alice's signal and the noise from the environment:

\begin{eqnarray}
  %f(%t) + %s_1 %g_1(%t) + %s_2 %g_2(%t) + %s_3 %g_3(%t)
\end{eqnarray}

However, she can first find two polynomials that are linearly independent from the polynomials %g_1, %g_2, and %g_3 that constitute the
radio noise in the environment:

\begin{eqnarray}
  #[1 #; 0 #; 0#; 0#; 0#] & \not\in & \span{#[0#;0#;4#;-2#;1#], #[0#;0#;0#;-4#;-2#], #[0#;2#;0#;6#;3#]} \\
  #[0 #; 0 #; 0#; 1#; 2#] & \not\in & \span{#[0#;0#;4#;-2#;1#], #[0#;0#;0#;-4#;-2#], #[0#;2#;0#;6#;3#]}
\end{eqnarray}

Then, to send a vector %v \in \R^2 to Bob, Alice simply takes the corresponding linear combination of the two vectors and transmits the signal
corresponding to the polynomial whose coefficients correspond to the components of this linear combination:

\begin{eqnarray}
  %x \cdot #[1 #; 0 #; 0#; 0#; 0#] + %y \cdot #[0 #; 0 #; 0#; 1#; 2#]
\end{eqnarray}

For example, suppose Alice wants to send the following vectors in \R^2:

\begin{eqnarray}
  #[ 1 #; 2 #]
\end{eqnarray}

Then she computes:

\begin{eqnarray}
  1 \cdot #[1 #; 0 #; 0#; 0#; 0#] + 2 \cdot #[0 #; 0 #; 0#; 1#; 2#] & = & #[1 #; 0 #; 0#; 2#; 4#]
\end{eqnarray}

She then transmits:

\begin{eqnarray}
  %f(%t) = %t^5 + 2 %t + 4
\end{eqnarray}

The signal Bob hears is some linear combination of Alice's signal and the noise signals in the environment. For example, suppose he hears the
following (note that Bob does not know %s_1, %s_2, and %s_3).

\begin{eqnarray}
  %f(%t) + %s_1 %g_1(%t) + %s_2 %g_2(%t) + %s_3 %g_3(%t)
\end{eqnarray}

For the purposes of this example, suppose that he hears the following (again, note that Bob does not know the coefficients):

\begin{eqnarray}
  %f(%t) + 1 \cdot %g_1(%t) + 2 \cdot %g_2(%t) + 0 \cdot %g_3(%t)
\end{eqnarray}

In order to receive the message, Bob can now do the following.
<ol>
 <li>Bob samples the signal at 5 time points to obtain an equation.

\begin{eqnarray}
  %f(0) & = & 1 \\
  %f(1) & = & -2  \\
  %f(2) & = & 33 \\
  %f(3) & = & 256 \\
  %f(4) & = & 1057  
\end{eqnarray}

\begin{eqnarray}
  #[ 0 #, 0 #, 0 #, 0 #, 1 #; 1 #, 1 #, 1 #, 1 #, 1 #; 16 #, 8 #, 4#, 2 #, 1 #; 81 #, 27 #, 9 #, 3 #, 1 #; 256 #, 64 #, 16 #, 4#, 1 #] \cdot #[%a #; %b #; %c#; %d#; %e#] & = & #[1 #; -2 #; 33#; 256#; 1057#] \\
\end{eqnarray}

 </li>
 <li>Next, Bob can solve the above equation to obtain a vector corresponding to the polynomial that includes both noise and Alice's signal.

\begin{eqnarray}
   #[%a #; %b #; %c#; %d#; %e#] & = & #[1 #; 0 #; 4#; -8#; 1#] \\
\end{eqnarray}

     At this point, Bob knows that this is a linear combination:
     
\begin{eqnarray}
    #[1 #; 0 #; 4#; -8#; 1#] & = & %x #[1 #; 0 #; 0#; 0#; 0#] + %y #[0 #; 0 #; 0#; 1#; 2#] + %s_1 #[0 #; 0 #; 4#; -2#; 1#] + %s_2 #[0 #; 0 #; 0#; -4#; -2#] + %s_3 #[0 #; 2 #; 0#; 6#; 3#]
\end{eqnarray}





 </li>
 <li>Remove the noise from that vector by multiplying it by a matrix with an appropriate kernel.
 
 </li>
 
 
 <li>Determine what linear combination of Alice's two chosen vectors constitute what remains to determine the vector Alice sent.
 
 
 </li>
</ol>

Alternatively, Bob can do the following:

<ol>
 <li>Sample the signal at 10 time points to obtain an equation.
 
 
 </li>
 <li>Solve the equation to obtain both the parameters of the polynomial and the linear combination of the vectors Alice sent and the noise vectors.
 
 
 
 </li>
</ol>











</solution>


<b>Example:</b> Alice has an unreliable communication device for transmitting scalars in \R: for every sequence of scalars she sends, e.g.:

  $$ (%s_1, ..., %s_n),$$
  
up to half of the scalars will be lost during transmission (Bob will know which positions in the sequence have missing scalars, but
he will not know what their quantities were). Alice wants to send %Bob a vector %v \in \R^2, but she cannot use naive methods of
duplication to send it reliably. For example, simply sending many copies of %v will not guarantee delivery of both components because up to
half of the components may be dropped, and these may (albeit with low probability) all be copies of the first component of %v.

Devise a way for Alice to send a Bob a vector %v \in \R^2 using her device so that Bob can recover it without error. Alice and Bob may
coordinate ahead of time to agree on the parameters of the method.

<solution>

We first illustrate the method they can use using an example. Before parting, Alice and Bob agree on a subspace of \R^2
and a finite set of vectors %W from that subspace:

\begin{eqnarray}
  %W & = & {#[ 1 #; 0 #], #[ 2 #; 0 #], #[ 3 #; 0 #], #[ 4 #; 0 #]} \\
  %W & \subset & \span{#[ 1 #; 0 #]}
\end{eqnarray}

They also agree that the first element in this set will be the special vector used for decoding. Next, suppose Alice wants to
send the following vector to Bob:

\begin{eqnarray}
  %v & = & #[ 1 #; 2 #]
\end{eqnarray}

Alice first computes the intersection of the orthogonal complement of the span of each vector in %W with \span{%v}. In this case, we have:

\begin{eqnarray}
  \span{%v} & = & {#[ %x #; %y #] | %y = 2 %x}
\end{eqnarray}

Thus, the intersections of the points in %W and \span{%v} would be:

\begin{eqnarray}
  {#[ 1 #; 2 #], #[ 2 #; 4 #], #[ 3 #; 6 #], #[ 4 #; 8 #]}
\end{eqnarray}

Next, Alice projects these onto the orthogonal complement of the subspace she and Bob initially chose to obtain:

\begin{eqnarray}
 {#[ 0 #; 2 #], #[ 0 #; 4 #], #[ 0 #; 6 #], #[ 0 #; 8 #]}
\end{eqnarray}

She can now discard the 0 entries and sends only the second components to Bob:

  $$ {2, 4, 6, 8} $$

Bob can recover the original message using any two of the above scalars. For example, suppose Bob receives only 4 and 8.
Bob knows that these were generated using the following vectors in %W:

\begin{eqnarray}
  #[ 2 #; 0 #], #[ 4 #; 0 #]
\end{eqnarray}

Thus, Bob knows two vector in \span{%v}:

\begin{eqnarray}
  #[ 2 #; 4 #], #[ 4 #; 8 #]
\end{eqnarray}

To determine \span{%v}, Bob must simply determine the slope of the line through these two vector:

\begin{eqnarray}
  { %a(#[ 4 #; 8 #] - #[ 2 #; 4 #]) + #[ 2 #; 4 #] | %a \in \R } & = & \span{#[1#;2#]}
\end{eqnarray}

Finally, Bob recalls that the first vector in %W was the decoding vector. How now compute the intersection of that vector's span and \span{%v}
to obtain the original %v:

\begin{eqnarray}
 \span{#[1#;2#]} \cap \span{#[1#;0#]} & = & {#[1#;2#]}
\end{eqnarray}

</solution>




<b>Example:</b> Suppose Alice has a very unreliable communication device for transmitting scalars in \R: for every sequence of scalars she sends,
up to half of them are lost; those scalars that do arrive have a bounded error of at most 5. Devise a way for Alice to send vectors in \R^2 to
Bob so that he can always recover the sent vectors with an error of at most 0.25.





<!--assignment5-->
<br/><hr/>
<a name="8.3"></a>
<a name="assignment5"></a>
<b>Assignment #5: Applications: Communication</b> <!--span class="btn_assignment">(<a href="materials.php?hw=5">show only this assignment</a>)</span-->

<p>In this assignment you will use concepts from linear algebra to solve several problems in the application domain of communication.</p>

<ol>
  <li> Alice wants to send Bob the matrix %T below. However, she can only afford to send 30 scalars or fewer. Will she be able to send a message
  from which Bob can recover all the information in the matrix?

@
%[
%T  :=[0#,  3#,  1#, -4#, -1#,  -8#,  0#, -6#, -2#,   8#,   2#,  16;
      1#,  0#, -2#,  0#, -2#,  -9#, -2#,  0#,  4#,   0#,   4#,  18;
      3#, -9#, -9#, 12#, -3#,  -3#, -6#, 18#, 18#, -24#,   6#,   6;
     -2#, -6#,  2#,  8#,  6#,  34#,  4#, 12#, -4#, -16#, -12#, -68;
      3#, -3#, -7#,  4#, -5#, -19#, -6#,  6#, 14#,  -8#,  10#,  38]
%]

%[
numColumns       & := & undefined    # replace with a formula in terms of %T \\
numRows          & := & undefined    # replace with a formula in terms of %T \\
basisSize        & := & undefined    # replace with a formula in terms of %T \\
numScalarsToSend & := & undefined    # replace with a formula that produces an integer \\
numScalarsToSend & \leq & 30
%]
/@

  </li>
  
  
  
  
  
    <li> You know that Alice is using a matrix %M \in \R^{2 \times 2} to send encoded vectors in \R^2 to Bob. You also know that she will only
         send vectors from the following set:

\begin{eqnarray}
{ #[ 3#; 0#], #[ 1#; 2#], #[ 4#; 4#], #[ 0#; 1#], #[ 0#; -2#], #[ 7#; 2#], #[ 2#; 1#], #[ 1#; -1#], #[ -2#; 2#] }
\end{eqnarray}

         You observe two encoded messages as they are being transmitted:

\begin{eqnarray}
{ #[ 4#; -6#], #[ 2#; 11#] }
\end{eqnarray}

         Determine which two unencoded vectors Alice sent, and then use them along with the encoded vectors and some matrix
         operations to provide a definition of the encoding matrix. You should be able to use <code>\augment</code> to accomplish this using a definition
         that fits on a single line.

@
\forall %v,%w \in \R^2, \forall %a,%b,%c,%d \in \R,
    %[
    %v & = & \undefined \and \\
    %w & = & \undefined \and \\
    [%a#,%b;%c#,%d] & = & \undefined
    %]
  \implies
    # ...
    %[
    [%a#,%b;%c#,%d] * %v & = & [4;-6] \and \\
    [%a#,%b;%c#,%d] * %w & = & [2;11]
    %]
/@

  </li>
  
  
  
  
  
  
  
  
  
  
  <li> 
  Alice wants to send Bob a vector %v \in \R^2. She has an unreliable device that can transmit a vector in \R^2 to Bob, but it always adds 
  the following error to %v during transmission (where %s is unknown ahead of time, and different
  each time):

\begin{eqnarray}
%v \mapsto %v + %s \cdot #[ -1 #; 1 #]
\end{eqnarray}

  Alice and Bob will agree on some %u, %M, and %M' before parting. When sending a vector with %x and %y components, Alice will use the device to
  transmit two vectors: (%x \cdot %u) and (%y \cdot %u). Bob will receive %w and %w' as defined below, and will decode %v by computing %M %w + %M %w'.
  Find %u,%M, and %M' that make it possible to retrieve %v, and complete the argument below. <b>Hint:</b> review the propositions available in the
  verification system that deal with matrices and vectors, as they may allow some algebraic manipulations to be more concise.

@
\forall %x,%y,%s,%s' \in \R, \forall %u,%v,%w,%w' \in \R^2, \forall %M,%M' \in \R^(2 \times 2),
   %[
   %u  & = & \undefined \and \\
   %M  & = & \undefined \and \\
   %M' & = & \undefined \and \\
   %w  & = & %x %u + %s [-1;1] \and \\
   %w' & = & %y %u + %s' [-1;1] \and \\
   %v  & = & [%x;%y]
   %]
 \implies
   \~ # ...
   \~ %v = %M %w + %M' %w'
/@

  </li>
      <li> In many of the communication examples being considered, it is useful to be able to construct a matrix whose corresponding linear
           transformation has a specific non-trivial image, and a specific non-trivial kernel (i.e., the span of a particular vector or set
           of vectors). Suppose we know that a vector [%x;%y] in \R^2 is a linear combination of two linearly independent vectors:

\begin{eqnarray}
#[%x#;%y#] & = &  %s \cdot #[%a#;%c#] + %r \cdot #[%b#;%d#]
\end{eqnarray}

We do not know %s and %r, but we do know all of the following:

\begin{eqnarray}
#[%x#;%y#], #[%a#;%c#], #[%b#;%d#]
\end{eqnarray}

One way to remove the [%b;%d] term from [%x;%y] is to first set up a matrix equation and solve for %s and %r:

\begin{eqnarray}
#[%a#,%b#;%c#,%d#] \cdot #[%s#;%r#] & = & #[%x#;%y#]
\end{eqnarray}

However, suppose we instead want to find a linear transformation %f such that for any [%x;%y] as defined above, we have:

\begin{eqnarray}
%f ( #[%x#;%y#] ) & = & %s \cdot #[%a#;%c#]
\end{eqnarray}

In fact, given the information we have, we can construct exactly such a linear transformation:

\begin{eqnarray}
%f ( %v ) & = & (#[%a#,%b#;%c#,%d#] \cdot #[1#,0#;0#,0#] \cdot #[%a#,%b#;%c#,%d#]^{-1}) \cdot %v
\end{eqnarray}

Complete the argument below showing that this linear transformation indeed has this property.

@
\forall %a,%b,%c,%d,%x,%y,%s,%r \in \R,
    %[
    `([%a;%c]) and ([%b;%d]) are linearly independent` \and
    %]
    %[
    %s * [%a;%c] + %r * [%b;%d] = [%x;%y]
    %]
  \implies
    # ...
    %[
    %s * [%a;%c] = ([%a#,%b;%c#,%d] * [1#,0;0#,0] * [%a#,%b;%c#,%d]^(-1)) * [%x;%y]
    %]
/@

  </li>

  <li>
    Let polynomials in %F = {%f | %f(%t) = %a %t^2 + %b %t + %c } represent a space of possible radio signals.
    To send a vector %v \in \R^3 to Bob, Alice sets her device to generate the signal corresponding to the polynomial in %F whose coefficients
    are represented by %v. Bob then has his receiver sample the radio signals in his environment at three time points to retrieve the message.
    <ol type="a">
     <li>
     Suppose Bob samples the signals at %t \in {1,2,3} and obtains the vectors %P defined below. What vector in \R^3 did Alice send? Use
     techniques you have learned in this course to retrieve the vector.

@
%[
%P := { [1;1], [2;-3], [3;-11] }
%]
\~ # ...
%v := \undefined
/@

     </li>
     <li>
     Suppose the environment contains noise from other communication devices; the possible signals in this noise are always from the span
     of the following polynomials:

     \begin{eqnarray}
       %g(%t) & = & 2 %t - 2 \\
       %h(%t) & = & %t^2 + 3 %t
     \end{eqnarray}
     
     Alice and Bob agree that Alice will only try to communicate to Bob one scalar %r \in \R at a time. 
     They agree on a vector %u ahead of time. Any time Alice wants to send some %r \in \R to Bob, she will 
     have her device generate the signal corresponding to the polynomial represented by
     %r \cdot %u.
     
     If the vectors %P below represent the samples collected by Bob, what scalar was Alice sending to Bob?

@
%[
%P := { [1;3], [2;9], [3;13] }
%]
\~ # ...
%s := \undefined
/@

     </li>
     <li>
     Suppose there is no noise in the environment. Alice, Bob, and Carol want to send individual scalars to one another at the same time:
     all three would be sending a signal simultaneously, and all three would be listening at the same time:
     <ul>
       <li>Alice's device would generate a signal corresponding to a scalar multiple of %f(%x) = 2 %x + 1;</li>
       <li>Bob's device would generate a signal corresponding to a scalar multiple of %g(%x) = -%x^2 + 1;</li>
       <li>Carol's device would generate a signal corresponding to a scalar multiple of %h(%x) = %x^2 - 3 %x.</li>
     </ul>

     Suppose you sample the radio signals at a given moment and obtain the vectors %P below. Determine which scalars
     Alice, Bob, and Carol are each transmitting. Your answer can be in the form of a vector.

@
%[
%P := { [0;5], [1;7], [2;7] }
%]
\~ # ...
%v := \undefined
/@

     </li>
    </ol>
  </li>
</ol>
<br/>

<hr/><br/>
<!--/assignment5-->

<a name="lecture22"></a>
<a name="8.4"></a>
<h3>Modelling Systems of Populations: Applications of Linear Transformations and Eigenvalues</h3>

We introduce a few concepts.

<b>Definition:</b> Given a linear transformation %f \in \R^n \to \R^n, %v \in \R^n is a <i>fixed point</i> of %f if:

  $$ %f(%v) = %v $$

<b>Example:</b> Consider the following linear transformation:

\begin{eqnarray}
  %f(%v) & = & #[ 2 #, 1 #; 1 #, 2 #] \cdot %v
\end{eqnarray}

Then the following vector %v is a fixed point of %f:

\begin{eqnarray}
  %v & = & #[ -1 #; 1 #]\\
  %f( #[ -1 #; 1 #] ) & = & #[ 2 #, 1 #; 1 #, 2 #] \cdot #[ -1 #; 1 #] \\
                      & = & #[ -1 #; 1 #]
\end{eqnarray}

<b>Fact:</b> If %v is a fixed point of %f and %v \in \ker %f, then %v = \0.

<b>Fact:</b> For any linear transformation %f, \0 is a fixed point of %f (see the definition of a linear transformation).

<b>Fact:</b> If %v is a fixed point of %f and %v \neq \0, then %f has an infinite number of fixed points. This is because for any %s \in \R and
any fixed point %v, we have that:

\begin{eqnarray}
           %f(%v) & = & %v \\
  %f(%s \cdot %v) & = & %s \cdot %f(%v) \\
                  & = & %s \cdot %v
\end{eqnarray}

<b>Fact:</b> For any linear transformation %f where %f(%v) = %T \cdot %v, let %S be the set of fixed points of %f:

  $$ %S = {%v | %f(%v) = %v } $$

Then %S is a vector space. There are several ways to show this. Let %v be any fixed point of %f. Then we have:

\begin{eqnarray}
  %f(%v) & = & %v \\
  %T \cdot %v & = & %v \\
  (%T \cdot %v)  - %v & = & \0 \\
  (%T \cdot %v)  - \I \cdot %v & = & \0 \\
  (%T  - \I) \cdot %v & = & \0
\end{eqnarray}

Thus, the fixed points of %f represented by %T are exactly the elements of \ker %h where %h(%v) = (%T - \I) \cdot %v,
so %S = \ker %h. Alternatively, we could show that %S contains \0, %S is closed under scalar addition, and %S is closed under scalar multiplication.


We can generalize the notion of a fixed point by observing that a fixed point is just a vector on which the linear transformation acts
as a scalar multiplier (in the case of a fixed point, the multiplier is 1). If %v is a fixed point of %f, then we have that:

  $$ %f(%v) = 1 \cdot %v$$

What if we created another notion that was not restricted to 1?

<b>Definition:</b> Given a linear transformation %f \in \R^n \to \R^n, %v \in \R^n is an <i>eigenvector</i> of %f with <i>eigenvalue</i>
\lambda if:

  $$ %f(%v) = \lambda \cdot %v $$

<b>Fact:</b> For any linear transformation %f, if %v is a fixed point of %f, then it is an eigenvector of %f with eigenvalue 1.

<b>Definition:</b> The set of eigenvectors of a linear transformation %f is its <i>eigenspace</i>.

<b>Fact:</b> Given a linear transformation %f \in \R^n \to \R^n represented by %T \in \R^{n \times n} and an eigenvector %v, consider the following:

\begin{eqnarray}
  %f(%v) & = & \lambda %v \\
  %T \cdot %v & = & \lambda %v  \\
  (%T \cdot %v)  - \lambda %v & = & \0 \\
  (%T \cdot %v)  - \lambda \I \cdot %v & = & \0 \\
  (%T  - \lambda \I) \cdot %v & = & \0
\end{eqnarray}

The above equation has only zero solutions if (%T  - \lambda \I) is invertible:

\begin{eqnarray}
  (%T  - \lambda \I) \cdot %v & = & \0 \\
   (%T  - \lambda \I)^{-1} \cdot (%T  - \lambda \I) \cdot %v & = &  (%T  - \lambda \I)^{-1} \cdot \0 \\
    %v & = &  (%T  - \lambda \I)^{-1} \cdot \0 \\
    %v & = &  \0
\end{eqnarray}

Thus, nonzero eigenvectors %v only if (%T  - \lambda \I) is not invertible. However, this means that \det (%T  - \lambda \I) = 0. Thus, if it
is the case that \det (%T  - \lambda \I) = 0, then there must exist at least one \lambda that solves this equation. In fact, the eigenvalues
of %T are exactly the solutions to the equation:

\begin{eqnarray}
   \det (%T  - \lambda \I) & = & \0
\end{eqnarray}

<b>Example:</b> Suppose that we are modelling a system with two dimensions that are each modelled using \R:
<ul>
 <li>population in the city;</li>
 <li>population in the suburbs.</li>
</ul>

The following matrix (call it %T) represents the movement of the two populations between the two locations (with entries in percentages) over
one year:

<table cellpadding="0" cellspacing="0" style="font-size:12px;">
 <tr>
  <td></td>
  <td></td>
  <td align="center">from city</td>
  <td align="center">from suburbs</td>
  <td></td>
 </tr>
 <tr>
  <td style="text-align:right;">to city \~ </td>
  <td><table style="border-left:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0.95 <i style="color:gray;">stay in the city</i> \~ </td>
  <td>0.03 <i style="color:gray;">move from suburbs to city</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-top:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
 <tr>
  <td style="text-align:right;">to suburbs \~ </td>
  <td><table style="border-left:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
  <td>0.05 <i style="color:gray;">move from city to suburbs</i> \~ </td>
  <td>0.97 <i style="color:gray;">stay in the suburbs</i> \~ </td>
  <td><table style="border-right:1px solid #000000; border-bottom:1px solid #000000; height:100%;"><tr><td>&nbsp;</td></tr></table></td>
 </tr>
</table>

For example, suppose the population in 1999 is represented by %v below. Then the population in 2000 is represented by %T \cdot %v:

\begin{eqnarray}
  %T \cdot %v & = & #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ 600,000 #; 400,000 #] \\
              & = & #[ 582,000 #; 418,000 #]
\end{eqnarray}

Let %f(%v) = %T \cdot %v be the linear transformation represented by %T.
<ol type="a">
  <li>What does a fixed point of %f represent?
  <solution>
  A fixed point of %f represents a stable population distribution that will not change from one year to the next. For example:

\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ 375,000 #; 625,000 #] & = & #[ 375,000 #; 625,000 #]
\end{eqnarray}

  Notice that any scalar multiple of this vector, including a vector that is normalized so that the two components add up to 1,
  is also a fixed point of %f:

\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ 375,000 #; 625,000 #] & = & #[ 0.375 #; 0.625 #]
\end{eqnarray}

 Thus, for this transformation, for any distribution of the population in which 37.5% live in the city, the distribution is stable.
  </solution>
  </li>

  <li>What does an eigenvector of %f represent?
  <solution>
  An eigenvector of %f represents a population distribution that may grow or shrink, but whose relative distribution between the city and
  the suburbs remains the same from one year to the next.
  </solution>
  </li>
  
  <li>
  Does %f have any nonzero eigenvectors other than the fixed points?
  <solution>
  No, because the sum of the components of any vector in \im %f is always the same as the sum of the components of the input vector that produced
  that image.
  
\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[ %x #; %y #] & = & #[ 0.95 %x + 0.03 %y #; 0.05 %x + 0.97 %y #] \\
   %x + %y & = & (0.95 %x + 0.05 %x) + (0.03 %y + 0.97 %y) \\ 
           & = & (0.95 %x + 0.03 %y) + (0.05 %x + 0.97 %y)
\end{eqnarray}

  This is because the sums of the components of the column vectors of %T are 1. This makes %T a <i>stochastic</i> matrix, and it makes
  the fixed points the <i>steady-state</i> or <i>equilibrium</i> vectors of %T.
  </solution>
  </li>
  
  <li>
  Find the vector space of fixed points of %f.
  <solution>
  Note that either the fixed points of %f are {\0}, or there are infinitely many. Thus, if we can find a matrix equation whose solutions
  are the fixed points, we will obtain either a system whose only solution is \0, or an underdetermined system.
  
  We know that for our particular %f and %T, the space of fixed points is \ker %h where:
  
\begin{eqnarray}
  %h(%v) & = & (#[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] - #[ 1 #, 0 #; 0 #, 1 #]) \cdot %v \\
         & = & #[ -0.05 #, 0.03 #; 0.05 #, -0.03 #] \cdot %v
\end{eqnarray}


We know that \ker %h consists of the vectors in the orthogonal complement of the row vectors of (%T - \I).
Recall that if %g(%v) = (%T - \I)^\top, then \ker %h = (\im %g)^\bot. Also, since the row vectors of (%T - \I) are linearly dependent
(the top row multiplied by -1 yields the bottom row and vice versa), we know that \dim \ker %h = 1. Thus,

\begin{eqnarray}
  \ker %h & = & {%v | %v \cdot #[ -0.05 #; 0.03 #] = 0} \\
          & = & \span {#[ 0.03 #; 0.05 #]} \\
          & = & \span {#[ 3 #; 5 #]}
\end{eqnarray}
  </solution>
  </li>


  <li>
  Suppose we want to find a closed formula for the population %k years after the initial state %v (i.e., after applying %T to an initial
  vector %v eight times, or %T^k \cdot %v) where we have:

\begin{eqnarray}
  %v & = &  #[ 0.6 #; 0.4 #]
\end{eqnarray}

  The formula should be in terms of %k and should not require matrix multiplication. In other words,
  we should be able to obtain closed formulas for the city population and the suburb population in terms of %k.

  <solution>
  We can approach this problem by finding the eigenvectors of %f, which span its eigenspace. Then, we can express the result of %T^k \cdot %v
  as a linear combination of eigenvectors.
  
\begin{eqnarray}
  \det (#[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] - \lambda \cdot #[ 1 #, 0 #; 0 #, 1 #]) & = & 0 \\
  \det #[ 0.95 - \lambda #, 0.03 #; 0.05 #, 0.97 - \lambda #] & = & 0 \\
  (0.95 - \lambda) \cdot (0.97 - \lambda) - (0.03 \cdot 0.05) & = & 0 \\
  \lambda^2 - 1.92 \lambda + 0.9215 - 0.0015 & = & 0 \\
  \lambda^2 - 1.92 \lambda + 0.92 & = & 0 \\
  \lambda & = & (1.92 &pm; &radic;(1.92^2 - 4(0.92)))/2 \\
  \lambda & = & 1.92/2 &pm; 0.08/2 \\
  \lambda & = & 0.96 &pm; 0.04 \\
  \lambda & = & 1 \~ <b>or</b> \~ 0.92
\end{eqnarray}

We already know the eigenvector for eigenvalue 1 (it is the fixed point). To find the eigenvector for the other eigenvalue, we solve:

\begin{eqnarray}
  #[ 0.95 #, 0.03 #; 0.05 #, 0.97 #] \cdot #[%x #; %y#] & = & 0.92 \cdot #[%x #; %y#] \\
  0.95 %x + 0.03 %y & = & 0.92 %x \\
  0.05 %x + 0.97 %y & = & 0.92 %y \\
  0.03 %x + 0.03 %y & = & 0 \\
  0.05 %x + 0.05 %y & = & 0 \\
  %x & = & 1 \\
  %y & = & -1
\end{eqnarray}

Thus, our eigenvectors are:

\begin{eqnarray}
  %e_1 & = & #[ 3 #; 5 #]\\
  %e_2 & = & #[ 1 #; -1 #]
\end{eqnarray}

Notice that we had a space of solutions because the system was underdetermined; we chose a particular eigenvector. Finally, notice that:

\begin{eqnarray}
  %T^k \cdot (%a \cdot %e_1 + %b \cdot %e_2) & = & (%T^k \cdot %a \cdot %e_1) + (%T^k \cdot %b \cdot %e_2) \\
                                             & = & %a \cdot (%T^k \cdot %e_1) + %b \cdot (%T^k \cdot %e_2) \\
                                             & = & %a \cdot %e_1 + %b \cdot 0.92^k \cdot %e_2
\end{eqnarray}

Thus, if the initial input vector can be rewritten in terms of the two eigenvectors, we can find the closed formula. In fact, it can be
because the two eigenvectors are linearly independent:

\begin{eqnarray}
  %a \cdot #[ 3 #; 5 #] + %b \cdot #[ 1 #; -1 #] & = & #[ %x #; %y #] \\
  #[ 3 #, 1 #; 5 #, -1 #] \cdot #[%a #; %b #] & = & #[ %x #; %y #] \\
  #[%a #; %b #] & = & #[ 3 #, 1 #; 5 #, -1 #]^{-1} #[ %x #; %y #] \\
  #[%a #; %b #] & = & #[ 3 #, 1 #; 5 #, -1 #]^{-1} #[ 0.6 #; 0.4 #] \\
  #[%a #; %b #] & = & #[ 0.125 #; 0.225 #] \\
  0.125 \cdot #[ 3 #; 5 #] + 0.225 \cdot #[ 1 #; -1 #] & = & #[ 0.6 #; 0.4 #] \\
\end{eqnarray}

The closed formula is:

\begin{eqnarray}
  %T^k \cdot #[ 0.6 #; 0.4 #] & = & 0.125 \cdot #[ 3 #; 5 #] +  0.225 \cdot 0.92^k \cdot #[ 1 #; -1 #]
\end{eqnarray}
  </solution>
  </li>
</ol>

Eigenspaces have many other applications. In particular, they make it possible to provide "natural" interpretations of general notions of
concepts such as differentiation in the context of vector spaces. 

<b>Example:</b> Differentiation is a linear transformation from the vector space of differentiable functions (or a subset, e.g., the polynomials):

\begin{eqnarray}
  \ddx (%a %f + %b %g) \~ & = & \~ %a \ddx %f + %b \ddx %g
\end{eqnarray}

As an example, consider the space of polynomials of the form %f(%x) = %a %x^2 + %b %x + %c. If each polynomial is represented as a vector
of its coefficients, the differentiation operator for this vector space of functions can be represented as a matrix:

\begin{eqnarray}
  #[ 0 #, 0 #, 0 #; 2 #, 1 #, 0 #; 0 #, 1 #, 0 #] \cdot #[ %a #; %b #; %c #] & = &  #[ 0 #; 2 %a #; %b #]
\end{eqnarray}

Thus, an eigenvector in this vector space is any differentiable function %f such that:

\begin{eqnarray}
  \ddx %f & = & \lambda %f
\end{eqnarray}

Notice that the above is a differential equation. If \lambda = 0, then we have for any constant %c \in \R the solution:

\begin{eqnarray}
  %f(%x) = %c
\end{eqnarray}

If \lambda \neq 0 and we do not restrict ourselves to polynomials but allow all infinitely differentiable functions, then we have the solution:

\begin{eqnarray}
  %f(%x) = %c %e<sup>\lambda%x</sup>
\end{eqnarray}

<a name="9"></a>
<a name="lecture23"></a>
<hr style="margin-bottom:120px;"/>
<h2>Review #3</h2>

<a name="9.1"></a>
<h3>Comprehensive List of Course Topics</h3>

The following is a breakdown of what you should be able to do at this point in the course (and of what you may be tested on in the final exam).
Notice that many of the tasks below can be composed (e.g., you could find the image of a linear transformation, and then because it is a
vector space, you could determine its dimension). This also means that many problems can be solved in more than one way.

<ul>
<li>vectors and matrices
  <ul>
  <li>algebraic properties of scalar, vector, and matrix multiplication and vector and matrix addition</li>
  <li>vector properties and relationships
    <ul>
    <li>norm of a vector</li>
    <li>unit vectors</li>
    <li>linear dependence</li>
    <li>linear independence</li>
    <li>dot product of two vectors</li>
    <li>orthogonal vectors</li>
    </ul>
  </li>
  <li>matrix properties and relationships
    <ul>
    <li>identity matrix</li>
    <li>elementary matrices</li>
    <li>scalar matrices</li>
    <li>diagonal matrices</li>
    <li>upper and lower triangular matrices</li>
    <li>determinant of a matrix</li>
    <li>inverse of a matrix and invertible matrices
      <ul>
      <li>determine whether a matrix is invertible</li>
      <li>algebraic properties of matrix inverses with respect to matrix addition and multiplication</li>
      </ul>
    </li>
    <li>transpose of a matrix
      <ul>
      <li>algebraic properties of transposed matrices with respect to matrix addition and multiplication</li>
      </ul>
    </li>
    <li>row echelon form and reduced row echelon form of a matrix</li>
    <li>matrix rank</li>
    </ul>
  </li>
  </ul>
</li>

<li>vector spaces
  <ul>
  <li>vector spaces and their properties
    <ul>
    <li>given a set of vectors or other objects, show that it is a vector space</li>
    <li>express a vector space as a span of a finite set of vectors</li>
    <li>given two vector spaces, show one is a subspace of the other</li>
    <li>given two vector spaces, show they are equal</li>
    <li>find the orthogonal complement of a vector space</li>
    <li>find the basis of a vector space</li>
    <li>find an orthonormal basis of a vector space</li>
    <li>find the dimension of a vector space</li>
    </ul>
  </li>
  <li>particular vector spaces
    <ul>
    <li>subsets of \R^n and \R^{n \times m}</li>
    <li>the set of polynomials of a given order</li>
    </ul>
  </li>
  </ul>
</li>

<li>linear transformations
  <ul>
  <li>show that a map is a linear transformation</li>
  <li>show a linear transformation is surjective</li>
  <li>show a linear transformation is injective</li>
  <li>show a linear transformation is bijective</li>
  <li>given a linear transformation (and/or its matrix representation)...
    <ul>
    <li>show it is injective</li>
    <li>show it is surjective</li>
    <li>show it is bijective</li>
    <li>find its kernel (a vector space)</li>
    <li>find its image (a vector space)</li>
    <li>find its space of fixed points</li>
    </ul>
  </li>
  <li>compositions of linear transformations and their properties</li>
  <li>given a kernal and image, find a linear transformation (i.e., the matrix that represents it) with that kernel and image
  </li>
  <li>compute orthogonal projections...
    <ul>
    <li>onto the span of a single vector in \R^n</li>
    <li>onto a subspace of \R^n...
      <ul>
      <li>by first computing an orthonormal basis and then using it to find the projection</li>
      <li>by using the formula %M (%M^\top \cdot %M)^{-1} %M^\top when \ker %f = {\0} for %f(%v) = %M \cdot %v</li>
      </ul>
    </li>
    </ul>
  </li>
  </ul>
</li>

<li>systems of equations and matrix equations
  <ul>
  <li>solve a linear system of the form %L%U = %v</li>
  <li>find the least-squares approximation of an overdetermined linear system</li>
  </ul>
</li>

<li>applications
  <ul>
  <li>systems of states
    <ul>
    <li>interpret a system of states consisting of dimensioned quantities as a vector space</li>
    <li>interpret relationships in a system of states as a matrix</li>
    <li>interpret system state transitions/transformations as matrices/linear transformations</li>
    <li>interpret partial observations of system states as matrices/linear transformations</li>
    <li>given a partial description of a system state and a matrix of relationships, find the full description of the system state</li>
    </ul>
  </li>
  <li>curve fitting and data approximation
    <ul>
    <li>find a polynomial that fits a given set of points in \R^2</li>
    <li>find a least-squares approximate curve that best fits a set of points in \R^2</li>
    </ul>
  </li>
  <li>communications (sending messages consisting of vectors)
    <ul>
    <li>decrypt message vectors encrypted using a matrix</li>
    <li>eliminate noise from a message vector</li>
    <li>encode a message vector as a polynomial and use sampling to transmit it</li>
    <li>decode a message vector sent as a polynomial</li>
    <li>filter out noise from a message vector sent as a polynomial</li>
    </ul>
  </li>
  </ul>
  <li>population distributions over time
    <ul>
    <li>find the fixed point of a transition matrix</li>
    </ul>
  </li>
  </ul>
</li>

</ul>


<a name="lecture24"></a>
<a name="9.2"></a>
<h3>Practice Problems</h3>

Below is a comprehensive collection of review problems going over all the course material. These problems
are an accurate representation of the kinds of problems you may see on an exam.

<b>Problem:</b> Consider the following vector space:

\begin{eqnarray}
 %V & = & \span{ #[2#;1#;4#], #[0#;1#;-1#]}
\end{eqnarray}

<ul type="a">
  <li>Find the orthogonal projection of the following vector onto %V:

\begin{eqnarray}
 %u & = & #[1#;2#;0#]
\end{eqnarray}
  
<solution>
If the two spanning vectors were orthogonal, one approach would be to project %u onto a normalized
form of each vector, and to add the results. If the two spanning vectors were orthonormal, it would be
sufficient to simply project onto each vector, and add the results. Because the two vectors are neither,
we must use the formula %A (%A^\top %A)^{-1} %A^\top %u for the projection where

\begin{eqnarray}
%A & = & #[ 2#,0 #; 1#,1 #; 4#,-1 #] 
\end{eqnarray}
</solution>


  </li>
  <li>Find a basis of %V^\bot.

<solution>
Note that \dim %V = 2 (since the two spanning vectors are linearly independent), and we know that:

  $$\dim %V + \dim %V^\bot = 3$$

Thus, \dim %V^\bot = 1, so we need one spanning vector of %V^\bot that is orthogonal to both spanning vectors of %V.
It is sufficient to set up an underdetermined
system, solve for two of the variables in terms of the third, and set the third to an arbitrary constant:

\begin{eqnarray}
 #[2#;1#;4#] \cdot #[%x#;%y#;%z#] & = & 0 \\
 #[0#;1#;-1#] \cdot #[%x#;%y#;%z#] & = & 0
\end{eqnarray}

\begin{eqnarray}
 2%x + %y + 4%z & = & 0 \\
 %y - %z & = & 0 \\
 %y & = & %z \\
 %x & = & -%y/2 - 4%z/2 = -%z/2 - 4%z/2 = -5/2 \cdot %z 
\end{eqnarray}

Setting %z = 2, we get:

\begin{eqnarray}
  %V^\bot & = & \span { #[ -5 #; 2 #; 2 #] }
\end{eqnarray}

</solution>

  </li>
  
  <li>Find any matrix %M such that for %f(%v) = %M \cdot %v, \ker %f = %V.

<solution>
It is sufficient to find a matrix that maps both spanning vectors to \0. One approach is to use the formula for a matrix with a specific
kernel and image. From above, we already have a vector that spans %V^\bot, so such a matrix can be computed using the formula:

\begin{eqnarray}
  #[ -5 #, 2 #, 0 #; 2 #, 1 #, 1 #; 2 #, 4 #, -1 #] \cdot  #[ 1 #, 0 #, 0 #; 0 #, 0 #, 0 #; 0 #, 0 #, 0 #] \cdot  #[ -5 #, 2 #, 0 #; 2 #, 1 #, 1 #; 2 #, 4 #, -1 #]^{-1}
\end{eqnarray}

</solution>
  </li>  
</ul>

<b>Problem:</b> Consider the vector space of polynomials of degree at most 2:

  $$ %F = { %f | %f(%x) = %a %x^2 + %b %x + %c, %a,%b,%c \in \R} $$

The map %d:%F \to %F represent differentiation. For example:

\begin{eqnarray}
 %f(%x) & = & 5 %x^2 - 2 %x + 3 \\
 %g(%x) & = & 10 %x - 2 \\
 %d(%f) & = & %g
\end{eqnarray}

<ul type="a">
  <li>Determine whether %d:%F \to %F is injective.  
<solution>
It is not injective because we can find two unequal inputs that produce the same output. Consider the following polynomials:

\begin{eqnarray}
%f(%x) & = & 1 \\
%g(%x) & = & 2 \\
%h(%x) & = & 0
\end{eqnarray}

Then we have that:

\begin{eqnarray}
%d(%f) & = & %h \\
%d(%g) & = & %h
\end{eqnarray}

Thus, the map %d is not injective.
</solution>
  </li>
  <li>Find \ker %d.
<solution>
We are looking for the set of all polynomials in %F whose derivative is the polynomial %f(%x) = 0. We know this is exactly the set of polynomials:

  $${%f | %f(%x) = 0 %x^2 + 0 %x + %r, \~ %r \in \R}$$
</solution>
  </li>
  <li>Recall that a polynomial can be represented as a vector. For example, %f(%x) = 5 %x^2 - 2 %x + 3 can be
  represented as:

\begin{eqnarray}
  #[ 5 #; -2 #; 3 #]
\end{eqnarray}
  
  Show that %d is a linear transformation by finding a matrix representation for %d.

<solution>
The matrix is:

\begin{eqnarray}
#[ 0 #, 0 #, 0 #; 2 #, 1 #, 0 #; 0 #, 1 #, 0 #]
\end{eqnarray}

Notice that:

\begin{eqnarray}
  #[ 0 #, 0 #, 0 #; 2 #, 1 #, 0 #; 0 #, 1 #, 0 #] \cdot #[ %a #; %b #; %c #] & = &  #[ 0 #; 2 %a #; %b #]
\end{eqnarray}

</solution>

  </li>
  <li>Show that %d:%F \to %F is not surjective by using \im and \dim.

<solution>
In order for a linear transformation to be surjective, it must be that \dim \im = \dim %F. We know that \dim %F = 3, and we also know that
there are only two linearly independent columns in the matrix representing %d, so \dim (\im %d) = 2. Thus, \dim %F \neq \dim (\im %d), so
there must be vectors in the codomain of %d that are not in the image of %d, so %d is not surjective.
</solution>
  </li>
</ul>

<b>Problem:</b> Alice wants to send vectors in \R^2 to Bob. For any vector %v \in \R^2 that
she wants to send, she generates a random scalar %r \in \R and sends %w
to Bob as defined below:

\begin{eqnarray}
 %v & = & #[ %x #; %y #] \\
 %w & = & #[ 1 #, 2 #, -1 #; 0 #, 3 #, 1 #; 0 #, 0 #, 2 #] #[ %x #; %y #; %r #]
\end{eqnarray}

<ul type="a">
  <li>Find a matrix %D \in \R^{3 \times 3} that Bob can use to decode Alice's messages.
<solution>
Alice is sending vectors that are a linear combination of two vectors that carry the two scalars in a message vector %v and a noise vector:

\begin{eqnarray}
#[ 1 #; 0 #; 0 #] %x + #[ 2 #; 3 #; 0 #] %y + #[ -1 #; 1 #; 2 #] %z
\end{eqnarray}

Bob must first cancel out the noise using a matrix in \R^3 that has the appropriate kernel. Then he must take the result of that and recover
the scalars %x and %y.

To find an appropriate matrix to cancel out the noise, Bob can use the following formula:

\begin{eqnarray}
  %C & = & #[ 1 #, 2 #, -1 #; 0 #, 3 #, 1 #; 0 #, 0 #, 2 #] \cdot  #[ 1 #, 0 #, 0 #; 0 #, 1 #, 0 #; 0 #, 0 #, 0 #] \cdot #[ 1 #, 2 #, -1 #; 0 #, 3 #, 1 #; 0 #, 0 #, 2 #]^{-1}
\end{eqnarray}

Once Bob applies %C to the vector he receives from Alice, he has:

\begin{eqnarray}
#[ 1 #; 0 #; 0 #] %x + #[ 2 #; 3 #; 0 #] %y + #[0#;0#;0#] \cdot 0 & = & #[ 1 #, 2 #, 0 #; 0 #, 3 #, 0 #; 0 #, 0 #, 0 #] #[ %x #; %y #; 0 #]
\end{eqnarray}

Notice that we can find a matrix that inverts this operation by replacing the top-left portion of the above encoding matrix with its inverse:

\begin{eqnarray}
#[ 1 #, 2 #; 0 #, 3 #]^{-1} & = & #[ 3 #, -2 #; 0 #, 1 #]\\
%D & = & #[ 3 #, -2 #, 0 #; 0 #, 1 #, 0 #; 0 #, 0 #, 0 #] \\
#[ 3 #, -2 #, 0 #; 0 #, 1 #, 0 #; 0 #, 0 #, 0 #] #[ 1 #, 2 #, 0 #; 0 #, 3 #, 0 #; 0 #, 0 #, 0 #] #[ %x #; %y #; 0 #] & = &  #[ %x #; %y #; 0 #]
\end{eqnarray}

Thus, Bob can use the following matrix to decode messages:

  $$ %D \cdot %C.$$

If Bob receives a message %w that is the encoded version of %v, he can retrieve it by computing:

  $$ %D \cdot %C \cdot %w.$$

</solution>
  </li>
  <li>Find a matrix %D' \in \R^{2 \times 3} that Bob can use to retrieve %v \in \R^2 given a transmitted %w \in \R^3.
<solution>
Bob simply needs to drop the third component of the result of %D \cdot %C \cdot %w. This can be accomplished by computing:

\begin{eqnarray}
#[1 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot %D \cdot %C \cdot %w
\end{eqnarray}

Thus, an appropriate matrix in \R^{2 \times 3} would be:

\begin{eqnarray}
#[1 #, 0 #, 0 #; 0 #, 1 #, 0 #] \cdot %D \cdot %C
\end{eqnarray}
</solution>
  </li>
</ul>

<b>Problem:</b> Suppose there are two locations across which a population is distributed. Over the course of each year,
the population moves between the two locations according to one of two population distribution transformations depending
on how well the economy is doing (%A if the economy is doing well, %B otherwise):

\begin{eqnarray}
 %A & = & #[ 0.55 #, 0.9  #; 0.45 #, 0.1 #] \\
 %B & = & #[ 0.75 #, 0.3  #; 0.25 #, 0.7 #] \\
\end{eqnarray}

<ul type="a">
  <li>Find an initial state %v \in \R^2 such that if the economy is always doing well,
      the population distribution will remain the same.

<solution>
We need to find a fixed point of %f(%v) = %A \cdot %v. Since \dim \R^2 = 2, we know that there are three possibilities:
<ul>
 <li>\0 is the only fixed point;</li>
 <li>the space of fixed points has dimension 1, so there are infinitely many fixed points that all fall on a single line;</li>
 <li>all points in \R^2 are fixed points.</li>
</ul>

We solve the following system:

\begin{eqnarray}
 #[ 0.55 #, 0.9  #; 0.45 #, 0.1 #] \cdot #[%x #; %y#] & = & #[%x #; %y#] \\
 0.55 %x + 0.9 %y & = & %x \\
 0.9 %y & = & 0.45 %x \\
 0.45 %x + 0.1 %y & = & %y \\
 0.45 %x & = & 0.9 %y \\
 %x & = & 2 %y
\end{eqnarray}

Since one equation can be derived from the other, the above system is underdetermined, but %x can be expressed in terms of %y. Thus, the space
of fixed points is one-dimensional. Setting %y = 10, it can be expessed as a span of the following fixed point vector:

\begin{eqnarray}
 #[ 20 #; 10 #]
\end{eqnarray}

</solution>
  </li>
  <li>Suppose that over the course of several years, the economy has both done well and not well.
      Has the total population (the sum of the populations in the two locations) changed over this
	  duration?

<solution>
Notice that neither %A nor %B change the total population in the two locations as represented by a state vector in \R^2. Thus, the total
population has not changed.
</solution>
  </li>
</ul>







<a name="A"></a>
<hr style="margin-bottom:120px;"/>
<h2>Appendix</h2>

This section contains a short review of concepts used in this course that are found throughout mathematics.

<a name="A.1"></a>
<h3>Logical formulas and quantifiers</h3>

A mathematical formula is a string of symbols that follow a certain syntax. If it the formula is written using a
correct syntax, we can ask whether the formula is <i>true</i> or <i>false</i>.

<table class="fig_table">
 <tr>
  <td><b>formula</b></td>
  <td><b>true or false</b></td>
  <td><b>example</b></td>
 </tr>
 <tr> 
  <td>\true</td>
  <td>always true</td>
  <td>

@


3 = 3
\true
/@

  </td>
 </tr>
 <tr> 
  <td>\false</td>
  <td>always false</td>
  <td>

@


1 < 2
\false
/@

  </td>
 </tr>
 <tr> 
  <td>%f_1 \and %f_2</td>
  <td>only true if both %f_1 and %f_2 are true</td>
  <td>

@


1 < 2 \and 2 < 3
/@

  </td>
 </tr>
 <tr> 
  <td>%f_1 \implies %f_2</td>
  <td>true if assuming %f_1 can let us derive %f_2<br/> always true if %f_1 is false (i.e., proof by contradiction)</td>
  <td>

@


1 = 2 \and 2 = 3 \implies 1 = 3
/@

  </td>
 </tr>
 <tr> 
  <td>\forall %x \in %S, &nbsp;%f </td>
  <td>true if no matter what element we choose in %S, <br/>substituting %x for that element in %f will result<br/> in a true formula</td>
  <td>

@


\forall x \in {1,2,3}, x > 0
\forall x \in {1,2,3}, x > 2
/@

  </td>
 </tr>
 <tr> 
  <td>\exists %x \in %S, &nbsp;%f </td>
  <td>true if there is at least one element in %S such that <br/>substituting %x for that element in %f will result<br/> in a true formula</td>
  <td>

@


\exists x \in {1,2,3}, x = 2
\exists x \in {1,2,3}, x > 3
/@

  </td>
 </tr>
</table>

<a name="A.2"></a>
<h3>Set comprehensions</h3>

Set comprehension notation is a way to specify how sets can be built or filtered by using formulas.

<table class="fig_table">
 <tr>
  <td><b>set comprehension</b></td>
  <td><b>description</b></td>
  <td><b>result</b></td>
  <td><b>example</b></td>
 </tr>
 <tr>
  <td>{%x | %x \in {1,2,3,4}}</td>
  <td>take each element in %S and put it in <br/>the set represented by {%x | %x \in %S}</td>
  <td>{1,2,3,4}</td>
  <td></td>
 </tr>
 <tr>
  <td>{%x | %x \in {1,2,3,4}, x > 2}</td>
  <td>take each element in %S and put it in <br/>the result set only if it is greater than 2</td>
  <td>{3,4}</td>
  <td>

@


{ x | x \in {1,2,3,4}, x > 2}
/@

  </td>
 </tr>
 <tr>
  <td>{%x | %x \in %S, %f}</td>
  <td>take each element in %S and put it in <br/>the result set only if substituting %x in %f <br/>with that element results in a true formula</td>
  <td></td>
  <td>

@


{ x | 
  x \in {1,2,3,4}, 
  \exists y \in {3,4}, x = y
  }
/@

  </td>
 </tr>
</table>

<a name="A.3"></a>
<h3>Sets and operators</h3>

We often consider sets together with operators over those sets. These operators can have properties
related to those sets.

Operators allow us to construct syntactic <i>terms</i> that are then interpreted as referring to
elements in a set. When two syntactic terms refer to the same element in a set (i.e., have the same
meaning), such as 1+(2+3) and (3+2)+1, we say they are <i>equivalent</i>:

  $$1+(2+3) = (3+2)+1.$$

We have names for several common properties that operators may possess for a given set. In the table below, We define them
precisely using logical notation.

<table class="fig_table">
 <tr>
  <td><b>property</b></td>
  <td><b>definition</b></td>
 </tr>
 <tr>
  <td>%S is closed under \oplus</td>
  <td>\forall %x,%y \in %S, <br/>&nbsp;&nbsp;&nbsp; %x \oplus %y \in %S</td>
 </tr>
 <tr>
  <td>\oplus is commutative on %S</td>
  <td>\forall %x,%y \in %S, <br/>&nbsp;&nbsp;&nbsp; %x \oplus %y = %y \oplus %x</td>
 </tr>
 <tr>
  <td>\oplus is associative on %S</td>
  <td>\forall %x,%y,%z \in %S, <br/>&nbsp;&nbsp;&nbsp; (%x \oplus %y) \oplus %z = %x \oplus (%y \oplus %z)</td>
 </tr>
 <tr>
  <td>\oplus has a left identity <b>I</b> in %S</td>
  <td>\forall %x \in %S, <br/>&nbsp;&nbsp;&nbsp; <b>I</b> \oplus %x = %x</td>
 </tr>
 <tr>
  <td>\oplus has a right identity <b>I</b> in %S</td>
  <td>\forall %x \in %S, <br/>&nbsp;&nbsp;&nbsp; %x \oplus <b>I</b> = %x</td>
 </tr>
 <tr>
  <td>\oplus has an identity <b>I</b> in %S</td>
  <td>\forall %x \in %S, <br/>&nbsp;&nbsp;&nbsp; <b>I</b> \oplus %x = %x \oplus <b>I</b> = %x</td>
 </tr>
 <tr>
  <td>\otimes distributes across \oplus in %S</td>
  <td>\forall %x,%y,%z \in %S, <br/>&nbsp;&nbsp;&nbsp; %x \otimes (%y \oplus %z) = (%x \otimes %y) \oplus (%x \otimes %z)</td>
 </tr>
</table>

Equality is assumed to be reflexive, symmetric, and transitive.

<table class="fig_table">
 <tr>
  <td><b>property</b></td>
  <td><b>definition</b></td>
 </tr>
 <tr>
  <td>= is reflexive for %S</td>
  <td>\forall %x \in %S, <br/>&nbsp;&nbsp;&nbsp; %x = %x</td>
 </tr>
 <tr>
  <td>= is symmetric for %S</td>
  <td>\forall %x,%y \in %S, <br/>&nbsp;&nbsp;&nbsp; %x = %y iff %y = %x</td>
 </tr>
 <tr>
  <td>= is transitive for %S</td>
  <td>\forall %x,%y,%z \in %S, <br/>&nbsp;&nbsp;&nbsp; %x = %y and %y = %z implies %x = %z</td>
 </tr>
</table>

<a name="A.4"></a>
<h3>Relations, maps, and functions</h3>

A relation between two sets %X and %Y is simply a subset of the collection of all pairs of objects drawn from two sets.

<table class="fig_table">
 <tr>
  <td><b>construct</b></td>
  <td><b>definition</b></td>
  <td><b>example</b></td>
  <td><b>graphical example</b></td>
 </tr>
 <tr>
  <td>%X \times %Y</td>
  <td>{ (%x,%y) | %x \in %X, %y \in %Y }</td>
  <td>{1,2,3} \times {4,5,6} <br/>=<br/>{(1,4),(1,5),(1,6),<br/>(2,4),(2,5),(2,6),<br/>(3,4),(3,5),(3,6)}</td>
  <td></td>
 </tr>
 <tr>
  <td>%R is a relation between %X and %Y</td>
  <td>%R \subset %X \times %Y</td>
  <td>{(1,D), (2,B), (2,C)} <br/>is a relation between<br/> {1,2,3,4} and {A,B,C,D}</td>
  <td><img src="images/relation.png"></td>
 </tr>
 <tr>
  <td>%f is a function from %X to %Y</td>
  <td>
    %f is a relation between %X and %Y and<br/>
    \forall %x \in %X, there is exactly one<br/>%y \in %Y s.t. %f relates %x to %y
  </td>
  <td>{ (%x,%f(%x)) | %f(%x) = %x^2 }</td>
  <td><img src="images/function.png"></td>
 </tr>
 <tr>
  <td>%R^{-1} is the inverse of %R</td>
  <td>{ (%b,%a) | (%a,%b) \in %R }</td>
  <td></td>
  <td></td>
 </tr>
 <tr>
  <td>%f: %X \to %Y is injective</td>
  <td>
  </td>
  <td></td>
  <td><img src="images/injection.png"></td>
 </tr>
 <tr>
  <td>%f: %X \to %Y is surjective</td>
  <td>
  </td>
  <td></td>
  <td><img src="images/surjection.png"></td>
 </tr>
 <tr>
  <td>%f: %X \to %Y is bijective</td>
  <td>
  </td>
  <td></td>
  <td><img src="images/bijection.png"></td>
 </tr>
</table>

Notice that we may have %f such that %f is a function, but %f^{-1} is not a function.

<!--eof-->
